project_id,title,description
1,Walmart Sales Forecasting Data Science Project,"Every Departmental store chain like Walmart wants to predict the store sales in the future so that inventory planning can be done. Along with that, sales prediction helps to increase/decrease store staff based on the rush (More sales can mean more customers are coming to the stores). Also, it is always a good idea to do sales and revenue forecasting to better understand the company's cash-flows and overall growth.
For inventory planning, you also need to know what products (or category of products aka department) will be utilized more. Under-stock some products and your sales are hit. Over-stock items like perishables and you run into losses if the product expires. That's why the sales prediction is done at a combination of store and department level (and sometimes even at product level for high-selling products).

Objective of Retail Analysis with Walmart Data Python Project
In this sales forecasting project, we will use the historical sales data of 45 Walmart stores based on store location, department, and week. Each store&rsquo;s land area and type have been provided, and holiday weeks have been marked. Along with these, price markdown data (almost like discount data) has been given. A few macro-indicators like CPI, Unemployment rate, Fuel price, etc., are also provided. Our goal is to analyze the Walmart sales data and predict their stores&rsquo; sales for all the weeks in a year.
&nbsp;

Data Science Techniques used for Walmart Store Sales Forecasting
For this Walmart project, you will learn about exciting data science tools and methods that data scientists use almost every day in their job. Here is an overview of the techniques used in this project.
Exploratory Data Analysis (EDA)
Handling large amounts of data is not an easy task. Before deciding upon which mathematical methods to use, one must draw statistical inferences from the data. Parameters like mean, median, mode, minimum and maximum value, etc., assist in understanding which models should be used for preprocessing. This project will use the Walmart dataset and plot various graphs to understand the overall distribution of different variables using Python libraries like NumPy, Pandas, and Seaborn.
Data Preprocessing
Once the data has been analyzed, the next task is to prepare the data for applying machine learning (ML) models. Many negative values do not make sense in the real world. Data scientists use methods like averaging the neighborhood values, making them all zero, adding a specific number to all the variables, etc., to pre-process data. In this Walmart sales prediction project, you will learn how to handle negative values, missing values, and duplicate values of different variables. Additionally, you will learn how to convert variables of varying data types into a specific data type for easy application of ML algorithms.
Data Merging
Information is often stored in separate condensed data files to save on hardware. A data scientist then merges the different data files to draw inferences. In this retail analysis with Walmarts dataset project, you will merge three separate data files for creating testing and validation datasets. The dataset will then be used for sales prediction.
Statistical Modelling
If we look at the dataset in this project at a more granular level, one can quickly build a Walmart sales forecasting model using elementary statistics. This project will guide you on choosing specific variables for forecasting sales with the help of more straightforward statistical methods.
Time Series Forecasting
After implementing essential statistical tools for this Walmart project, you will learn about various time series forecasting models as this project aims to predict sales with respect to time. The method discussed in detail in this project solution is the AutoRegressive Integrated Moving Average (ARIMA) model. You will get to explore how to prepare the dataset for implementing the ARIMA model and draw valuable inferences.
Machine Learning Algorithms
The most crucial part of any data sci project is the implementation of a machine learning/deep learning model. You will learn how to implement regression models like the random forest for sales forecasting using the Walmart dataset in this project. You will learn how to perform hyperparameter tuning for training the model efficiently.
Comparing Different Machine Learning Models
Finally, the project will help you prepare the Walmart sales prediction project report by comparing the performance of different models discussed above. You will use a few statistical formulae to analyze their performance and some metrics to choose the best model.
&nbsp;
Frequently Asked Questions on Walmart Data Science Project
&nbsp;
What type of forecasting does Walmart use?
Walmart uses its data for time series analysis and prediction methods for forecasting its sales across its different stores across the United States. As per the company&rsquo;s distinguished data scientist and director of data science, John Bowman, Walmart estimates the predictions for a full 52-week horizon in weekly increments, and produces a new set of forecasts every week, over the weekend. Additionally, the company has a 12-hour window to complete all of the forecasting tasks, and about three days to evaluate all of the training tasks. The methods include Deep Learning models as well and Walmart also assesses the forecasts which it has received from its vendors for comparison and evaluation.
&nbsp;
What are the methods of sales forecasting?&nbsp;
Here are a few methods used for sales forecasting:

Time-series forecasting models like ARIMA.

Machine learning models like Random forests.

Deep Learning models like Neural Prophet and SilverKite

"
2,Credit Card Default Prediction using Machine learning techniques,"Business Context
Banks are primarily known for the money lending business. The more money they lend to people whom they can get good interest with timely repayment, the more revenue is for the banks. This not only save banks money from having bad loans but also improves image in the public figure and among the regulatory bodies.
The better the banks can identify people who are likely to miss their repayment charges, the more in advance they can take purposeful actions whether to remind them in person or take some strict action to avoid delinquency.
In cases where a borrower is not paying monthly charges when credit is issued against some monetary thing, two terms are frequently used which are delinquent and default.
&nbsp;
Delinquent in general is a slightly mild term where a borrower is not repaying charges and is behind by certain months whereas Default is a term where a borrower has not been able to pay charges and is behind for a long period of months and is unlikely to repay the charges.
This case study is about identifying the borrowers who are likely to default in the next two years with serious delinquency of having delinquent more than 3 months.
We will also use TrueFoundry's ML Monitoring and Experiment tracking Solution known as MLFoundry to log the experiments, models, metrics, data &amp; features which can be used to generate informative dashboards and insights.&nbsp;
&nbsp;
&nbsp;
Objective
Building a model using the inputs/attributes which are general profile and historical records of a borrower to predict whether one is likely to have serious delinquency in the next 2 years
We will be using Python as a tool to perform all kind of operations in this credit score prediction machine learning project.&nbsp;
Dataset
In this credit scoring system project, we will use a dataset containing two files- training data and test data. We have a general profile about the borrower such as age, Monthly Income, Dependents, and the historical data such as what is the Debt Ratio, what ratio of the amount is owed with respect to the credit limit, and the no of times defaulted in the past one, two, three months.
We will be using all these features to predict whether the borrower is likely to default in the next 2 years or not having a delinquency of more than 3 months.
Main Libraries used
Pandas for data manipulation, aggregation
Matplotlib and Seaborn for visualization and behavior with respect to the target variable
NumPy for computationally efficient operations
Scikit Learn for model training, model optimization, and metrics calculation
Imblearn for tackling class imbalance problem
Shap and LIME for model interpretability
Keras for Neural Network(Deep Learning architecture)
Approach for Credit Card Default Prediction in Python


Data Cleaning


Data cleaning is the process of organizing and correcting data that is badly structured, incomplete, duplicate or, otherwise messy. It involves eliminating inconsistencies in data, as well as reorganizing data to make it much easier to use. Standardization of dates and addresses, ensuring consistent field values (e.g., ""data cleaning"" and ""Data Cleaning""), parsing area codes from phone numbers, etc., are all instances of data cleaning.&nbsp;
In this project, we will treat outliers, resolve some accounting errors, and treat missing value values.


Feature Engineering


When developing a prediction model using machine learning or statistical modeling, feature engineering refers to the method of selecting and transforming the most significant variables from actual data using industry knowledge. The purpose of feature engineering and selection is to boost machine learning algorithms' efficiency. This credit score prediction project entails applying feature engineering techniques to the training and test dataset. It also involves scaling features with Box-Cox transformation, standardization, upsampling, downsampling, and SMOTE.
Deep Learning Algorithms
Deep Learning is a set of algorithms driven by the human brain's data-processing and pattern-creation capabilities, which are advancing and developing on the idea of a single model architecture termed Artificial Neural Network. Deep learning is a part of Machine Learning that does data processing and calculations on a large quantity of data using numerous layers of neural networks. In this credit scoring system project, we have built a neural network model and fitted it on Box-Cox transformed credit score dataset, Standardized credit score dataset, etc. For this credit scoring system project, we have a number of deep learning algorithms (Logistic regression, Random Forest, XGBoost, etc.) being applied to the prediction model.&nbsp;


ROC AUC Curve


The Receiver Operating Characteristic curve, or the ROC curve, is a graph of the false positive rate (x-axis) vs. the true positive rate (y-axis) for a variety of candidate threshold values ranging from 0.0 to 1.0. The roc_auc_score() function computes the area under the ROC curve. The project involves plotting ROC AUC plots for each of the machine learning algorithms and for each transformed dataset.


MLFoundry


TrueFoundry's MLFoundry experiment tracking and model monitoring system combines the strengths of open-source tools such as MLFlow, Whylogs, and others. It comes with a shareable dashboard where you can keep track of your tests and model, among other things.&nbsp; We create a client for the MLFoundry repository and assign a project name. To make experiment tracking easier, we assign different names for different experiments as well as different runs.&nbsp;
FAQs
Q1. What are the types of credit scoring models?
FICO and VantageScore are the two different types of credit scoring models.
Q2. What is the most common credit scoring system?
The FICO scoring system is the most commonly used and reliable scoring system due to its proven track record.
"
3,Analyse Yelp Dataset with Spark & Parquet Format on Azure Databricks,"Business Overview
Azure Databricks is a data analytics tool tailored for the Microsoft Azure cloud services platform. In massive data pipelines, raw and structured data is imported into Azure in batches via Azure Data Factory or streamed near real-time via Apache Kafka, Event Hub, or IoT Hub. This data is stored in a data lake for long-term sustained storage, either in Azure Blob Storage or Azure Data Lake Storage. Azure Databricks is utilized to read data from different data sources and transform it into breakthrough insights using Spark as part of the analytics workflow.
Yelp is a community review site and an American multinational firm based in San Francisco, California. It publishes crowd-sourced reviews of local businesses as well as the online reservation service Yelp Reservations. Yelp has made a portion of their data available in order to launch a new activity called the Yelp Dataset Challenge, which allows anyone to do research or analysis to find what insights are buried in their data. Due to the bulk of the data, this project only selects a subset of Yelp data in a zip file named 'dataset.zip,' which comprises three JSON files, including 'business.json', which provides business data such as location data, attributes, and categories.
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
Approach
- &nbsp; Read yelp datasets in ADLS and convert JSON to parquet for better performance.
- &nbsp; Convert JSON to Delta Format.
- &nbsp; Total records in each dataset.
- &nbsp; Partition tip dataset tip by a date column.
- &nbsp; repartition() vs coalesce()
- &nbsp; Find the top 3 users based on their total number of reviews.
- &nbsp; Find the top 10 users with the most fans
- &nbsp; Analyse the top 10 categories by a number of reviews.
- &nbsp; Analyse top businesses which have over 1000 reviews.
- &nbsp; Analyse Business Data: Number of restaurants per state.
- &nbsp; Analyze the top 3 restaurants in each state.
- &nbsp; List the top restaurants in a state by the number of reviews.
- &nbsp; Numbers of restaurants in Arizona state per city.
- &nbsp; Broadcast Join: restaurants as per review ratings in Pheonix city.
- &nbsp; Most rated Italian restaurant in Pheonix.
Tech Stack
➔ Language: Python3
➔ Services: Azure Data factory, Azure Databricks, ADLS
Architecture&nbsp;

&nbsp;"
4,Expedia Hotel Recommendations Data Science Project,"Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination,&nbsp;it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a&nbsp;trendy pool bar?&nbsp;

Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with&nbsp;hundreds of millions of visitors every month!
Currently,&nbsp;Expedia uses&nbsp;search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them&nbsp;for each&nbsp;user. In this competition, Expedia is challenging you to contextualize customer data and predict the likelihood a user will&nbsp;stay at 100 different hotel groups.
Data Description:
Expedia has provided you logs of customer behavior. These include what customers searched for, how they interacted with search results (click/book), whether or not the search result was a travel package. The data in this project is a random selection from Expedia and is not representative of the overall statistics.

Expedia is interested in predicting which hotel group a user is going to book. Expedia has in-house algorithms to form hotel clusters, where similar hotels for a search (based on historical price, customer star ratings, geographical locations relative to city center, etc) are grouped together. These hotel clusters serve as good identifiers to which types of hotels people are going to book, while avoiding outliers such as new hotels that don't have historical data.
Your goal of this project is to predict the booking outcome (hotel cluster) for a user event, based on their search and other attributes associated with that user event.
The train and test datasets are split based on time: training data from 2013 and 2014, while test data are from 2015. Training data includes all the users in the logs, including both click events and booking events. Test data only includes booking events.
destinations.csv data consists of features extracted from hotel reviews text.
Note that some srch_destination_id's in the train/test files don't exist in the destinations.csv file. This is because some hotels are new and don't have enough features in the latent space. Your algorithm should be able to handle this missing information.
Field Description:
train/test.csv


Column name
Description
Data type




date_time
Timestamp
string


site_name
ID of the Expedia point of sale (i.e. Expedia.com,&nbsp;Expedia.co.uk,&nbsp;Expedia.co.jp, ...)
int


posa_continent
ID of continent associated with site_name
int


user_location_country
The ID of the country the customer is located
int


user_location_region
The ID of the region the customer is located
int


user_location_city
The ID of the city the customer is located
int


orig_destination_distance
Physical distance between a hotel and a customer at the time of search.&nbsp;A null means the distance could not be calculated
double


user_id
ID of user
int


is_mobile
1 when a user connected from a mobile device, 0 otherwise
tinyint


is_package
1 if the click/booking was generated as a part of a package (i.e. combined with a flight), 0 otherwise
int


channel
ID of a marketing channel
int


srch_ci
Checkin date
string


srch_co
Checkout date
string


srch_adults_cnt
The number of adults specified in the hotel room
int


srch_children_cnt
The number of (extra occupancy) children specified in the hotel room
int


srch_rm_cnt
The number of hotel rooms specified in the search
int


srch_destination_id
ID of the destination where the hotel search was performed
int


srch_destination_type_id
Type of destination
int


hotel_continent
Hotel continent
int


hotel_country
Hotel country
int


hotel_market
Hotel market
int


is_booking
1 if a booking, 0 if a click
tinyint


cnt
Numer of similar events in the context of the same user session
bigint


hotel_cluster
ID of a hotel cluster
int



&nbsp;
destinations.csv


Column name
Description
Data type




srch_destination_id
ID of the destination where the hotel search was performed
int


d1-d149
latent description of search regions
double


"
5,Data Science Project in Python on BigMart Sales Prediction,"Occasionally, we all love to go on a shopping spree, only to realize later that most of the things we purchase may not be precisely valuable. One of the primary reasons behind this could be store executives&rsquo; clever placement of different products. You might be wondering if that is the case, how can one then protect themselves from falling for such smartly curated traps? Find out the answer to it as we discuss briefly one of the most straightforward machine learning project ideas for beginners: BigMart Sales Prediction.
Introduction to Big Mart Sales Prediction Project in Python
The BigMart sales prediction dataset contains 2013's annual sales records for 1559 products across ten stores in different cities. Such a vast amount of data can reveal insights about apparent customer preferences as specific attributes of each product and store have been defined in the dataset.&nbsp;

&nbsp;
This data science project aims to build a predictive model and determine the sales of each product at a particular store. Using this model, BigMart will try to understand the properties of products and stores, which play a crucial role in increasing sales and developing better business strategies.
Tech Stack
Language: Python
Libraries: Pandas, NumPy, MatplotLib, sklearn
Data Science Concepts You Will Master in this BigMart Sales Prediction ML Project
We have briefly discussed the key learning takeaways from this python sales prediction project.

Exploratory Data Analysis
Understanding the dataset and the distribution of variables is critical in implementing any ML project ideas. Thus, the first data science concept to explore in this big mart sales prediction project is plotting graphs to understand the data. You will learn how to generate visualizations that help analyze outliers threshold and reveal variables containing missing values. Furthermore, the graphs will also assist in determining insights about different stores, for example, how the size of an outlet is related to the profit it generates. Visualizing data also helps decide thresholds for converting categorical variables into numerical values and deducing irrelevant feature variables.
Data Preprocessing
The Bigmart sales dataset contains different types of variables, and this project will assist you in processing different data types. Additionally, the data has missing values as some stores do not report all the data due to technical glitches. Hence, you get to learn how to treat them accordingly. Also, you will learn about two essential methods: label encoding and one-hot encoding for treating categorical variables. The dataset also contains many outliers, and it is not a good practice to train your model with those distant values. This big mart sales prediction machine learning project will thus help you in learning how to eliminate specific outliers from the dataset. Furthermore, you will learn how to perform and create the test-train dataset to apply machine learning algorithms.
Machine Learning Algorithms
As predicting the Big mart sales is a regression problem, this project will guide you on applying regression machine learning algorithms. You will learn about linear regression, the Bayesian model, ensemble bagging model like the random forest, boosting models, neural network, and MLP regressor. Additionally, the project will teach you about hyperparameter tuning for all the models that one can use to prepare the Bigmart sales prediction project report. This project is one of the excellent ML Projects for beginners to explore how machine learning and data science tools assist in solving large-scale business problems.
FAQs on Sales Prediction in Python
1) Which algorithm is best for sales prediction?
Various models are used for predicting sales. These include random forest, linear regression, ARIMA, LSTM, and XGBoost.Based on the given dataset, you can test any of these machine learning models for efficiency after hyperparameter tuning.
&nbsp;
2) How does Python predict future sales?&nbsp;
Python contains a sklearn library that contains various regression models like the random forest, linear regression, etc., that one can use for sales prediction. Another way to predict sales is to use Time series models for prediction.
&nbsp;"
6,Inventory Demand Forecasting using Machine Learning in R,"Business Overview
&nbsp;
Demand Forecasting is a process by which the future customer demand of any product is estimated by using the available data for the product. Demand forecasts are important to the most basic processes in any organization. To plan and deliver products and services is necessary to know what the future might hold. A demand forecast is important to plan all business decisions: sales, finance, production management, logistics and also marketing. To be able to predict next purchases is a valuable thing for any business. If we make the best forecast using data then it can help businesses to provide consumers with the right product at the right place at the right time which is very much important to business to get more profit margin.
&nbsp;
Aim
To Forecast Inventory demand using historical sales data of Grupo Bimbo bakery products in R.
&nbsp;&nbsp;
Data Description
The dataset used is from Mexican multinational company, Grupo Bimbo. The delivery chain is present in countries of America, Europe, Asia, and Africa. It has an annual sales volume of 15 billion dollars. Grupo Bimbo delivers fresh bakery products to 1 million stores along its 45,000 routes across Mexico. There are five datasets available. 
&nbsp;

train.csv: It contains data available for training. The dataset contains about 7.4 billion entries over 11 features. 
train.csv: It contains data available for testing. 
cliente_tabla.csv: It contains the data of 93 million clients over 2 different features.
producto_tabla.csv : It contains the data for 2592 products.
town_state.csv : It contains the data for 790 towns&nbsp;

Tech Stack

Language : R
Libraries : dplyr, ggplot2, caTools, xgboost, data.table, Matrix, lightgbm, gbm, caret, zoo, DataCombine, e1071

&nbsp;
Approach 
&nbsp;

Exploratory data analysis (EDA)

Data visualization
Inference about features
Feature engineering


&nbsp;

Data cleaning (outlier/missing values/categorical) 

Missing value detection 
Replacing special characters in data


&nbsp;

Feature Engineering

Adding lag columns for target variable
Adding moving average columns for target variable


&nbsp;

Model building on training data

XGBoost
GBM
SVM


&nbsp;

Model validation

RMSE


&nbsp;

Conclusion
"
7,Machine Learning Project to Forecast Rossmann Store Sales,"Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.

In this machine learning project, you will work on forecasting 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what&rsquo;s most important to them: their customers and their teams!&nbsp;"
8,Build an Azure Recommendation Engine on Movielens Dataset,"What is Dataset Analysis?&nbsp;
Dataset Analysis is defined as manipulating or processing unstructured data or raw data to draw valuable insights and conclusions that will help derive critical decisions that will add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.&nbsp;
Data Pipeline:&nbsp;
Data pipeline involves extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into a query-worthy format, visualizing KPIs, and orchestration of the above process. It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches.
What is the Agenda of the project?&nbsp;
The Agenda of the project involves deriving Movie Recommendations using Python and Spark on Microsoft Azure. We first understand the problem and download the Movielens dataset from the grouplens website. Then a subscription is set up for using Microsoft Azure, and categorization of resources is done into a resource group. A standard storage account is a setup to store all the data required for serving movie recommendations using Python and Spark on Azure, followed by creating a standard storage blob account in the same resource group. Firstly, we make containers in a standard storage account and standard storage blob account and upload the movielens zip file dataset in its standard storage blob account. Then we create an Azure data factory, a copy data pipeline, and start link storage for standard blob storage account in the Azure data factory. We are copying data from Azure blob storage to Azure data lake storage using a copy data pipeline in the Azure data factory. It is followed by creating the databricks workspace, cluster on databricks, and accessing Azure data lake storage from databricks. We are creating mount points and extracting the zip file to get CSV files. Finally, we upload files into databricks, read the datasets into Spark dataframes in databricks, and analyze the dataset to get the movie recommendations.
Usage of Dataset:&nbsp;
Here we are going to use Movielens data in the following ways:


Extraction: During the extraction process, the Movielens data zip file is extracted to get the CSV files out of it in two ways: the Databricks local file system(DFS) and the Azure data factory(ADF) copy pipeline.


Transformation and Load: During the transformation and load process, the uploaded dataset in Spark is read into Spark dataframes. Data tags are also read into Spark in Databricks, and output is displayed through Bar chart. And dataset is finally analyzed in Databricks into Spark, and movies are recommended.&nbsp;


Data Analysis:&nbsp;


From the grouplens website, data is downloaded containing names of movies, ratings given to the movies, links of the movies, and tags assigned to the movies.


Resource manager is created in Azure to categorize the resources required, followed by a Storage account.


The Copy Data pipeline is created to copy the data from Azure blob storage to Azure data lake storage in the Azure data factory.&nbsp;


The Databricks workspace and cluster are created and accessed Azure data lake storage from databricks followed by the creation of Mount pairs.&nbsp;


The extraction process is done by extracting the Movielens data zip file to get the CSV files out of it using the Databricks file system(DFS) and using the Azure data factory(ADF).&nbsp;


In the transformation and load process, the uploaded dataset in Spark is read into Spark dataframes. Data tags are read into Spark in Databricks.


Finally, data is analyzed into Spark in Databricks using mount points, and data is visualized using bar charts.

"
9,Bitcoin Data Mining on AWS Free Tier,"Why Big data?
Real time streaming data being captured at regular intervals of time from millions of IOT devices like sensors, clickstreams, logs from the device APIs and historical data from SQL databases. To store the huge volumes of data with high velocity and veracity, we need an efficient scalable storage system which is distributed across different nodes either in local or in cloud.&nbsp;Here comes the Hadoop concept which can be classified into two groups -Storage and processing.&nbsp;Storage will be done in HDFS and processing is done using Map reduce.
Data Pipeline: 
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real time (or streaming) instead of batches. Right from extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into query worthy format, visualisation of KPIs including Orchestration of the above process is data pipeline.
What we are going to do?
We are going to extract&nbsp;data from APIs using Python, parse it, save it to EC2 instance locally after that upload the data onto HDFS. Then reading the data using Pyspark from HDFS and perform analysis. The techniques we are going to use is Kyro serialisation technique and Spark optimisation techniques. An External table is going to be created on Hive/Presto and at last for visualizing the data we are going to use AWS Quicksight."
10,Ensemble Machine Learning Project - All State Insurance Claims Severity Prediction,"All State, a personal insurance company in the United States, is interested in leveraging data science to predict the severity and the cost of insurance claims post an unforeseen event.
This ensemble machine learning project will help you understand the best practices followed in approaching a data analytics problem through python language focusing on using data science packages. We will predict how severe insurance claims will be for All State. We accomplish this using ensemble machine learning algorithms.
We have also used TrueFoundry's ML Monitoring and Experiment tracking Solution known as MLFoundry to log the experiments, models, metrics, data &amp; features which can be used to generate informative dashboards and insights.&nbsp;
&nbsp;"
11,Web Server Log Processing using Hadoop in Azure,"Agenda
Data processing is a crucial step in understanding any data. As a part of this video series, we understand various features and techniques available in Hadoop to store data in distributed manner on HDFS. We then use&nbsp;Hive to create projection of data stored in HDFS, Flume to ingest data from external systems to HDFS and Spark and Scala to process and transform the NASA log data to gain insights.
Aim:
In this project, you will use Hadoop, Flume, Spark and Hive to process the Web Server logs dataset to get more insights on the log data. As part of this, you will create Azure Virtual Machine and install Hadoop, Flume, Spark, Scala and Hive to perform Flume agent execution, Build Scala code, submit Spark jobs and Hive Queries using the dataset.
Data Format:
The logs are an ASCII file with one line per request, with the following columns:


host making the request. A hostname when possible. Otherwise the Internet address.


Timestamp in the format ""DAY MON DD HH:MM:SS YYYY"", where DAY is the day of the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is the time of day using a 24-hour clock, and YYYY is the year. The timezone is -0400.


Request given in quotes.


HTTP reply code.


Bytes in the reply.


Tech Stack
➔ Language: Scala
➔ Services: Microsoft Azure, Hadoop, Hive, Flume, Spark
Scala
Scala is a multi-paradigm, general-purpose, high-level programming language. It's an object-oriented programming language that also supports functional programming. Scala applications can be converted to bytecodes and run on the Java Virtual Machine (JVM). Scala is a scalable programming language, and Javascript runtimes are also available.
Hive
Apache Hive is a fault-tolerant distributed data warehouse that allows for massive-scale analytics. Using SQL, Hive allows users to read, write, and manage petabytes of data. Hive is built on top of Apache Hadoop, an open-source platform for storing and processing large amounts of data. As a result, Hive is inextricably linked to Hadoop and is designed to process petabytes of data quickly. Hive is distinguished by its ability to query large datasets with a SQL-like interface utilizing Apache Tez or MapReduce.
Flume
Flume is a service for rapidly gathering, aggregating, and transporting massive amounts of log data that is distributed, reliable, and available. Its architecture is simple and adaptable, based on streaming data flows. It has configurable reliability techniques as well as several failovers and recovery mechanisms, making it resilient and fault tolerant. It employs a straightforward extensible data model that enables online analytic applications.
Approach


Sign-in to the Microsoft Azure account


Create a virtual machine






Select the tab to create a new VM


Add the basic configuration details to create a VM instance.






Connect to the Virtual machine






Download and install the putty application


Add the configuration details.






Install hadoop, hive, flume, scala, spark


Execute the Scripts in code.Zip step by step.


&nbsp;"
12,Hadoop Project-Analysis of Yelp Dataset using Hadoop Hive,"Business Overview
Big Data is the collection of huge datasets of semi-structured and unstructured data, generated by the high-performance heterogeneous group of devices ranging from social networks to scientific computing applications. Companies have the potential to gather large volumes of data, and they must guarantee that the data is in a highly useable shape by the time it reaches data scientists and analysts. Data engineering is the profession of creating and constructing systems for gathering, storing, and analyzing large amounts of data. It is a vast field with applications in almost every sector.
Apache Hadoop is a Big Data technology that enables the distributed processing of massive data volumes across computer clusters using simple programming concepts. It is intended to grow from a single server to thousands of computers, each supplying local computing and storage.
Yelp is a community review site and an American multinational firm based in San Francisco, California. It publishes crowd-sourced reviews of local businesses as well as the online reservation service Yelp Reservations. Yelp has made a portion of their data available in order to launch a new activity called the Yelp Dataset Challenge, which allows anyone to do research or analysis to find what insights are buried in their data. Due to the bulk of the data, this project only selects a subset of Yelp data. User and Review dataset is considered for this session.
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
Tech Stack
➔ Language: HQL
➔ Services: AWS EMR, Hive, HDFS, AWS S3
AWS EMR
Amazon EMR is a managed cluster platform that makes it easier to use big data frameworks like Apache Hadoop and Apache Spark to handle and analyze large volumes of data on AWS. You may process data for analytics and business intelligence tasks using these frameworks and related open-source projects. Amazon EMR also allows you to convert and transport huge volumes of data across AWS data storage and databases, such as Amazon S3 and Amazon DynamoDB.
Hive
Apache Hive is a fault-tolerant distributed data warehousing solution that enables massive-scale analytics. Using SQL, Hive allows users to read, write, and manage petabytes of data.
Hive is based on Apache Hadoop, an open-source system for storing and processing massive information. As a result, Hive is tightly linked with Hadoop and is built to handle petabytes of data fast. The ability to query massive datasets with a SQL-like interface, using Apache Tez or MapReduce, distinguishes Hive."
13,Predict Macro Economic Trends using Kaggle Financial Dataset,"Two Sigma is a technology company dedicated to finding value in the world&rsquo;s data. Since its founding in 2001, Two Sigma has built an innovative platform that combines extraordinary computing power, vast amounts of information, and advanced data science to produce breakthroughs in investment management, insurance, and related fields. Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty.

By accurately predicting financial movements, you will learn about scientifically-driven approaches to unlocking significant predictive capability.

Two Sigma is excited to find predictive value and gain a better understanding of the skills offered by the global data science crowd.
"
14,"Airline Dataset Analysis using Hadoop, Hive, Pig and Impala","Before data on any platform will become an asset to any organization, it has to pass through processing stage to ensure quality and availability. Afterward, that data has to be available to users (both human and system users). The availability of quality data in any organization is the guarantee of the value that data science (in general) will be to that organization.&nbsp;
We are using the airline on-time performance dataset (flights data csv) to demonstrate these principles and techniques in this hadoop project and we will proceed to answer the below&nbsp;questions -

When is the best time of day/day of week/time of year to fly to minimize delays?
Do older planes suffer more delays?
How does the number of people flying between different locations change over time?

We will also transform the data access model into time series and demonstrate how clients can access data in our big data infrastructure using a simple tool like the Excel spreadsheet."
15,Real-time Auto Tracking with Spark-Redis,"The era of IOT brought with it the need to stream data, process and sometimes display its information in real or near-real time.&nbsp;In this spark streaming&nbsp;project, we will be using a dataset that passes for real-time data sensor feeds for tracking auto vehicles around the city of Bejing. We will track each vehicle as the signal is received from our streaming simulation (using Flume). We will receive the streams of data using Spark Streaming and use the Redis as a pub/sub middleware.
Furthermore, we will use a java swing based application to display real-time information about all vehicles being tracked. While tracking the vehicle, we will be looking for indexes like current speed, total time and distance covered.
While this spark project is about tracking autos, the principles shared in this big data project will cover wide areas of implementing real-time sensor data processing and much more IOT."
16,Hive Mini Project to Build a Data Warehouse for e-Commerce,"Agenda
We will dig deeper into some of the Hive's analytical features for this hive project. Using SQL is still highly popular, and it will be for the foreseeable future. Most big data technologies have been modified to allow users to interact with them using SQL. This is due to the years of experience and expertise put into training, acceptance, tooling, standard development, and re-engineering. So, in many circumstances, employing these excellent SQL tools to access data may answer many analytical queries without resorting to machine learning, business intelligence, or data mining.
This big data project will look at Hive's capabilities to run analytical queries on massive datasets. We will use the Adventure works dataset in a MySQL dataset for this project, and we'll need to ingest and modify the data. We'll use Adventure works sales and Customer demographics data to perform analysis and answer the following questions:


Which age group of customers contribute to more sales?


To find the upper and lower discount limits offered for any product


Sales contributions by customer


To Understand customer persona purchasing pattern based on gender, education and yearly income


To find the sales contribution by customers on the overall year to date sales belong to categorized by same gender, yearly income.


To identify the top performing territory based on sales


To find the territory-wise sales and their adherence to the defined sales quota.


Aim
To perform Hive analytics on Sales and Customer Demographics data using big data tools such as Sqoop, Spark, and HDFS.
Data Description
Adventure Works is a free sample database of retail sales data. In this project, we will be only using Customer test, Individual test, Credit card, Sales order details, Store, Sales territory, Salesperson, Sales order header, Special offer tables from this database.
Tech Stack
➔ Language: SQL, Scala
➔ Services: AWS EC2, Docker, MySQL, Sqoop, Hive, HDFS, Spark
AWS EC2
Amazon EC2 instance is a virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture. Corporate customers can use the Amazon Elastic Compute Cloud (EC2) service to run applications in a computer environment. Amazon EC2 eliminates the need for upfront hardware investment, allowing customers to design and deploy projects quickly. Users can launch up to 10 virtual servers, configure security and networking, and manage storage on Amazon EC2.
Docker
Docker is a free and open-source containerization platform, and it enables programmers to package programs into containers. These standardized executable components combine application source code with the libraries and dependencies required to run that code in any environment.
MySQL
MySQL is a SQL (Structured Query Language) based relational database management system. The platform can be used for data warehousing, e-commerce, logging applications, etc.
Sqoop&nbsp;&nbsp;&nbsp;
Sqoop is a data transfer mechanism for Hadoop and relational database servers. It is used to import data from relational databases such as MySQL and Oracle into Hadoop HDFS, Hive, and export data from the Hadoop file system to relational databases.
Scala
Scala is a multi-paradigm, general-purpose, high-level programming language. It's an object-oriented programming language that also supports functional programming. Scala applications can be converted to bytecodes and run on the Java Virtual Machine (JVM). Scala is a scalable programming language, and JavaScript runtimes are also available.
Hive
Apache Hive is a fault-tolerant distributed data warehouse that allows for massive-scale analytics. Using SQL, Hive allows users to read, write, and manage petabytes of data. Hive is built on top of Apache Hadoop, an open-source platform for storing and processing large amounts of data. As a result, Hive is inextricably linked to Hadoop and is designed to process petabytes of data quickly. Hive is distinguished by its ability to query large datasets with a SQL-like interface utilizing Apache Tez or MapReduce.
Approach


Create an AWS EC2 instance and launch it.


Create docker images using docker-compose file on EC2 machine via ssh.


Create tables in MySQL.


Load data from MySQL into HDFS storage using Sqoop commands.


Move data from HDFS to Hive.


Integrate Hive into Spark.&nbsp;


Using Scala programming language, extract Customer demographics information from data and store it as parquet files.


Move parquet files from Spark to Hive.


Create tables in Hive and load data from Parquet files into tables.


Perform Hive analytics on Sales and Customer demographics data.

"
17,Online Hadoop Projects -Solving small file problem in Hadoop,"We have come to learn that Hadoop&#39;s distributed file system was engineered to favor fewer larger files over many small files. However, we mostly would not have control over how data come. Many data ingestion to data infrastructures come in small bits and whether we are implementing a data lake on HDFS or not, we will have to deal with this data inputs.

In this online hadoop project, we are going to be continuing the series on data engineering by discussing and implementing various ways to resolve the small file problem in hadoop.

We will start by defining what it means, how inevitable this situation could arise, how to identify bottlenecks in a hadoop cluster owing to the small file problem and varieties of ways to solve them.
"
18,Real-Time Streaming of Twitter Sentiments AWS EC2 NiFi,"


What is Twitter Sentiment?
Twitter sentiment is a term used to define the analysis of sentiments in the tweets generated by users on social media platform like Twitter. Generally, twitter sentiments are analysed in most of the projects using parsing. Analyzing sentiments of users on twitter is fruitful to companies for their product that is mostly focused on social media trends, users sentiments and future view of the online community.
Data Pipeline:
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real time (or streaming) instead of batches. Right from extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into query worthy format, visualisation of KPIs including Orchestration of the above process is data pipeline.
What is the Agenda of the project?
Agenda of the project involves Real-time streaming of Twitter Sentiments with visualization web app. We first launch an EC2 instance on AWS, and install Docker in it with tools like Apache Spark, Apache NiFi, Apache Kafka, Jupyter Lab, MongoDB, Plotly and Dash. Then, supervised classification model is created using Data exploration, Bucketizing, Stratified sampling, Dataset splitting, Extracting the features using tokenizing, removing stop words, TF-IDF etc., Creating Pipeline, Training the model, Evaluating model with binary classification evaluation and Saving classified model. It is followed by Extraction using Apache NiFi and Apache Kafka, followed by Transformation and Load using MongoDB and finally Visualizing it using python plotly and Dash with the usage of graph and table app call-back.
Usage of Dataset:
Here we are going to use Twitter sentiments data in the following ways:
- Extraction: During extraction process, NiFi process and connections are set up followed by creation of twitter app in twitter developer account. The data is streamed from the twitter API using NiFi followed by creation of topics and publishing tweets in NiFi using apache Kafka.
- Transformation and Load: During transformation and load process, schema is extracted from the stream of tweets followed by reading of data form apache Kafka as streaming a dataframe with extraction and cleansing of twitter data and analyzing sentiments in tweets. Then data is written in MongoDB for the visualization in Dash.
Data Analysis:


From given website, data is downloaded containing text of review, rating of product and summary of review. Data is bucketized to label features followed by partitioning of data to homogenous sample..


Dataset is splitted in appropriate ratios following by features extraction using tokenisation, TF-IDF and logistic regression.










Data pipeline is created to train the model and evaluate it with binary classification evaluator followed by saving of classified model.


The extraction process is done using NiFi and Kafka, by streaming data from twitter API using NiFi and creating topics, publishing tweets using Kafka.


In transformation and load process, schema is extracted from twitter streams and data is read from Kafka as streaming dataframe.


Twitter data is extracted and cleansed followed by sentiment analysis of tweets.


Finally continuous data is loaded into MongoDB and data is visualized using scatter graph and table definitions in python plotly and Dash.




"
19,Credit Card Fraud Detection as a Classification Problem,"Introduction to Credit Card Fraud Detection Project
Like any other technology that has been introduced in the world, the internet also comes with pros and cons. All of us enjoy the pros as the internet has changed our lifestyle by enhancing our communication. But, at the same time, we are witnessing digital frauds, which include fraudulent transactions through stolen credit cards. Credit card companies must identify fraudulent credit card transactions so that customers are not charged for items that they did not purchase. And this project is all about detecting such fraudulent transactions with the help of customers' attributes and transactions information.
&nbsp;
&nbsp;


Credit Card Fraud Detection Dataset
The dataset used contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced; the positive class (frauds) account for 0.172% of all transactions. The dataset has been collected and analyzed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universite Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BuFence and http://mlg.ulb.ac.be/ARTML.
&nbsp;
As the dataset was created using the PCA method, preprocessing of data has little scope. The imbalance between classes is compensated using oversampling and undersampling. The logistic regression, random forest, support vector machine, k-means are used within a cross-validation framework. Lastly, Recall and Accuracy are chosen as metrics while deducing the best classifier. A buffer section on outlier detection is added at the end.
&nbsp;
Learning Outcomes from the Fraud Detection Data Science Project
Here is a fun project to work on as it will help you realize the inclination of companies toward using machine learning algorithms for detecting credit card frauds. Let us explore the learning takeaways from this project in detail.
Exploratory Data Analysis
The dataset in this project does not have much information about what physical quantity each variable represents in this dataset except the two, amount, and time. Thus, analyzing the dataset using statistical tools is critical for such a dataset. You will learn how to draw statistical conclusions for all the variables. Additionally, the project solution will teach you how to create plots for visualizing the distribution of variables and deduce which variables play an essential role in segregating fraudulent and non-fraudulent transactions. The analysis will also assist you in concluding that the data is imbalanced and by what amount. Furthermore, you will learn plotting boxplots for visualizing outliers and evaluating the interquartile range of different features.
&nbsp;
Data Preparation
As the dataset is highly skewed towards non-fraudulent transactions, using classification algorithms in this project will reveal that one needs to use either undersampling or oversampling methods. This project will discuss both the ways in detail and assist you in understanding which technique will suit a particular problem. You will also learn how to prepare the data to implement algorithms of the scikit-learn library in Python. Additionally, you will learn about different methods for scaling the variables and outliers detection.
&nbsp;
Machine Learning Algorithms
This project is a beginner-friendly project on machine learning as it will teach you all the basics of this exciting domain. You will learn about the four types of machine learning problems: unsupervised learning, supervised learning, semi-supervised learning, and reinforcement learning. The project defines all these problems in detail with examples and various use cases. The goal is to detect which transactions are fraudulent or not, and this problem is an instance of a binary classification problem in supervised learning. To solve this problem, you will use algorithms like Random Forests, K-Nearest Neighbour, and Logistic Regression and deduce which is the best among them with the help of different statistical parameters like Precision, Recall, Accuracy, etc. Also, you will learn about preparing a credit card fraud detection project report with the help of classification metrics like the ROC curve, confusion matrix. The project will also assist you in understanding why accuracy is not an important metric for an imbalanced dataset. Furthermore, you will learn about using hyperparameter tuning techniques: GridSearchCV for undersampled data and RandomSearchCV for oversampled data.
&nbsp;
FAQs on the Credit Card Fraud Detection Data Science Project
1) Which are the best algorithms for credit card fraud detection?
The problem of credit card fraud detection is an example of a binary classification problem that can be solved using classification algorithms like Random Forests, Logistic Regression, Support Vector Machines, K-Nearest Neighbour, etc. You can analyze the performance of these algorithms using metrics like Recall, Precision, Accuracy, Confusion Matrix, ROC Curve, etc., and deduce which works best for your dataset.
&nbsp;
Another way of solving this problem is to treat this as an anomaly detection problem wherein the frauds are treated as anomalies. Then algorithms like Autoencoders, Isolation forest can be used to find out these anomalies.&nbsp;
&nbsp;
2) Why is Machine learning used in Credit Card Fraud Detection?
Machine learning algorithms do not assume the logic that differentiates fraudulent transactions from non-fraudulent ones. Rather, they leverage the transactions details and customers&rsquo; information to deduce the characteristics of fraudulent transactions. These algorithms are best suited to reveal the hidden patterns in the dataset and are therefore becoming a popular choice for solving problems like detecting credit card frauds."
20,Learn Data Processing with Spark SQL using Scala on AWS,"Agenda
Apache Spark is an open-source distributed processing solution for substantial data workloads. It combines in-memory caching and rapid query execution for quick analytic queries on any amount of data. It includes development APIs in Java, Scala, Python, and R. It allows code reuse across various workloads, including batch processing, interactive queries, real-time analytics, machine learning, and graph processing.
Scala is a multi-paradigm, general-purpose, high-level programming language. It's an object-oriented programming language that also supports functional programming. Scala applications can be converted to byte-codes and run on the Java Virtual Machine (JVM). Scala is a scalable programming language, and JavaScript run-times are also available. This project presents the fundamentals of Scala in an easy-to-understand manner.
&nbsp;
Aim
To understand the fundamentals of Scala in an easy-to-understand manner, also creating RDDs and performing transformation operations on them. This project also involves analyzing the Movies dataset using RDD and Spark SQL.
&nbsp;
Data Description
In the project, we will use Movies and Rating datasets. The Movies dataset contains movie id, title, release date, etc. The Rating dataset contains customer id, movie id, ratings, and timestamp information.
&nbsp;
Tech Stack
➔ Language: SQL, Scala
➔ Services: AWS EC2, Docker, Hive, HDFS, Spark
&nbsp;
AWS EC2
Amazon EC2 instance is a virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture. Corporate customers can use the Amazon Elastic Compute Cloud (EC2) service to run applications in a computer environment. Amazon EC2 eliminates the requirement for upfront hardware investment, allowing customers to design and deploy projects quickly. Users can launch as many or as few virtual servers as they like, configure security and networking, and manage storage on Amazon EC2.
&nbsp;
Docker
Docker is a free and open-source containerization platform, and it enables programmers to package programs into containers. These standardized executable components combine application source code with the libraries and dependencies required to run that code in any environment.
&nbsp;
Scala
Scala is a multi-paradigm, general-purpose, high-level programming language. It's an object-oriented programming language that also supports functional programming. Scala applications can be converted to byte-codes and run on the Java Virtual Machine (JVM). Scala is a scalable programming language, and JavaScript run-times are also available.
&nbsp;
Hive
Apache Hive is a fault-tolerant distributed data warehouse that allows for massive-scale analytics. Using SQL, Hive allows users to read, write, and manage petabytes of data. Hive is built on top of Apache Hadoop, an open-source platform for storing and processing large amounts of data. As a result, Hive is inextricably linked to Hadoop and is designed to process petabytes of data quickly. Hive is distinguished by its ability to query large datasets with a SQL-like interface utilizing Apache Tez or MapReduce.
&nbsp;
Approach


Create an AWS EC2 instance and launch it.


Create docker images using docker-compose file on EC2 machine via ssh.


Load data from local machine into Spark container via EC2 machine.


Perform analysis on Movie and Ratings data.

"
21,Spark Project -Real-time data collection and Spark Streaming Aggregation,"In this spark project, we will embark on real-time data collection and aggregation from a simulated real-time system.

The dataset for the project which will simulate our sensor data delivery is from Microsoft Research Asia GeoLife project. According to the paper, the dataset recoded a broad range of users&rsquo; outdoor movements, including not only life routines like go home and go to work but also some entertainments and sports activities, such as shopping, sightseeing, dining, hiking, and cycling. This trajectory dataset can be used in many research fields, such as mobility pattern mining, user activity recognition, location-based social networks, location privacy, and location recommendation.

As a part of this big data project, we will use the data to provide real time aggregates of the movements along a number of dimension like effective distance, duration, trajectories and more. All streamed data will be stored in the NoSQL database - HBase.
"
22,Time Series Forecasting with LSTM Neural Network Python,"Introduction to LSTM Time Series Forecasting in Python
Deep learning is an upcoming field where we see a lot of implementations in the day-to-day business operations, including segmentation, clustering, forecasting, prediction or recommendation, etc. These exciting implementations are realized because of the variety of deep learning architectures that scientists and researchers have developed. One of such models used for time series forecasting is the LSTM model, and it is a particular type of neural network algorithm that we will discuss in this project.
Project Overview: Time Series Forecasting using LSTM in Python
Deep learning architecture has many branches, and one of them is the recurrent neural network (RNN). The method we will analyze in this deep learning project is Long Short Term Memory Network (LSTM) to perform time series forecasting for univariate time series data.
&nbsp;

&nbsp;
As LSTM is a slightly advanced deep learning algorithm, the project will first introduce simpler neural network algorithms like perceptron to help you understand various neural-networks-related jargon. After that, you will learn different deep learning architectures and utilize LSTM for time series forecasting.
Dataset
The dataset used in this LSTM-python project is an airline&rsquo;s passengers' data. There are two columns available, one column contains the year and month to represent time, and the other column has information about the number of passengers that traveled in that month.
Tech Stack
Language: Python
Libraries: Pandas, NumPy, Matplotlib, Math, Theano, Keras, scikit-learn
Data Science Concepts Explored in this RNN For Time Series Forecasting Python Project
Here is a list of exciting topics we will cover in this LSTM forecasting Python project.
Deep Learning Architectures
This project covers popular deep learning architectures like deep neural networks, convolutional neural networks, recurrent neural networks, deep belief networks, and the Boltzmann networks. After introducing the basics of these architectures, it will cover essential elements like activation functions, perceptron elements, bias terms, etc. Learning about these elements will help you understand the art of fine-tuning different deep learning algorithms. Additionally, it will assist you in learning how each deep learning algorithm is different from one another.
Setting up the Project Environment
As this RNN time-series forecasting python project will teach you about implementing the LSTM model from scratch, you must know how to install the necessary libraries in your system. And you don&rsquo;t need to worry about it because the project has a detailed guide for installing all the prerequisite libraries. Additionally, you will learn how to set Theano as the backend library of the Keras framework in Python.
Data Preprocessing
You will learn how to normalize the variables in the dataset using the Python library: sklearn&rsquo;s functions like MinMaxScaler and StandardScaler. Additionally, you will know how to split the dataset into the test and train subsets and prepare it for the application of deep learning algorithms.
Implementing LSTM in Python for Time Series Forecasting
The Keras framework in Python allows its users to create deep learning models from scratch. In this time series forecasting LSTM python project, you will create all the layers of the LSTM-RNN model using Keras and make predictions for the number of passengers that will fly in the coming years. Furthermore, you will use statistical tools to evaluate the model&rsquo;s accuracy.
FAQs on Python LSTM Time Series Forecasting
Is LSTM good for Time Series Forecasting?
Yes, LSTM is a good option for forecasting time series data as it is a sequential deep learning model that considers all the values in a given sequence.&nbsp;
When should I use an RNN LSTM and when to use ARIMA for a time series forecasting problem?
RNN-LSTM model works best for those time series forecasting problems where the relationship between the feature variables and the target variable is non-linear. On the other hand, the ARIMA model works best for situations where the relationship is linear. Additionally, depending on the dataset size, one can pick among the two as LSTMs are computationally expenisve and may take time for large data while ARIMA can be faster in such cases."
23,Yelp Data Processing Using Spark And Hive Part 1,"Data engineering is the science of acquiring, aggregating or collection, processing and storage of data either in batch or in real time as well as providing variety of means of serving these data to other users which could include a data scientist. It involves software engineering practises on big data.

In this big data project for beginners, we will continue from a previous hive project on &quot;Data engineering on Yelp Datasets using Hadoop tools&quot; where we applied some data engineering principles to the Yelp Dataset in the areas of processing, storage and retrieval. Like in that session, We will not include data ingestion since we are already downloading the data from the yelp challenge website. But unlike that session, we will focus on doing the entire data processing using spark.
"
24,PySpark Tutorial - Learn to use Apache Spark with Python,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. For quick analytic queries against any quantity of data, it uses in-memory caching and efficient query execution. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.&nbsp;
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
The NYC yellow taxi trip records data will be used to implement this project. A Few of the fields from the dataset of 17 parameters include:


VendorID


tpep_pickup_datetime


tpep_dropoff_datetime


&nbsp;passenger_count


trip_distance


total_amount


&nbsp;
Tech Stack
➔ Language: Python3
➔ Library: PySpark
➔ Services: Jupyter Notebook
"
25,Credit Card Anomaly Detection using Autoencoders,"What is anomaly detection?
Anomaly detection (aka outlier analysis) is a step in data mining that identifies data points, events, and/or observations that deviate from a dataset&rsquo;s normal behavior. Anomalous data can indicate critical incidents, such as a technical glitch, or potential opportunities, for instance, a change in consumer behavior.
Applications of Anomaly detection


Banking, Financial Services, and Insurance (BFSI) &ndash; In the banking sector, some of the use cases for anomaly detection are to flag abnormally high transactions, fraudulent activity, and phishing attacks.


Retail &ndash; In Retail, anomaly detection is used for processing large volumes of financial transactions to identify fraudulent behaviors, such as identity theft and fraudulent credit card usage.


Manufacturing &ndash; In Manufacturing, anomaly detection can be used in several important ways, such as identifying machines and tools that are underperforming, which can take months to find without anomaly detection technology.


IT and Telecom &ndash; In IT and Telecommunications, anomaly detection is increasingly valuable to detect and act on personal threats to users, financial threats to service providers, or other types of unexpected threats.


Defense and Government &ndash; In the Defence and Government setting, anomaly detection is best used for identifying excessive and fraudulent government spending, budgeting, and audits. This can save governments an immense amount of money.


Healthcare &ndash; In Health Care, anomaly detection is used for its application in a crucial management task that can improve the quality of the health services and avoid loss of huge amounts of money. In terms of identifying fraudulent claims from hospitals and on the side of the insurance providers.


Tech Stack&nbsp;


Language used: R


Machine Learning interface: H2O


Other packages used: caret, e1071, ROCR, and many more


Dataset Overview
In this project, we will be using a credit card fraud dataset that represents fraudulent and legal transactions over a certain period. The data is available in a .csv format. In the dataset, we can see that&nbsp;most of the column names (V1 to V28)&nbsp; are not mentioned explicitly. This is because PCA (Principal Component Analysis)&nbsp; transformation has been performed on the original dataset&nbsp;to maintain the confidentiality of the data. Apart from these variables, we have a few explicit variables as follows


Time - Difference in seconds between each transaction and its previous transaction


Amount - Transaction Amount


Class&nbsp;


0 - Non-fraudulent transaction


1 - Fraudulent Transaction




Approach


Business context and objective&nbsp;


Translating into Data Science approach


What, why, where Anomaly Detection?


Why we are using a fraud dataset for this problem


Algorithms used to solve this problem




Data importing and Data Understanding


Data Preprocessing&nbsp;


&nbsp;Creating time variable




EDA


Preparing data for modelling


Understanding neural networks and deep neural networks


Understanding Autoencoders&nbsp;


Unsupervised Learning using h2o


Building model and Model Details




Evaluation parameters understanding


Evaluating based on Reconstructed MSE




Supervised Learning using h2o&nbsp;


Building and tuning supervised learning model using H2O




Transfer learning&nbsp;


Supervised Learning using Pretrained model and evaluation




Precision-recall curve


Try different thresholds to improve accuracy




Making production-ready code

"
26,Yelp Data Processing using Spark and Hive Part 2,"In the previous&nbsp;Spark Project on the same subject- Yelp Data Processing Using Spark And Hive Part 1,&nbsp;we began the development of Yelp dataset into domains that can easily be understood and consumed. Amongst other things we did


	Various ways to ingest data using spark
	Data transformation using Spark
	Various ways of integrating spark and hive
	Denormalize&nbsp;dataset into the hive tables thereby creating multiple datasets
	Discuss how to handle snapshots and incremental data loads


In this Spark project, we are going to continue building the data warehouse. The purpose in this big data project is to do further data processing to deliver different kinds of the data products.
"
27,House Price Prediction Project using Machine Learning in Python,"Introduction to the House Price Prediction using Machine Learning Project
A home is often the most expensive purchase a person makes in their lifetime, and ensuring homeowners have a trusted way to monitor this asset is critical. Zillow's Zestimate was created to give buyers as much information as possible about homes and the housing market, marking the first time they had access to this type of home value information at no cost.
This project aims to build a machine learning model that can predict the log error between the Zestimate and the actual sale price. This house price prediction project will help you predict the price of houses based on different features and house properties.
We have also used TrueFoundry's ML Monitoring and Experiment tracking Solution known as MLFoundry to log the experiments, models, metrics, data &amp; features which can be used to generate informative dashboards and insights.&nbsp;
Overview of the Zillow House Price Prediction ML Project
We use the Zillow dataset to build our prediction model for this project. Given the attributes, the project entails predicting the log error between the Zillow Zestimate and the actual sale price. We create a prediction model for improving the Zestimate residual error using machine learning techniques.
Zillow Dataset Kaggle
The dataset contains two CSV files, which have around 60 features based on which log error (target) has to be predicted.
We combine the two datasets to form a single dataset containing all the featured properties and the target variable, the &lsquo;logerror&rsquo;.The final dataset has around 90000 rows and 60 columns. After that, we examine the final dataset for-


Missing values


Numerical variables


Outliers


Distribution of numerical variables


Categorical variables


The cardinality of categorical variables


Potential relationships between variables and the target (sale price/log error)


&nbsp;
Aim of the House Price Prediction Project
In this python house price prediction project we will build a Regression model to predict the sale prices of the houses and improve the log error i.e. the error due to the difference between the actual and the predicted home values.&nbsp;
You can calculate the logerror as-
log error= log(Zestimate) - log(SalePrice)
Tech stack


Language - Python


Libraries - Scikit-learn, pandas, numpy, matplotlib, seaborn, scipy, xgboost, joblib


House Price Prediction Machine Learning Project Source Code


Importing the required libraries and reading the dataset.




Merging of the two datasets


Understanding the dataset




Exploratory Data Analysis (EDA)&nbsp;




Data Visualization




Feature Engineering




Duplicate value removal


Missing value imputation


Rescaling of incorrectly scaled data


Standardization


Encoding of categorical variables


Generation of new feature wherever required


Dropping of redundant feature columns


Checking for multi-collinearity and removal of highly correlated features


Handling outliers




Model Building




Performing train test split


Feature Scaling


Dropping features if necessary


Linear Regression Model


Elastic Net


Ridge Regression


Lasso Regressor


XGBoost Regressor


Adaboost Regressor


Gradient Boosting Regressor


Decision Tree Regressor


Random Forest Regressor




Model Validation




Mean Absolute Error




Hyperparameter Tuning (GridSearchCV)




For Random Forest Regressor




Checking for Feature Importance


Creating the final model and making predictions



Key Concepts in the Project
This project is one of the best machine learning project ideas for beginners. It introduces you to various machine learning concepts and helps you strengthen your fundamental knowledge of machine learning.
Exploratory Data Analysis
In this machine learning projects for beginners, you will plot a bar chart between each variable with respect to the target variable and find the total number of variables with missing values in the dataset. The next step in the EDA is to check the cardinality of the categorical variables, i.e., the uniqueness of each category. This helps to determine the percentage of observations in each category and obtain the rare categories. After Identifying the temporal variables we will plot scatter plot to depict their relation with respect to the target variable.Also, you will learn how to analyze the outliers using box plots.
You will perform EDA with the help of various Python libraries such as NumPy, Pandas, etc.
Feature Engineering
This predicting project in machine learning will teach you how to perform Feature Engineering on the final dataset obtained after the EDA. This step helps to analyze the features present in the dataset . Additionally, you will learn to visualize the multi-collinearity between features by plotting a heatmap using the Seaborn library in Python. You will learn how to prepare the dataset to be fed to Machine Learning algorithms by performing a train test split on the data followed by feature scaling.
Model Building and Prediction
For training the prediction model, you&rsquo;ll work with various machine learning models such as Linear regression, Random Forest Regressor, XGBoost Regressor, etc. You will pass the test dataset through each of these models to determine which one gives the best results. You will calculate the mean absolute error, mean squared error, and the root mean squared error for each model to find the best model.
"
28,Data Science Project - Instacart Market Basket Analysis,"Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering&nbsp;and delivery app aim to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.
Instacart&rsquo;s data science team plays a big part in providing this delightful shopping experience. Currently, they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open-sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced.
In this data science project, we are going to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user&rsquo;s next order."
29,Hadoop Project to Perform Hive Analytics using SQL and Scala,"Agenda
We will dig deeper into some of Hive's analytical features for this hive project. Using SQL is still highly popular, and it will be for the foreseeable future. Most big data technologies have been modified to allow users to interact with them using SQL. This is due to the years of experience and expertise put into training, acceptance, tooling, standard development, and re-engineering. So, in many circumstances, employing these excellent SQL tools to access data may answer many analytical queries without resorting to machine learning, business intelligence, or data mining.
This big data project will look at the Hive capabilities that let us run analytical queries on massive datasets. We will be using the adventure works dataset in a MySQL dataset for this project. As a result, before we can go on to analytics, we'll need to ingest and modify the data.
Aim
To perform Hive analytics on Customer Demographics data using big data tools such as Sqoop, Spark, and HDFS.
Data Description
Adventure Works is a free sample database of retail sales data. In this project, we will be only using Customer test, Individual test, and Credit card tables from this database.
Customer test table contains data like Customer ID, Territory ID, Account number, Customer Type etc. Individual test table contains data like Customer ID, Contact ID and Demographics. Credit card table contains data like Credit card ID, Card type, Card number, Expiry month, Expiry year.
Tech Stack
➔ Language: SQL, Scala
➔ Services: AWS EC2, Docker, MySQL, Sqoop, Hive, HDFS, Spark
AWS EC2
Amazon EC2 instance is a virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture. Corporate customers can use the Amazon Elastic Compute Cloud (EC2) service to run applications in a computer environment. Amazon EC2 eliminates the requirement for upfront hardware investment, allowing customers to design and deploy projects quickly. Users can launch as many or as few virtual servers as they like, configure security and networking, and manage storage on Amazon EC2.
Docker
Docker is a free and open-source containerization platform, and it enables programmers to package programs into containers. These standardized executable components combine application source code with the libraries and dependencies required to run that code in any environment.
MySQL
MySQL is a SQL (Structured Query Language) based relational database management system. The platform can be used for data warehousing, e-commerce, logging applications, etc.
Sqoop&nbsp;&nbsp;&nbsp;
Sqoop is a data transfer mechanism for Hadoop and relational database servers. It is used to import data from relational databases such as MySQL and Oracle into Hadoop HDFS, Hive, and export data from the Hadoop file system to relational databases.
Hive
Apache Hive is a fault-tolerant distributed data warehouse that allows for massive-scale analytics. Using SQL, Hive allows users to read, write, and manage petabytes of data. Hive is built on top of Apache Hadoop, an open-source platform for storing and processing large amounts of data. As a result, Hive is inextricably linked to Hadoop and is designed to process petabytes of data quickly. Hive is distinguished by its ability to query large datasets with a SQL-like interface utilizing Apache Tez or MapReduce.
"
30,Log Analytics Project with Spark Streaming and Kafka,"What is Log Analysis?
The process of evaluating, understanding, and comprehending computer-generated documents known as logs is known as log analysis. A wide range of programmable technologies, including networking devices, operating systems, apps, and more, produce logs. A log is a collection of messages in chronological order that describe what is going on in a system. Log files can be broadcast to a log collector over an active network or saved in files for later analysis. Regardless, log analysis is the subtle technique of evaluating and interpreting these messages in order to get insights&nbsp;into any&nbsp;system's underlying functioning. Web server log analysis can offer important insights on everything from security to customer service to SEO. The information collected in web server logs can help you with:

Network troubleshooting efforts
Development and quality assurance
Identifying and understanding security issues
Customer service
Maintaining compliance with both government and corporate policies

&nbsp;The common log-file format is as follows:
&nbsp;&nbsp;&nbsp; remotehost rfc931 authuser [date] ""request"" status bytes
Data Pipeline:&nbsp;
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches. Right from extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into query worthy format, visualization of KPIs including Orchestration of the above process is data pipeline.&nbsp;
What is the agenda of the project?&nbsp;
The agenda of the project involves Real-time log analysis with the visualization web app. We first launch an EC2 instance on AWS and install Docker in it with tools like Apache Spark, Apache NiFi, Apache Kafka, Jupyter Lab, Plotly and Dash. Then, we perform preprocessing on sample data, parse it into individual columns, cleaning the data and formatting timestamp. It is followed by Extraction of NASA access log dataset using Apache NiFi and Apache Kafka, followed by Transformation and Load using Cassandra and HDFS and finally Visualizing it using Python Plotly and Dash with the usage of graph and table app call-back.&nbsp;
Usage of Dataset:&nbsp;
Here we are going to use NASA access log data in the following ways:&nbsp;

Extraction: During the extraction process, the downloaded dataset from Kaggle &nbsp;is ingested using NiFi processors and connections. The data is streamed from the data file using NiFi followed by the creation of topics and publishing logs using Apache Kafka.&nbsp;
Transformation and Load: During the transformation and load process, we read data from Apache Kafka as streaming&nbsp; Dataframe according to schema creation with extraction and cleansing of log data and loading to Cassandra for Speed layer and HDFS for Batch layer. Then data is visualized using Plotly in Dash.&nbsp;

Architecture Diagram:
"
31,Machine Learning or Predictive Models in IoT - Energy Prediction Use Case,"This IoT project presents and discusses data-driven predictive models for the energy use of appliances. Data used include measurements of temperature and humidity sensors from a wireless network, whether from a nearby airport station and recorded energy use of lighting fixtures. The machine learning project discusses data filtering to remove non-predictive parameters and feature ranking.&nbsp;The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru) and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non-predictive attributes (parameters).&nbsp;
"
32,Time Series Analysis Project in R on Stock Market forecasting,"Business Objective
&nbsp;
The main objective of this problem is to forecast stock market data using traditional and advanced state of the art algorithms. EuroStockMarket Dataset contains the daily closing prices of major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. The data are sampled in business time, i.e., weekends and holidays are omitted. The stock market can have a huge impact on the people and the countries economy as a whole and hence predicting the prices of stock can reduce the risk of loss and maximize the profit.
&nbsp;

&nbsp;
Aim
To predict the stock price
&nbsp;
&nbsp;
Data Overview
The data is from the EU Stock market with the following columns with a time index.

DAX&nbsp;&nbsp;&nbsp; - Germany DAX Stock index
SMI&nbsp;&nbsp;&nbsp;&nbsp; - Switzerland SMI Stock index
CAC&nbsp;&nbsp; - France CAC Stock index
FTSE - UK Stock index

&nbsp;
&nbsp;
Tech Stack

Language used : R
Libraries used : FitAR, tseries, forecast, neuralnet and so on.

&nbsp;
&nbsp;
Approach
&nbsp;

Data cleaning / Pre-processing (outlier/missing values/categorical) - 

&nbsp;

Extract the trend, seasonality and random terms from the model
Decompose the time series
Identify autocorrelation and partial autocorrelation
Detrend a time series

&nbsp;
2. Holt winter Method

Understanding holt winter method
Building model and tuning
Evaluation parameters understanding
Evaluation

&nbsp;
3. ARIMA method

Understanding ARIMA method
Building model and tuning
Evaluation

&nbsp;
&nbsp;
4. VAR method

Understanding VAR method
Building model and tuning
Evaluation

&nbsp;
5. Neural Network method

Understanding neural network method
Building model and tuning
Evaluation

&nbsp;
6. Finalise Model 

Which model will you finalize on and based on what metrics

&nbsp;
7. Comparison and other potential approaches
&nbsp;
8. Deployment 

Making production ready code
"
33,Explore features of Spark SQL in practice on Spark 2.0,"Spark 2 offers a huge but yet backward-compatible break from the Spark 1.x, not only in terms of high-level API but also in performance. And spark the module with the most significant new features is Spark SQL.
In this apache spark project, we will explore a number of this features in practice.
We will discuss using various dataset, the new unified spark API as well as the optimization features that makes Spark SQL the first way to explore in processing structured data.
However, there are times when it is inevitable to resort to Spark Core - RDD in Spark 2. We will explore that as well alongside the newest and cool structured streaming API that enables fault-tolerant stream processing engine built on the Spark SQL engine."
34,Data Processing and Transformation in Hive using Azure VM,"Business Overview
Big Data is a collection of massive quantities of semi-structured and unstructured data created by a heterogeneous group of high-performance devices spanning from social networks to scientific computing applications. Companies have the ability to collect massive amounts of data, and they must ensure that the data is in highly usable condition by the time it reaches data scientists and analysts. The profession of data engineering involves designing and constructing systems for acquiring, storing, and analyzing vast volumes of data. It is a broad field with applications in nearly every industry.
Apache Hadoop is a Big Data solution that allows for the distributed processing of enormous data volumes across computer clusters by employing basic programming techniques. It is meant to scale from a single server to thousands of computers, each of which will provide local computation and storage.
Apache Hive is a fault-tolerant distributed data warehouse system that allows large-scale analytics. Hive allows users to access, write, and manage petabytes of data using SQL. It is built on Apache Hadoop, and as a result, it is tightly integrated with Hadoop and is designed to manage petabytes of data quickly. Hive is distinguished by its ability to query enormous datasets utilizing a SQL-like interface and an Apache Tez, MapReduce, or Spark engine.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
In this project, we will use the Airlines dataset to demonstrate the issues related to massive amounts of data and how various Hive components can be used to tackle them. Following are the files used in this project, along with a few of their fields :


airlines.csv - IATA_code, airport_name, city, state, country


carrier.csv - code, description


plane-data.csv - tail_number, type, manufacturer, model, engine_type


Flights data (yearly) - flight_num, departure, arrival, origin, destination, distance


&nbsp;
Tech Stack
➔ Language: HQL
➔ Services: Azure VM, Hive, Hadoop
&nbsp;"
35,Build a Music Recommendation Algorithm using KKBox's Dataset,"Introduction to Music Recommendation System&nbsp;
Music is one of the most popular sources of entertainment today. Listening to music has become much easier due to the digital revolution. A few years ago, many users used to listen to a particular artist or band; some used to love specific types of music. However, as the world is getting more and more connected through technology, users have started gaining access to various genres of music on different platforms. Nowadays, the availability of music and music streaming services has increased exponentially, and the public can easily listen to all kinds of music ranging from classical, jazz to pop.
&nbsp;
Music streaming applications such as Spotify, youtube music, amazon music have features to recommend music to the users based on their listening history and preferences. Such features play a vital role in the business of these streaming services. As the time spent on the platform is directly linked to the growth of the streaming services, appropriate recommendations are essential. The music recommendation system by which the music provider can predict and suggest the right songs based on the characteristics of the music the user has heard over time.
&nbsp;
Due to the increasing number of songs, artists, and music, it has become challenging to suggest appropriate music pieces to the user. The challenge of a music recommendation system is to build a system that can understand the users&rsquo; preferences and offer the songs. Therefore, many music streaming service providers rely on data scientists to use their excellent mathematical tools and develop more efficient recommendation systems.

Overview of Music Recommendation System Project using Machine Learning
We use the KKBOX dataset to build a music recommendation system in this project. This music recommendation app project will walk you through some Machine learning techniques that one can apply to recommend songs to users based on their listening patterns. To predict the chance of a user listening to a piece of music repetitively after the first observable listening event within a particular time.&nbsp;
Music Recommendation Dataset
The dataset used is from Asia&rsquo;s leading music streaming service, KKBOX. It holds the world&rsquo;s most comprehensive Asia-Pop music library with over 30 million tracks. The training data set contains the first observable listening event for each unique user-song pair within a specific time duration. Metadata of each user and song pair is also provided. There are three datasets available.
train.csv: It contains data for different users with attributes such as msno, user_id, song_id, source_system_tab, etc. There are about 7.3 million entries available with 30755 unique user ids.
songs.csv: It contains the data related to songs with attributes such as song_id, song_length, genre_ids, artist_name, etc. The dataset contains about 2.2 million unique song ids.
members.csv: The data is related to users' information over 34403 different users.
Tech Stack for the Music Recommendation Project
Language: Python
Libraries: sklearn, xgboost, pandas, NumPy
An Outline of the Music Recommendation System Source Code


Exploratory data analysis (EDA)


Data visualization


Inference about features


Feature engineering&nbsp;






Data cleaning (outlier/missing values/categorical)


Outlier detection and treatment


Imputing missing values


Replacing by mode


Removing null values


Making a new label as missing




Converting labeled or string values by numerical values






Model building on training data


Logistic regression


Decision Tree


Random Forest


XGBoost






Model validation


Roc_Auc






Feature importance and conclusion


Learning Takeaways from Music Recommendation System using Machine Learning Project
This project can easily make it to the list of top machine learning projects for beginners because of the simple tools and techniques used to implement a music recommendation system in Python. Here are details of the machine learning tools and techniques used in this project.
Exploratory Data Analysis
The dataset for the music recommender system project has about 3 million rows, and such large-scale data can be easily analyzed using Pandas dataframes in Python. The analysis involves understanding app-user behavior and, more precisely, what makes a user listen to songs again and again. We will achieve this by plotting insightful plots using Python libraries, matplotlib, and seaborn. For this project though, we will be using the first 10,000 rows only.
Data Cleaning
The music recommendation system dataset has a lot of missing values that must be treated mathematically before serving the values as an input to a machine learning model. This project will help you learn three powerful techniques to handle null values in the data. You will also learn how to handle non-numerical data and treat outliers in the dataset. Additionally, you will learn how to perform feature engineering over the dataset and prepare it to apply machine learning algorithms.
Machine Learning Algorithms
The task in this music recommendation system using python project simplifies predicting the value of a target variable which takes value '1' if the user listened to a particular song and '0' if they didn&rsquo;t. It helps design the recommendation system as songs rows that correspond to the target value = &lsquo;1&rsquo; are likely to be heard by the user and should be recommended more often. As the prediction problem falls under the umbrella of binary classification problems, you will explore classification machine learning algorithms: decision tree, logistic regression, XGBoost, and Random forests. After their implementation, you will learn how to compare the performance of different algorithms using statistical scores.
FAQs on Music Recommendation Systems
Here are a few of the most popular questions that one is likely to ask when exploring music recommendation systems.
1) How Music Recommendation works?
The best multi-touch attribution model will be the one that works the best for your dataset. A multi-touch attribution model is a model which changes from business to business, lineage to lineage. It solely depends on which level of touch you are attributing a conversion. Some of the famous multitouch attribution models are:&nbsp;
&nbsp;


Linear Multi-Touch Marketing Attribution Model


U-Shaped Multi-Touch Marketing Attribution Model


Time Decay Multi-Touch Marketing Attribution Model


W-Shaped Multi-Touch Marketing Attribution Model


2) Which is the best music recommendation algorithm?&nbsp;


Choose the multi-touch attribution model that you believe will suit your dataset by analyzing which channels were prominent in serving as the point of conversion.&nbsp;


&nbsp;If you conclude a probabilistic model works the best, then follow the steps for a Shapely model or any other Position Decay model.


&nbsp;If you deduce that the first and last steps of the conversion process are the most important, then will follow the steps for a Position Based attribution model.&nbsp;


Use a programming language like Python to define a function that takes your data as the input. The function must filter the conversions from the data, store the corresponding cookie IDs, aggregate the click counts and then distribute them according to the chosen model.


Once the model has been designed, you can use it to analyze the influence of each channel.

"
36,Implementing Slow Changing Dimensions in a Data Warehouse using Hive and Spark,"One of the broadest use of Hadoop today is building data warehousing platform off a data lake. And in building a data warehouse, the traditions left us by Kimball and Inmon is still very much in play.

Why not every one of the legacy rules should be implemented as as-is in the big data platform, the issue of slow-changing dimensions is still a front-burner.

The slow changing dimension of warehouse dimension that is said to rarely change. However, when they change, there should be a systematic approach to capturing that change. Examples of SCDs are customer and products information.

In this hive project, we will look at the various types of SCDs and learn to implements SCDs in Hive and Spark.
"
37,Predict Churn for a Telecom company using Logistic Regression,"Every company wants to increase its revenue and profitability. To do that, while they acquire new customers, they also want to make sure that the existing ones stay with them for a long term. Also, its strategically important to know beforehand whether a set of customers are planning to stop using their services (especially recurring ones like internet, cable, phone etc.). To do that, every company or business creates and tracks customer metrics which are then used to predict their likelihood of churn.
Customer Churn for a company occurs when a customer decides to stop using the services of that company. In this project, we will be using the customer data of a telecom sector company based in the US to predict the probability of churn for each of the customer. We will look at the standard practices that are followed in the industry to solve these problems and also go beyond just those techniques. We have chosen the telecom company data for the churn problem as it is a major area of concern for companies in that sector.
Once we have built a model, the churn model output can also be used as a warning indicator that some customers are likely to churn. The key drivers that are making the customer more likely to churn can be alleviated and ensure that the customers are actually retained."
38,Machine learning for Retail Price Recommendation with R,"Business Objective
Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.
Mercari, Japan&rsquo;s biggest community-powered shopping app, knows this problem deeply. They&rsquo;d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace.
In this project, Mercari&rsquo;s challenging us to build an algorithm that automatically suggests the right product prices.
Data Overview
There are two files available. They are train.tsv and test.tsv
Both are tab separated files
The following are the data fields

train_id or test_id - the id of the listing
name - the title of the listing. Note that we have cleaned the data to remove text that looks like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]
item_condition_id - the condition of the items provided by the seller
category_name - category of the listing
brand_name
price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.
shipping - 1 if shipping fee is paid by seller and 0 by buyer
item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]

Source &nbsp;https://www.kaggle.com/c/mercari-price-suggestion-challenge/overview/description
Aim 
To predict the price of the product using the given description and other information
Tech Stack

Language used : R
Packages used : superml, textstem, neuralnet, gbm, quantenda, and so on..
UI support : R Shiny Dashboard

Approach 

Exploratory Data Analysis

Exploratory data analysis is the process of analysing the dataset to understand its characteristics. In this step, we perform the following.


Univariate analysis - Analysis of a single variable
Bivariate analysis - Analysis of relationship between two variable



Data cleaning / Pre-processing (outlier/missing values/categorical)

Machine learning algorithms for regression can understand the input only in the form of numbers and hence it is highly essential to convert the non - numeric data that we have to numeric data by providing them labels.


Label Encoding



Missing value treatment

This step involves the process of filling the missing values in appropriate ways so that the data is not lost.

Feature Engineering

CountVectorizer
TFIDF for text data



Modelling

Various regression algorithms are applied on the dataset and the model that suits best for the dataset is selected. The models that we apply for this dataset are


Random forest
SVM
Evaluation
Neural networks
Evaluation

"
39,Data Science Project on Wine Quality Prediction in R,"In this data science project, we will explore wine&nbsp;dataset for red wine quality. The objective is to explore which chemical properties influence the quality of red wines. As interesting relationships in the data are discovered, we&rsquo;ll produce and refine plots to illustrate them.
We will learn how to ask the right questions for data analysis at certain points in the project. Finally, we would learn how to storyboard our analysis to create a final picture from our work to help decision makers understand how wine qualities were influenced."
40,Human Activity Recognition Using Multiclass Classification,"In this machine learning project you will build a classification system to classify human activities.
The Human Activity Recognition dataset was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% the test data."
41,Create A Data Pipeline based on Messaging Using PySpark Hive,"Project Description
PySpark is a Python API for Apache Spark that was created to facilitate Apache Spark-Python integration. In addition, PySpark in Apache Spark and Python allows you to work with Resilient Distributed Datasets (RDDs). PySpark Py4J is a popular PySpark tool that allows Python to dynamically communicate with JVM objects. PySpark includes a number of libraries that can assist you in writing efficient programs.
PySpark is a useful tool for data scientists since it simplifies the process of turning prototype models into production-ready model workflows. Model workflows for model training and serving can be created with PySpark in cluster environments. PySpark can be used for exploratory data analysis and developing machine learning pipelines, which is important in a data science workflow.
Apache Hive is a data warehouse framework that can process huge amounts of data. The datasets are typically stored in Hadoop Distributed File Systems and other platforms' databases. Hive is a framework for reading, writing, and managing data that is based on top of Hadoop. The query language used with Apache Hive to do querying and analytics is HQL or HiveQL.
Hive is a database designed for batch transformations and massive analytical queries, with restricted write capabilities and interaction. RDBMS experts adore Apache Hive because it allows them to map HDFS files to Hive tables and query the data with ease. HBase tables can also be mapped and Hive can be used to process the data.
Objective of PySpark Hive Data Engineering Project
In this Big Data project, a senior Big Data Architect will demonstrate how to implement a Big Data pipeline on AWS at scale. You will be using the Covid-19 dataset. This will be streamed in real-time from an external API using NiFi. The complex JSON data will be parsed into CSV format using NiFi and the result will be stored in HDFS.
Then this data will be sent to Kafka for data processing using PySpark. The processed data will then be consumed from Spark and stored in HDFS. Then a Hive external table is created on top of HDFS. Finally the cleaned, transformed data is stored in the data lake and deployed. Visualization is then done using Tableau and AWS QuickSight.

Dataset used in the Spark Pipeline Project
This PySpark pipeline project involves working on the Covid-19 dataset. The dataset includes the total number of confirmed cases, the total number of recovered cases, the total number of deaths, country name, country code, etc.
Learning Takeaways from the Hive PySpark Project
NiFi
Apache NiFi is a data logistics platform that automates the transfer of data across different systems. It gives real-time control, making data transfer among any source and any target simple to monitor. To build a data pipeline using spark in this project, you first need to extract the data using NiFi. After the data has been successfully extracted, the next step is to encrypt certain information (country code) to ensure data security. This is done by applying various hashing algorithms to the data. Also, you must ensure that all encrypted data are in uppercase format for the algorithms to function properly.
Kafka
Apache Kafka is a pub-sub (publish-subscribe) messaging service and a powerful queue that can manage a large amount of data and allows you to send messages from one terminal to another. Kafka may be used to accept messages both offline and online. To avoid data loss, Kafka messages are stored on a disc and replicated throughout the cluster. The Kafka messaging system is based on the ZooKeeper synchronization service. For real-time streaming data processing, it works well with Apache Storm and Spark. This data engineering project entails publishing the real-time streaming data into Kafka using the PublishKafka processor. Once the data is stored in Kafka topic, it needs to be streamed into PySpark for further processing.
PySpark
PySpark is a Python Spark framework for executing Python programs employing Apache Spark capabilities. PySpark is widely used in the Data Science and Machine Learning industry since many popular data science libraries are written in Python, such as NumPy and TensorFlow. It's also popular since it can handle enormous datasets quickly. The next step of this PySpark pipeline project is to read the streaming data from the Kafka topic and perform some operations on it using PySpark. Once the data has been processed, it is streamed into the output Kafka topic.
Hive
Apache Hive is a fault-tolerant distributed data warehouse that allows for huge analytics. Hive users can read, write, and manage huge amounts of data using SQL. Hive is built on top of Apache Hadoop, an open-source platform for storing and processing large amounts of data. As a result, Hive is inextricably linked to Hadoop and is designed to process gigabytes of data efficiently. Hive is characterized by its capability to search large datasets with a SQL-like interface utilizing Apache Tez or MapReduce. In this project, once the data is stored in HDFS, an external table is created using Hive on top of HDFS. This is done to perform queries on the stored data.
Quicksight
Amazon QuickSight is a cloud-based business intelligence (BI) tool that is scalable, serverless, embeddable, and powered by machine learning. Businesses may use Amazon QuickSight BI to build and analyze data visualizations and extract easy-to-understand insights to help them make better business decisions. Quicksight allows you to easily integrate the interactive dashboards into various apps, platforms, and websites. This PySpark HIve project involves creating multiple dashboards using Bar graphs, Pie charts, Scatter plots, etc. The dashboards depict data such as the average of total confirmed cases, the average of total recovered cases, the average of total deaths, etc.
Tableau
Tableau is a visual analytics tool capable of managing a company's full data landscape. The analytics tool focuses on providing engaging data graphics, with a focus on business scenarios. Tableau offers a variety of baseline visualizations. Line charts, heat maps, and other visual aids are among them. To create and access advanced visualizations, the tool does not require the user to have specialized coding expertise. During the analysis, users can include as many data points as they want. Tableau also offers low-cost/free non-profit tools as well as other academic alternatives. In this spark pipeline project, Tableau is used for data visualization with help of an Area chart, Bar graph, Bubble chart, etc. The various dashboards show the country-wise analysis such as the average of total confirmed cases, the average of total deaths, etc.
FAQs
Q1. What is a Spark pipeline?
A pipeline in Apache Spark is an object that combines convert, evaluate, and fit steps into a single object. A pipeline is made up of several stages, each of which is an Estimator or a Transformer.
&nbsp;
Q2. Can Spark be used for ETL?
Apache Spark is a popular and effective framework for ETL, i.e. it is used for processing, querying, and analyzing large amounts of data. By setting up a cluster of several nodes, you can easily load and handle huge amounts of data.
"
42,Churn Prediction in Telecom using Machine Learning in R,"Business Objective
In any service providing industry, when a customer decides to stop using the service either by canceling the subscription or not paying for the service, we call this customer churn.
Churn is defined as how many customers are not using the service for a certain period.
Hence, customer churn is one of the essential metrics that every business must evaluate to grow. The churn rate is calculated by dividing the number of lost customers by the last number of customers. Thus, a company churn rate must be as low as possible, ideally 0%.
But why is it so important to calculate the churn rate? Does it affect the business if you lose around 5% of customers? Yes, the answer is that it costs more to acquire a new customer than retain the existing customers. Retaining the current customers, any company can spend less on operating costs needed to reach new customers.
So, we will use advanced machine learning techniques to predict the potential churners who are about to leave a company&rsquo;s service and take the necessary steps to prevent it.
This project aims to build a deep learning model that will help predict customers who are likely to churn in the next N months and facilitate in taking business actions for reducing the churn.
Data Description&nbsp;
The Churn Prediction dataset is a dataset from Kaggle, that is used for predicting customer churn.
The available dataset is:
Telco-Customer-Churn &ndash; This dataset has 7043 rows and 21 columns present.
The 21 features of this dataset are as follows:

Churn &ndash; the target variable, if the customer is churned or not (Yes / No)
customerID &ndash; The unique identification of every customer
gender- If the customer is a male or a female (Female / Male)
SeniorCitizen &ndash; If the customer is a senior citizen or not (0 / 1)
Partner &ndash; If the customer has a partner or not (Yes/No)
Dependents &ndash; If the customer has any dependents (Yes / No)
Tenure &ndash; The time period(months) the customer has stayed with the company.
PhoneService &ndash; If the customer has a phone service or not (Yes/No)
MultipleLines &ndash; If the customer has multiple lines or not (Yes/No/No Phone service)
InternetService &ndash; If the customer has any internet service or not (DSL/ Fibre optics/ No)
OnlineSecurity &ndash; If the customer has any online security (Yes/No/No internet service)
OnlineBackup &ndash; If the customer has any online backup (Yes/No/No internet service)
DeviceProtection &ndash; If the customer has device protection (Yes/No/No internet service)
TechSupport &ndash; If the customer has tech support (Yes/ No/ No internet service)
StreamingTV &ndash; If the customer has any streaming TV (Yes/ No/ No internet service)
StreamingMovies &ndash; If the customer has streaming movies (Yes/ No/ No internet service)
Contract &ndash; The customer term period with the company (Month-to-month, One year, Two years)
PaperlessBilling &ndash; If the customer has paperless billing or not (Yes/ No)
PaymentMethod &ndash; The payment mode of each customer (Electronic check, mailed check,Bank transfer, Credit card)
MonthlyCharges &ndash; The amount that is charged to the customer every month
TotalCharges &ndash; The total amount charged to the customer

Aim
To predict the customers who are likely to churn in the next N months &amp; facilitate in taking business actions for reducing the churn
Tech stack

Language &ndash; R 
Libraries - keras, dplyr, mice, ggplot2, recipes. 

Approach

Importing the required libraries and reading the dataset.


Understanding the dataset


Exploratory Data Analysis (EDA) &ndash; 


Data Visualization (Categorical and Numerical data)


Feature Engineering 


Missing value imputation 
Using Recipe function 


Step discretization (Creating intervals)
Step log 
Step dummy (One hot encoding)
Step centre (Mean centring) 
Step scale (Scaling the data)


Model Building


Performing train test split
Keras sequential model building
Compile the model
Fit the model


Model Validation 


Confusion Matrix
AUC 
Recall value
F1-Score


Model Insights


Feature Importance
Correlation analysis


Save the model &ndash; model deployment 

&nbsp;"
43,German Credit Dataset Analysis to Classify Loan Applications,"The German credit dataset contains information on 1000 loan applicants. Each applicant is described by a set of 20 different attributes. Of these 20 attributes, seventeen attributes are discrete while three are continuous. The main idea is to use techniques from the field of information theory to select a set of important attributes that can be used to classify tuples. In this data science project, you will train a&nbsp;neural network using these attributes; the neural network is then used to classify tuples."
44,Spark Project-Analysis and Visualization on Yelp Dataset,"Most businesses seek to get reviews on their goods and services one way or another. It is a most basic way for the business to improve their efficiency and subsequently their bottom-line. Get the review is not only the issue, ability to extract and visualize analytics from review data is critical to business success.
In Apache Spark Project, we will use the yelp review dataset to analyze businesses and reviews over a period of time. Perhaps we will spot potential gaps in service delivery or see how business thrive in different scenarios.
Beyond processing this data, we will ingest the final output of our data processing in Elasticsearch and use the visualization tool in the ELK stack to visualize various kinds of ad-hoc reports from the data.&nbsp;"
45,Build an Image Classifier for Plant Species Identification,"Introduction to Plant Species Identification
Imagine you are planning to start your own tea-leaves business. You&rsquo;d want to price them depending on their quality. And if you have no idea how to do that, there is no need to worry as we have the leaf classification python code in this image identification project that will help you achieve it. Due to their volume, prevalence, and unique characteristics, leaves are an effective means of differentiating plant species. They also provide a fun introduction to applying techniques that involve image-based features. Thus, one can build an image classifier for plant species identification by implementing image processing techniques and computer vision methods over images of leaves.

Plant Identification Project Objective
The objective of this machine learning project is to use binary leaf images and extracted features, including shape, margin, and texture, to identify 99 species of plants accurately. We will apply different classification techniques to benchmark the relevance of classifiers in image classification problems. This project will help you understand which Python libraries out of sklearn, scipy, and TensorFlow will best suit the specific files in the dataset for building an efficient plant species identification system.
&nbsp;

Plant Data used in this Machine Learning Project
The dataset for this plant classification project is divided into four files: train.csv, test.csv, sample_submission.csv, and images. The train and test files contain the following:
&nbsp;
id- an anonymous id unique to an image
margin_1, margin_2,..., margin_64 - each of the 64 attribute vectors for the margin feature
shape_1, shape_2,..., shape_64- each of the 64 attribute vectors for the shape feature
texture_1, texture_2,..., texture_64 - each of the 64 attribute vectors for the texture feature
Species- species of the plant in the image
&nbsp;
There are about 990 images in the dataset, 10 for each class of species. Thus, the dataset contains information on 99 different plant species.
Concepts to Learn in this Machine Learning Plant Identification Project
Let us discuss the concepts that you will master through this plant identification project.
Data Preprocessing
In this project, the task is to build a plant classifier using the given dataset. The dataset has various features: margins, shapes, and textures of different plants, which have been extracted from the plant images. And before serving this data of 99 other species to machine learning algorithms, the species variable (target variable) will be converted into numerical value using encoding techniques in this project. Additionally, the project solution shows how to split the training dataset into training and validation subsets so that the split is unbiased towards any particular class in the dataset.
Machine Learning Classification Algorithms
As the problem is of classification type, you will learn about different classification algorithms in this project. We will discuss the following algorithms in detail:


K-Nearest neighbors


Support Vector Classifier


Decision Tree


Random Forests


Gradient Boosting


Adaptive Boosting


Naive Bayes


Linear Discriminant Analysis


Quadrant Discriminant Analysis


You will apply all these algorithms to the given dataset and evaluate which model performs the best by default. For hyperparameter tuning, the project solution will use the grid search method and assess the accuracy of each model. Additionally, it will show you how to evaluate the loss for each algorithm and visualize it using a logarithmic plot.
Deep Learning Algorithm: Neural Networks
Besides the train and test data files, the dataset contains images of different leaves. So, why not build a plant classifier app that can search a plant by its image? This project will cover that and teach you how to perform plant identification using TensorFlow. You will learn simple neural network algorithms and a special type of neural network called CNN (convolutional neural networks) that are widely used with image datasets. Using Keras with TensorFlow in the backend, you will learn how to build different layers of a CNN, how to set activation functions, choose pooling methods, etc., in the Python programming language. Furthermore, the project will also teach you how to determine the accuracy and loss of a convolutional neural network.
FAQs for Plant Search by Image Project
What is an Image Classifier?
An image classifier is a generic term used to describe application systems that can correctly identify the image's class by analyzing its pixels.
Is CNN a Classifier?
CNN(convolutional neural network) is a kind of neural network used to solve classification problems in deep learning. It is not only limited to classification problems but can also be used in summarization, machine translation, time series, etc. Because it involves the convolution operation, CNNs are widely used to solve image and text classification problems."
46,Loan Eligibility Prediction in Python using H2O.ai,"Business Objective When a customer applies for a loan at our company, we use statistical models to determine whether or not to grant the loan based on the likelihood of the loan being repaid. The factors involved in determining this likelihood are complex, and extensive statistical analysis and modelling are required to predict the outcome for each individual case.AimYou must implement a model that predicts if a loan should be granted to an individual based on the data provided&nbsp;Tech StackLanguage : PythonLibraries : Scikit-learn, H2O, pandas, numpy, flask, Seaborn, MatplotlibContainerization : DockerDataset DescriptionThe dataset used is an anonymized synthetic data that was generated specifically for use in this project. The data is designed to exhibit similar characteristics to genuine loan data.In this dataset, you must explore and cleanse a dataset consisting of over 1,00,000 loan records to determine the best way to predict whether a loan applicant should be granted a loan or not."
47,Avocado Machine Learning Project Python for Price Prediction,"Business Objective
Hass avocados, a Mexico based company produces a variety of Avocados which are sold in the US. They have been having good success for the past several years and want to expand. For this, they want to build and assess a plausible model to predict the average price of Hass avocado to consider the expansion of different types of Avocado farms that are available for growing in other regions.
&nbsp;

&nbsp;
Aim
Forecast the prices of Avocado in the US
&nbsp;
&nbsp;
Data
The data comes directly from retailers&rsquo; cash registers based on the actual retail sales of Hass avocados.

Data represents weekly retail scan data for National retail volume (units) and price from Apr 2015 to Mar 2018.
The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags.
The Product Lookup codes (PLU&rsquo;s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.

Some relevant columns in the dataset:

Date - date of the observation
AveragePrice - average price of a single avocado
Type - conventional / organic
Region - region of the observation
Total Volume - Total number of avocados sold
4046 - Total number of avocados with PLU 4046 sold
4225 - Total number of avocados with PLU 4225 sold
4770 - Total number of avocados with PLU 4770 sold
Total Bags &ndash; Total bags sold
Small/Large/XLarge Bags &ndash; Total bags sold by size

There are two types of avocados in the dataset as well as several different regions represented. This allows you to do all sorts of analysis for different areas of the United States, specific cities, or just the overall United States on either type of avocado. Our analysis will be focused on the complete dataset.
&nbsp;
Dataset : https://www.kaggle.com/neuromusic/avocado-prices#avocado.csv
&nbsp;
Tech Stack

Language used : Python
Libraries used : statmodels, pmdarima, fbprophet, scikit-learn

&nbsp;
Approach

Data Preprocessing 

Check for missing values
Label Encoding
One hot encoding

Exploratory Data Analysis

Identifying any overarching trend in data over time
Identifying any repetitive, seasonal patterns in the data

Feature Engineering

Creating new columns

Building Forecast models

Linear Regression
Random Forest Regressor
XGB Regressor
Facebook Prophet
ARIMA
SARIMAX

Evaluating Forecast models

R-squared
MAPE
&nbsp;MAE
Plots comprising the actual values, forecast and confidence intervals.

"
48,Census Income Data Set Project - Predict Adult Census Income,"Business Context
A census as the total process of collecting, compiling, and publishing demographic, economic, and social data pertaining to a specific time to all persons in a country or delimited part of a country. As part of a census count, most countries also include a census of housing. It is the process of collecting, compiling and publishing information on buildings, living quarters and building-related facilities such as sewage systems, bathrooms, and electricity, to name a few.
Possible Uses of Census Information





Census Information


Potential Uses




Total Population Size


When two or more census counts are compared for the same location, planners can determine if locales are increasing or decreasing in size.




Age


Used to help identify segments of the population that require different types of services.




Sex


Sex ratios can be calculated by 5-year age groups to crudely observe migration, especially among the working age cohorts.




Marital Status


Used to provide insights into family formation and housing needs.




Household Composition and Size


Used to help determine housing needs for related and unrelated households.




Educational Attainment and Literacy


Used to provide information on the educational skills of the workforce. These measures also help planners select the best strategies to communicate with residents.




Location of Residence and Place of Prior Residence


Helps assess changes in rural and urban areas. Place of prior residence helps to identify communities that are experiencing in- or out-migration.




Occupation and Labor Force Participation


Helps to provide insights into the labor force of a given locale. The information can be used to develop economic development strategies.




Living Quarter Characteristics


Can help planners determine housing and community facility needs





&nbsp;
Data Description
In this project, we will use a standard imbalanced machine learning dataset referred to as the &ldquo;Adult Income&rdquo; or simply the &ldquo;adult&rdquo; dataset.
The dataset is credited to Ronny Kohavi and Barry Becker and was drawn from the 1994 United States Census Bureau data and involves using personal details such as education level to predict whether an individual will earn more or less than $50,000 per year.
The dataset provides 14 input variables that are a mixture of categorical, ordinal, and numerical data types. The complete list of variables is as follows:


Age.


Workclass.


Final Weight.


Education.


Education Number of Years.


Marital-status.


Occupation.


Relationship.


Race.


Sex.


Capital-gain.


Capital-loss.


Hours-per-week.


Native-country.


The dataset contains missing values that are marked with a question mark character (?).
There are a total of 48,842 rows of data, and 3,620 with missing values, leaving 45,222 complete rows.
There are two class values &lsquo;&gt;50K&lsquo; and &lsquo;&lt;=50K&lsquo;, meaning it is a binary classification task. The classes are imbalanced, with a skew toward the &lsquo;&lt;=50K&lsquo; class label.


&lsquo;&gt;50K&rsquo;: majority class, approximately 25%.


&lsquo;&lt;=50K&rsquo;: minority class, approximately 75%.


Data Source&nbsp;
https://archive.ics.uci.edu/ml/datasets/Adult
Tools/Libraries&nbsp;


Python


scikit-learn(machine learning library)


h2o.ai


Aim of the Census Income Dataset Project
Census Salary Prediction where we have to classify between &gt;50K &lt;=50K. This mainly helps to understand the real estate demands and also, the demands for basic amenities according to one&rsquo;s salary range.
How Does it help


Real Estate Demands


Basic Amenities&nbsp;


Fulfilling Infrastructure Demands


Learning Takeaways from the Adult Census Income Prediction Project
This income prediction machine learning project involves various deep learning techniques and approaches which are discussed below-
Exploratory Data Analysis
Data scientists utilize exploratory data analysis (EDA) to study and investigate data sets and highlight their key properties, typically using data visualization techniques.&nbsp; It can help in error detection, as well as a better understanding of data patterns, the detection of outliers or unusual events, and the discovery of interesting relationships between variables. This income prediction project entails performing EDA on the census income dataset. The dataset is visualized using histograms for various categories such as Age range, Education range, etc. Also, descriptive analysis is done in order to treat the missing values problem.
Perceptron Deep Learning Algorithm
Perceptron is a binary classification algorithm that uses a linear learning approach. It is one of the earliest and most basic types of artificial neural networks. For two-class classification tasks, it can quickly learn a linear separation in feature space. It learns with help of the stochastic gradient descent optimization process and does not predict calibrated probability, unlike logistic regression.
H2O.AI
H2O AI is a Java-based data-modeling and general-purpose computing package. Its key purpose is to serve as a distributed, parallel, in-memory processing engine. It supports the simple horizontal scaling of an issue in order to get a quick solution. It supports data in a variety of formats, including CSV, ORC, Parquet, and Hive, and allows data intake from a variety of sources, including the local file system, remote file system, HDFS, and Hive. In this project, once the data has been ingested and processed, we obtain the training and validation accuracy which shows that our data is not overfitting. Further, the project involves working with Area under curve (AUC) and Grid search techniques for hyperparameter tuning purposes.
FAQs
Q1. What is Census Income?
Income received on a routine basis (excluding specific money receipts such as capital gains) before payments for personal taxes, welfare benefits, union dues, healthcare liabilities, and other deductions is regarded as Census money income.
Q2. What are the benchmarking regression algorithms for income prediction modeling?
Ordinary least squares regression, beta regression, robust regression, ridge regression, MARS, ANN, LSSVM, and CART are some of the benchmark regression methods for income prediction modeling.
"
49,NLP and Deep Learning For Fake News Classification in Python,"Business Overview
What is Fake News?
Fake news is the deliberate presentation of (typically) false or misleading claims as news, where the claims are misleading by design.
How News and digital media evolved?
The news media evolved from newspapers, tabloids, and magazines to a digital form such as online news platforms, blogs, social media feeds, and many news mobile apps. News outlets benefitted from the widespread use of social media/mobile platforms by providing updated news in near real time to its subscribers.
It became easier for consumers to acquire the latest news at their fingertips. So, These digital media platforms become very powerful due to their easy accessibility to the world and ability to allow users to discuss and share ideas and debate over issues such as democracy, education, health, research and history.
However, apart from advantage, false/fake news articles on digital platforms are getting very common and mainly used with a negative intent for their own benefit such as political and financial benefit, creating biased opinions, manipulating mindsets, and spreading absurdity.
How big is this Problem ?
With the rapid adoption of Internet, social media and digital platforms (such as Facebook, Twitter, news portals or any social media), anybody can spread untrue and biased information. It is virtually impossible to prevent Fake News from being created. There has been a rapid increase in the spread of fake news in the last decade, it's not limited to any one domain like politics but covering various other domains such as sports, health, history, entertainment and also science and research. If we take the 2016 US presidential election, there were lots biased and fake news published to influence. Another example could be of COVID-19, we generally come across many misleading/fake news everyday which can have serious consequences and may lead to create panic among people and spread pandemic more rapidly.
What is Solution?
Therefore, It is important and absolutely necessary to identify and differentiate Fake News from real news. One of the ways is to determine by expert and fact check of every news, but this is time consuming and requires skills which can not be shared. Second, we can automate the detection of Fake News by using the techniques of Machine learning and Artificial Intelligence. The Online news content has diverse unstructured format data(such as documents, videos, and audios), here we will concentrate on text format news. With the advancement of and Natural language processing It is possible now that we can identify the deceptive and fake nature of articles or sentences.
There is widespread study and experimentation happening in this area to identify the Fake news for all medium(Video, audio and Text) news.
Data Description
In our study we used the Fake news dataset from Kaggle to classify unreliable news articles as Fake news using Deep learning Technique Sequence to Sequence programming.
A full training dataset with the following attributes

id : unique id for a news article
title: the title of a news article
author: author of the news article
text : the text of the article; could be incomplete
label : a label that marks the article as potentially unreliable

1 : unreliable
0 : reliable



Tech Stack

Language : Python
Libraries : Scikit-learn , Tensorflow , Keras, Glove, Flask, nltk, pandas, numpy
"
50,Data Science Project-TalkingData AdTracking Fraud Detection,"Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest mobile market in the world and therefore suffers from huge volumes of fraudulent traffic.&nbsp;

In this machine learning project, you will build a machine learning model to determine whether a click is fraud or not.
"
51,Identifying Product Bundles from Sales Data Using R Language,"The weekly sales transaction dataset consists of&nbsp;weekly purchased quantities of 800 products over 52 weeks. Normalised values are provided too. The objective of this data science project in R is to find out product bundles that can be put together on sale. Typically Market Basket Analysis was used to identify such bundles, here we are going to compare the relative importance of time series clustering in identifying product bundles."
52,Ecommerce product reviews - Pairwise ranking and sentiment analysis,"E-Commerce applications provide an added advantage to customers to buy a product with added suggestions in the form of reviews. Obviously, reviews are useful and impactful for customers who are going to buy the products. But these enormous amounts of reviews also create problems for customers as they are not able to segregate useful ones. Regardless, these immense proportions of reviews make an issue for customers as it becomes very difficult to filter informative reviews. This proportional issue has been attempted in this project. The approach that we discuss in detail later ranks reviews based on their relevance with the product and rank down irrelevant reviews. This work has been done in four phases- data preprocessing/filtering (which includes Language Detection, Gibberish Detection, Profanity Detection), feature extraction, pairwise review ranking, and classification. The outcome will be a list of reviews for a particular product ranking on the basis of relevance using a pairwise ranking approach."
53,Loan Eligibility Prediction using Gradient Boosting Classifier,"SYL bank is one of Australia�s largest banks. Currently, the loan applications which come in to their various branches are processed manually. The decision whether to grant a loan or not is subjective and due to a lot of applications coming in, it is getting harder for them to decide the loan grant status. Thus, they want to build an automated machine learning solution which will look at different factors and decide whether to grant loan or not to the respective individual.
In this ML problem, we will building a classification model as we have to predict if an applicant should get a loan or not. We will look at various factors of the applicant like credit score, past history and from those we will try to predict the loan granting status. We will also cleanse the data and fill in the missing values so that our ML model performs as expected. Thus we will be giving out a probability score along with Loan Granted or Loan Refused output from the model."
54,Machine Learning project for Retail Price Optimization,"Introduction to Price Optimization
Pricing a product is a crucial aspect of any business. A lot of thought process is put into it. There are different strategies to estimate prices for different kinds of products. There are products whose sales are pretty sensitive to their costs, and as such, a slight change in their price can lead to a noticeable difference in their sales. At the same time, there are also products whose sales are not much affected by their worth - these tend to be luxury items or necessities (like certain medicines).&nbsp;
&nbsp;
Price elasticity of demand (EPD), or elasticity, is the degree to which the compelling desire for something changes as its price changes. In general, people desire things less as those things become more expensive. However, for some products, the customers&rsquo; desire could drop sharply even with a bit of price increase, and for other products, it could stay almost the same even with a hefty price increase. Economists use the term elasticity to denote this sensitivity of sales to price fluctuations. More precisely, price elasticity gives the percentage change in quantity demanded when there is a one percent increase in price, holding everything else constant.

Retail Price Optimization in Python
In this machine learning pricing optimization case study, we will take the data of a cafe and, based on their past sales, identify the optimal prices for their items based on the price elasticity of the items. First, you will calculate the price elasticity for each item, then figure out the optimal price. While taking a particular cafe data, one can extend this work to price any product. This machine learning retail price optimization project will focus on the former products.

Dynamic Pricing Dataset
The data is contained in three CSV files.
Cafe - Sell MetaData.csv This file has details about sales made by the cafe.&nbsp;
Columns: Sell ID, Sell Category, Item ID, Item Name
Cafe - Transaction - Store.csv This file contains information about transactions and sale receipts of the cafe.
Columns: Calendar Date, Price, Quantity, Sell ID, Sell Category
Cafe - DateInfo.csv This has date information corresponding to the transactions performed.
Columns: Date, Year, Holiday, Weekend, School Break, Temperature, Outdoor
Topics Covered in Price Optimization Machine Learning Project
In detail, let us discuss all tools and techniques you will explore in this project.
&nbsp;
Price Optimization Algorithms
Understanding customer behavior through sales data is crucial for the growth of any business. Not only it contributes to improved quality of products, but it additionally assists in determining the right price for the different products. For instance, products perceived as luxury items by the masses are sold at unreasonably high prices. In this dynamic pricing python project, you will use previous sales data to estimate the cost of different food items in a cafe. Additionally, you will learn about other price optimization methods like cost-less pricing, competition-based pricing, perceived value pricing, and demand-based pricing. This project will also introduce you to price elasticity, a concept that plays a critical role in determining price estimates.
&nbsp;
Exploratory Data Analysis
Before the price optimization dataset is used for modeling, it must be processed. The dataset may contain redundancy that one must remove, and one must bring on all the variables of different data types on the same foot. In this project, you will work on the dataset of a burger cafe and use their three datasets related to sales, transactions, and corresponding dates. You will learn how to analyze the dataset using data visualization libraries of Python: matplotlib and seaborn. This price optimization machine learning project will also guide you on merging the datasets and preparing them to apply machine learning algorithms using Pandas dataframes.
&nbsp;
Machine Learning Algorithms
Instead of traditional statistical methods of price estimation, this project will perform price optimization using machine learning in Python. You will learn how to use the regression trees and ordinary least square method to estimate the price elasticity for different products. Furthermore, you will understand how statistical parameters like the r-squared value are interpreted for analysis. The project will also teach you how to improve the accuracy of the models by eliminating specific variable values. Additionally, you will explore maximizing profit using the results of price elasticities evaluation.
&nbsp;
Application of Machine Learning for Pricing Optimization in Python Project
Primarily, this project focuses on optimizing the prices of various items available in a burger cafe. The solution of this pricing optimization in Python project can be easily used by experts of different industries like medical, hospitality, insurance, etc. For example, an analyst can recommend changes to the prices of various services offered by a hotel depending on the previous residents&rsquo; feedback.
FAQs for Pricing Optimization with Machine Learning
1) How do you do Price Optimization?
One can apply different types of price optimization techniques like reducing cannibalization for inter and intra products of the same company, reducing cost drastically while playing a volume game etc. &nbsp;&nbsp;
2) What is Price Optimization Machine learning?
Price Optimization can be achieved using regression machine learning algorithms like linear regression. One can first estimate the price elasticity for each product using the past sales data and then use that coefficient for price optimization."
55,Customer Churn Prediction Analysis using Ensemble Techniques,"A well-known bank has been observing a lot of customers closing their accounts or switching to competitor banks over the past couple of quarters. This has caused a huge dent in their quarterly revenues and might drastically affect annual revenues for the ongoing financial year, causing stocks to plunge and market cap to reduce significantly. The idea is to be able to predict which customers are going to churn so that necessary actions/interventions can be taken by the bank to retain such customers.
In this machine learning churn prediction project, we are provided with customer data pertaining to his past transactions with the bank and some demographic information. We use this to establish relations/associations between data features and customer's propensity to churn and build a classification model to predict whether the customer will leave the bank or not. We also go about explaining model predictions through multiple visualizations and give insight into which factor(s) are responsible for the churn of the customers.
This project walks you through a complete end-to-end cycle of a data science project in the banking industry, right from the deliberations during formation of the problem statement to making the model deployment-ready."
56,Event Data Analysis using AWS ELK Stack,"In this pre-built big data industry project, we extract real time streaming event data from New York City accidents dataset API. We then process the data on AWS to extract KPIs and metrics which will eventually be pushed to Elasticsearch for text based search and analysis using Kibana visualization.
This is an end-to-end project pipeline right from Data extraction - Cleaning - Transformation - Exploratory analysis - Visualisation - Data flow orchestration of event data on the cloud."
57,Natural language processing Chatbot application using NLTK for text classification,"Human language is astoundingly complex and diverse. When we write, we often misspell or abbreviate words, or omit punctuation. There is a lot of unstructured data around us. Natural language processing helps computers communicate with humans in their own language and scales other language-related tasks. NLP makes it possible for computers to read text, interpret it, measure sentiment and determine which parts are important.
Understanding this will enable you to build the core component of any conversational chatbot. In this NLP application we will create the core engine of a chat bot. We will learn text classification using the techniques of natural language processing by using the nltk library."
58,NLP Project to Build a Resume Parser in Python using Spacy,"Imagine working as an intern in a company's Human Resource Department, and you have been provided with a massive pile of about 1000 resumes. Your task is to prepare a list of candidates suitable for the software engineer role. Now, since this company didn't provide the candidates with a resume format, it is your job to analyze each resume manually. How tiring, right? Well, there is an easy way out: building a Resume Parsing Application that takes resumes as input, then extracts and analyses all the valuable information from it. Companies' recruiters and HR teams have a tough time scanning thousands of qualified resumes. They either need many people to do this or miss out on qualified candidates. Spending too many labor hours segregating candidates' resume's manually is a waste of a company's time, money, and productivity. We thus suggest you work on this Resume Parsing project that can automate the segregation task and save companies a lot of time.

Resume Parsing in Python Project Objective
This project uses Python's library, SpaCy to implement various NLP (natural language processing) techniques like tokenization, lemmatization, parts of speech tagging, etc., for building a resume parser in Python. And, considering all the resumes are submitted in PDF format, you will learn how to implement optical character recognition (OCR) for extracting textual data from the documents. The resulting application will require minimum human intervention to extract crucial information from a resume, such as an applicant's work experience, name, geographical location, etc. It is one of the most exciting NLP projects for beginners, so make sure you attempt it.

To solve this, our resume parser application can take in millions of resumes, parse the needed fields and categorise them. This resume parser uses the popular python library - SpaCy for OCR and text classifications. First, we train our model with these fields, then the application can pick out the values of these fields from new resumes being input.
The dataset of resumes has the following fields:

Location
Designation
Name
Years of Experience
College
Degree
Graduation Year
Companies worked at
Email address

NLP Tools and Techniques You Will Master in this SpaCy Resume Parser Project
Here is an introduction to the exciting concepts you will learn when building a python resume parser application system.&nbsp;
Tokenization
It is the process of splitting textual data into different pieces called tokens. One can either break a sentence into tokens of words or characters; the choice depends on the problem one is interested in solving. It is usually the first step that is performed in any NLP project, and the same will be the case with this resume parser using NLP project. Tokenization helps in further steps of an NLP pipeline which usually involves evaluating the weights of all the words depending on their significance in the corpus.
Lemmatization
The larger goal of this resume parsing python application is to decode the semantics of the text. For that, the form of the verb that is used does not have a significant impact. Therefore, lemmatization is used to convert all the words into their root form, called 'lemma.' For example, 'drive,' 'driving, 'drove' all have the same lemma 'drive.'
Parts-of-Speech Tagging
If you consider the word ""Apple,"" it can have two meanings in a sentence. Depending on whether it has been used as a proper noun or a common noun, you will understand whether one is discussing the multinational tech company or the fruit. This CV parser python project will understand how POS Tagging is implemented in Python.
Stopwords Elimination
Stopwords are the words like 'a', 'the,' 'am', 'is', etc., that hardly add any meaning to a sentence. These words are usually deleted to save on processing power and time. In their CV, an applicant may submit their work experience in long paragraphs with many stopwords. For such cases, it becomes essential to know how to extract experience from a resume in python, which you will learn in this project.
SpaCy
SpaCy is a library in Python that is widely used in many NLP-based projects by data scientists as it offers quick implementation of techniques mentioned above. Additionally, one can use SpaCy to visualize different entities in text data through its built-in visualizer called displacy. Furthermore, SpaCy supports the implementation of rule-based matching, shallow parsing, dependency parsing, etc. This NLP resume parser project will guide you on using SpaCy for Named Entity Recognition (NER).
OCR using TIKA
You will use Apache Tika, an open-source library for implementing OCR in this project. OCR stands for optical character recognition. It involves converting images into text and will be used in this resume extraction python project for decoding text information from the PDF files. The textual data is processed using various NLP methods to extract meaningful information.
Machine Learning Pipeline
As this project is about resume parsing using machine learning and NLP, you will learn how an end-to-end machine learning project is implemented to solve practical problems. Different machine learning algorithms Neural Networks using SpaCy library are used in this project to build a model that can pull out relevant fields like location, name, etc., from different resumes of different formats.
Scaling up the Resume Parser in Python!
This project lays out the solution for a small dataset. However, if you are interested in building a production-ready model for resume parsing in Python that can analyze millions of resume documents, then refer to Model Deployment on GCP using Streamlit for Resume Parsing.&nbsp;Please keep in mind that before you deploy this model for large-scale r&eacute;sum&eacute;s, you will need to tag them and make the model learn any new entities which might have been added.&nbsp;
&nbsp;
Frequently Asked Questions on Resume Parser in Python
&nbsp;
1) How do you extract skills from a resume using Python?
The first step in extracting skills or any other entity from a resume is to do data preprocessing and applying techniques like tokenization, lemmatization, pos tagging, stopwords elimination, etc. with the help of the library, SpaCy. &nbsp;Post this, you can apply neural networks to tag entities and make the model learn from custom tags or you can then use an existing Neural Networks model to do the predictions.
&nbsp;
2) &nbsp;How do you parse a resume in Python?
To parse a resume in Python, especially for PDFs,&nbsp;one needs to perform optical character recognition (OCR) to extract text from the documents. You can use Apache Tika or Tesseract library for extracting the text out of these PDFs. If the resume is in a word document, then you need to write a code that can read a document and extract text from it."
59,Customer Market Basket Analysis using Apriori and Fpgrowth algorithms,"Analysis of historical customer data can highlight if a certain combination of products purchased makes an additional purchase more likely. This is called market basket analysis (also called as MBA). It is a widely used technique to identify the best possible mix of frequently bought products or services. This is also called product association analysis. The set of items a customer buys is referred to as an itemset, and market basket analysis seeks to find relationships between purchases. Market Basket Analysis creates If-Then scenario rules, for example, if item A is purchased then item B is likely to be purchased. The rules are probabilistic in nature or, in other words, they are derived from the frequencies of co-occurrence in the observations. Market Basket analysis is particularly useful for physical retail stores as it can help in planning floor space and product placement amongst many other benefits."
60,Topic modelling using Kmeans clustering to group customer reviews,"Topic modelling is a method for finding a group of words (i.e. topics) from a collection of documents that best represents the information in the collection of text documents. It can also be thought of as a form of text mining - a way to obtain recurring patterns of words in textual data. The topics identified are crucial data points in helping the business figure out where to put their efforts in improving their product or services.In this project we will use unsupervised technique - Kmeans, to cluster/ group reviews to identify main topics/ ideas in the sea of text. This will be applicable to any textual reviews. In this series, we will focus on twitter data which is more real world and more complex data compared to reviews obtained from review or survey forms.Topic modelling provides us with methods to organize, understand and summarize large collections of textual information. It helps in:Discovering hidden topical patterns that are present across the collectionAnnotating documents according to these topicsUsing these annotations to organize, search and summarize texts"
61,Demand prediction of driver availability using multistep time series analysis,"Food delivery supported through advanced applications has emerged as one of the fastest growing developments in the e-commerce space. We all love to order online, one thing that we don't like to experience is variable pricing for delivery charges. Delivery charges highly depend on the availability of riders in your area, demand of orders in your area, and distance covered. Due to driver unavailability, there is a surge in delivery pricing and many customers drop off resulting in loss to the company.
To tackle such issues if we track the number of hours a particular delivery executive is active, we can efficiently allocate certain drivers to a particular area depending on demand."
62,Build a Collaborative Filtering Recommender System in Python,"Recommender systems are an integral part of many online systems. From e-commerce to online streaming platforms. Recommender systems employ the past purchase pattern on its user to predict which other products they may be interested in and likely to purchase. Recommending the right products gives a significant advantage to the business. A major portion of the revenue is generated through recommendations.
The Collaborative Filtering algorithm is very popular in online streaming platforms and e-commerce sites where the customer interacts with each product (which can be a movie/ song or consumer products) by either liking/ disliking or giving a rating of sorts. One of the requirements to be able to apply collaborative filtering is that sufficient number of products need ratings associated with not them. User interaction is required.
This machine learning project walks you through the implementation of collaborative filtering using memory based technique of distance proximity using cosine distances and nearest neighbours."
63,"Build a big data pipeline with AWS Quicksight, Druid, and Hive","In this Big Data project, a senior Big Data Architect will demonstrate how to implement a Big Data pipeline on AWS at scale. You will be using the Aviation dataset. Analyse Aviation data using highly competitive technology big data stack such as NiFi, Kafka, HDFS ,Hive, Druid, AWS quicksight to derive metrics out of the existing data . Big data pipelines built on AWS to serve both batch and real time streaming ingestions of the data for various consumers according to their needs . This project is highly scalable and implemented on a very large scale organisation set up ."
64,AWS Project - Build an ETL Data Pipeline on AWS EMR Cluster,"In this Big Data project, a senior Big Data Architect will demonstrate how to implement a Big Data pipeline on AWS at scale. You will be using the sales dataset. Analyse sales data using highly competitive technology big data stack such as Amazon S3, EMR , Tableau to derive metrics out of the existing data . Big data pipelines built on AWS to serve batch ingestions of the data for various consumers according to their needs . This project is highly scalable and implemented on a very large scale organisation set up."
65,NLP Project on LDA Topic Modelling Python using RACE Dataset,"Business Context&nbsp;
With the advent of big data and Machine Learning along with Natural Language Processing, it has become the need of an hour to extract a certain topic or a collection of topics that the document is about. Think when you have to analyze or go through thousands of documents and categorize under 10 &ndash; 15 buckets. How tedious and boring will it be ?
Thanks to Topic Modeling where instead of manually going through numerous documents, with the help of Natural Language Processing and Text Mining, each document can be categorized under a certain topic.
Thus, we expect that logically related words will co-exist in the same document more frequently than words from different topics. For example, in a document about space, it is more possible to find words such as: planet, satellite, universe, galaxy, and asteroid. Whereas, in a document about the wildlife, it is more likely to find words such as: ecosystem, species, animal, and plant, landscape. A topic contains a cluster of words that frequently occurs together. A topic modeling can connect words with similar meanings and distinguish between uses of words with multiple meanings.
A sentence or a document is made up of numerous topics and each topic is made up of numerous words.
Data Overview
The dataset has odd 25000 documents where words are of various nature such as Noun,Adjective,Verb,Preposition and many more. Even the length of documents varies vastly from having a minimum number of words in the range around 40 to maximum number of words in the range around 500. Complete data is split 90% in the training and the rest 10% to get an idea how to predict a topic on unseen documents.
Objective&nbsp;
&nbsp; To extract or identify a dominant topic from each document and perform topic modeling.
Tools and Libraries
We will be using Python as a tool to perform all kinds of operations.
Main Libraries used are

Pandas for data manipulation, aggregation
Matplotlib and bokeh for visualization of how documents are structured.
NumPy for computationally efficient operations.
Scikit Learn and Gensim packages for topic modeling
nltk for text cleaning and preprocessing
TSNE and pyLDAvis for visualization of topics

Approach&nbsp;
Topic EDA

Top Words within topics using Word Cloud
Topics distribution using t-SNE
Topics distribution and words importance within topics using interactive tool pyLDAvis

Documents Pre-processing&nbsp;

Lowering all the words in documents and removing everything except alphabets.
Tokenizing each sentence and lemmatizing each word and storing in a list only if it is not a stop word and length of a word is greater than 3 alphabets.
Joining the list to make a document and also keeping the lemmatized tokens for NMF Topic Modelling.
Transforming the above pre-processed documents using TF IDF and Count Vectorizer depending on the chosen algorithm

&nbsp;Topic Modelling algorithms&nbsp;

Latent Semantic Analysis or Latent Semantic Indexing (LSA)
Latent Dirichlet Allocation (LDA)
Non-Negative Matrix Factorization (NMF)
Popular topic modelling metric score known as Coherence Score
Predicting a set of topics and the dominant topic for each documents
Running a python script end to end using Command Prompt

Code Overview

Complete dataset is splitted into 90% for training and 10% for predicting unseen documents.
Preprocessing is done to avoid noise


Lowering all the words and replacing words in their normal form and keeping only alphabets.
Making a new document after tokenizing each sentence and lemmatizing every word.&nbsp;


For LSA and LDA Topic Modeling


&nbsp;TF IDF Vectorizer and Countvectorizer is fitted and transformed on a clean set of documents and topics are extracted using sklean LSA and LDA packages respectively and proceeded with 10 topics for both the algorithms.


For NMF Topic Modeling


TF IDF Vectorizer is fitted and transformed on clean tokens and 13 topics are extracted and the number was found using Coherence Score.


Topics distribution is analyzed using t-SNE algorithm and iterative tool using pyLDAvis.
For unseen documents, topics were predicted using the above three algorithms.
"
66,Ola Bike Rides Request Demand Forecast,"Project Description
The taxi service (ride-hailing) industry is growing for the last couple of years and it is expected to grow in near future. Taxi drivers need to choose where to hang tight for passengers as they can get somebody quickly. Passengers also prefer a quick taxi service whenever needed. We all have faced problems with taxi booking requests, that sometimes cannot be fulfilled or the wait time for ride arrival is very long due to the unavailability of a nearby taxi. One should feel fortunate in the event that you get a taxi booked in one go.
Taxi demand prediction has become extremely important for taxi-hailing (and e-haling) companies to understand their demand and optimize their fleet management.
To handle such issues, we would be building a model based on users ride request dataset, which would contain attributes: ride booking time, pickup point, and drop point latitude-longitude. This model would forecast the demand, for a particular time in different areas of the city which would help the company optimize taxi concentration to fulfill users demand.
Business ProblemOla Bikes are suffering losses and losing out from their competition due to their inability to fulfil the ride requests of many users. To tackle this problem, you are asked to predict demand for rides in a certain region and a given future time window. This would help them allocate drivers more intelligently to meet the ride requests from users.
Goal
You have to predict ride requests (demand forecast) for a particular latitude
and longitude for a requested future time window/duration.
Data Description
Raw Data contains a `number` (unique for every user), ride request DateTime (IST time),
pickup and drop location latitude, and longitude.
Data Fields 

number: unique id for every user
ts: DateTime of booking ride (IST time)
pick_lat: ride request pickup latitude
pick_lng: ride request pickup longitude
drop_lat: ride request drop latitude
drop_lng: ride request drop longitude

Defining a Good Ride Request
Ola Management knows the task is not easy and very important for their business to grow.
Hence, their business team has provided you some guidelines to follow.

Count only 1 ride request by a user, if there are multiple bookings from the same latitude and longitude within 1hour of the last booking time.
If there are ride requests within 8mins of the last booking time consider only 1 ride request from a user (latitude and longitude may or may not be the same).
If the geodesic distance from pickup and drop point is less than 50meters consider that ride request as a fraud ride request.
Consider all ride requests where pick up or drop location is outside India bounding box: [""6.2325274"", ""35.6745457"", ""68.1113787"", ""97.395561""] as system error.
Karnataka is our prime city where we have a lot of drivers and ride requests to fulfil. We would not love to serve rides that are outside Karnataka and have pickup and drop geodesic distance &gt; 500kms. Karnataka bounding box: [""11.5945587"", ""18.4767308"",""74.0543908"", ""78.588083""]

Predict Task to test your model
After model development, Ola has requested us to build a prediction pipeline for the deployment of the model. To test our prediction pipeline, they have provided us clean data (filtered rides requests data based on good ride definition conditions) of 2021-03-26 based on which they have requested us to predict/show initial hours rides request demand forecast for 2021-03-27.
&nbsp;"
67,GCP Data Ingestion with SQL using Google Cloud Dataflow,"What is Data Ingestion?
Data Ingestion is defined as the transportation of data from various assorted sources to the storage medium where it can be thoroughly accessed and analyzed by any organization. The storage medium acts as a destination which is typically the data warehouse, database, data mart or any document store. The data can come from various sources such as RDBMS and other different types of databases like S3 buckets, CSVs files etc.
Data Pipeline:
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches. Right from extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into query worthy format, visualisation of KPIs including Orchestration of the above process is data pipeline.
What is the Agenda of the project?
The agenda of the project involves Data ingestion and processing pipeline on Google cloud platform with real-time streaming and batch loads. .Yelp dataset, which is used for academics and research purposes is used. We first create a service account on GCP followed by downloading Google Cloud SDK(Software developer kit). Then, Python software and all other dependencies are downloaded and connected to the GCP account for further processes. Then, the Yelp dataset is downloaded in JSON format, is connected to Cloud SDK following connections to Cloud storage which is then connected with Cloud Composer and Yelp dataset JSON stream is published to PubSub topic. Cloud composer and PubSub outputs are Apache Beam and connecting to Google Dataflow. Google BigQuery receives the structured data from workers. Finally., the data is passed to Google Data studio for visualization.
Usage of Dataset:
Here we are going to use Yelp data in JSON format in the following ways:
- Yelp dataset File: In Yelp dataset File, JSON file is connected to Cloud storage Fuse or Cloud SDK to the Google cloud storage which stores the incoming raw data followed by connections to Google Cloud Composer or Airflow to the Google cloud storage for scheduling and orchestration to batch workloads.
- Yelp dataset Stream: In Yelp dataset Stream, JSON Streams are published to Google PubSub topic for real-time data ingestion followed by connections to Apache beam for further processing.
Data Analysis:
- From the given website, the Yelp dataset is downloaded in JSON format. The Yelp JSON file is connected to Google SDK or GcsFuse for transfer of data to Google cloud storage which is connected to Google Cloud composer/Airflow for scheduling and orchestration of batch workloads.
- Yelp dataset JSON streams are published to Google PubSub which is used for real-time ingestion or streaming datasets.
- Data pipeline is created by apache beam which receives the real-time data from Google PubSub and the data from Google cloud storage as inputs which are followed by creating Google dataflow stream job and batch job scaling the compute based on throughput.
- Apache beam orchestrates stream and batch jobs following the output of Google Dataflow to workers.
- Google BigQuery acts as a Data warehouse storing structured data which receives the input from workers and queries the data.
- Finally data is visualized using different graphs and table definitions in Google Data Studio."
68,Building Real-Time AWS Log Analytics Solution,"Business ProblemThe common big data use case that we are going to take is &ldquo;Log Analytics&rdquo; where there is a requirement for analysing the log data which comes from various sources such as websites, mobile devices, sensors and applications. The tracking of application availability, fraud detection, and SLA monitoring can be achieved using log analytics. Automated trigger can be setup. The logs from different sources can be transformed to common format for easy query execution.Project Architecture&nbsp;Solution by using AWS Native Services: As now a days various applications are running over the cloud so the logs from these applications can be parsed and stored in S3. End to end log analytics solution that collects, ingests, processes and loads both the batch data and streaming data. Processed data will be available to users in near real time. The solution is highly reliable, cost effective, scales automatically to varying data volumes and require almost no IT administration.&nbsp;Services that we are going to use:Amazon S3 &ndash; This is easy to use service with a simple web services interface to store and retrieve any amount of data from anywhere on the web. It is a safe place to store the files. The data is spread across multiple devices and facilities, this is object based service and the file size can be from 0 bytes to 5 TB for uploading. There is unlimited storage and the files are stored in buckets.AWS IAM &ndash; This is nothing but identity and access management which enables us to manage access to AWS services and resources securely. We can create and manage AWS users and groups, use permissions to allow and deny their access to AWS resources. It is a feature of AWS with no additional charge.AWS EC2 &ndash; It is a service which provides resizable compute capacity in cloud and designed to make web-scale cloud computing easier. We can launch instances with a variety of operating systems.AWS kinesis firehose &ndash; In this the delivery stream is the underlying entity of firehose. Use the firehose by creating a delivery stream to a specified destination and send the data to it. The record is the data of interest which is our data producer sends to a delivery stream which can be large as 1000KB. The data producers send records to a delivery stream.AWS Glue &ndash; It is a fully managed ETL service in which we can categorise our data, clean it, enrich it and move it reliably between various data stores. It is simple and cost effective.AWS Athena &ndash; It is an interactive query service for S3 in which there is no need to load data it stays in S3. It is server less and supports many data formats e.g CSV, JSON, ORC, Parquet, AVRO.AWS Cloud Watch &ndash; It monitors our AWS resources and applications that we run on AWS in real time.&nbsp;Project Execution:Go to AWS Console and create a S3 bucket with a unique name.After that create IAM policy and role, remember we are creating the role and policy for EC2.Create an EC2 instance using the free tier storage and Amazon Linux os for it.Create a Glue database and table which is used for logs and also edit the table properties in it.Create a kinesis firehose Delivery stream and configure that stream for S3 and other things.Install the kinesis agent on the EC2 instance using External terminal (e.g Putty) and update the agent.json file into it.Start the kinesis agent by using the command and after that create a app.log file update the log file with logs.Open a duplicate terminal window run the command for reading the logs of kinesis agent.After the logs transformed and sent successfully, go to AWS S3 and see if the bucket is being updated with the &ldquo;parquet&rdquo; log file.We can check the log file which is in S3 with Query with S3 and with Athena query editor. "
69,"Build a Similar Images Finder with Python, Keras, and Tensorflow","Business Objective
&nbsp;We are all aware of how online shopping and e commerce is growing rapidly. Hence, it is imperative for computer vision systems to automatically and accurately recognize products based on images at the stock keeping unit (SKU) level. This project mainly focuses on meeting this market need. The core idea of this project is search and find images of&nbsp; products similar to any given image of a product.

Goal
To find images similar to any given image from the database
&nbsp;
Tech Stack 

Language : Python
Cloud support : AWS
Libraries : Elasticsearch, Tensorflow, Keras, Numpy, Pandas, Requests, Scikit-learn

&nbsp;
Data Overview
The dataset includes images from 2,019 product categories with one ground truth class label for each image. It includes a total of 1,011,532 images for training, 10,095 images for validation and 90,834 images for testing.
It is to be noted that for each image,only the URL is provided. Users need to download the images by themselves. It is also to be noted that the image URLs may become unavailable over time.
&nbsp;
Data Source : https://www.kaggle.com/c/imaterialist-product-2019/overview
&nbsp;
Approach

Download images from label_id - Downloading all the images using the given URLs of images.
Indexing using ElasticSearch - Feature extraction is done using the weights of imagenets from MobileNetV
Image2Image Query - Use K Nearest Neighbour in Elastic search to find K nearest vectors which are having maximum similarity for the queried image.
"
70,Medical Image Segmentation Deep Learning Project,"Business Objective 
Machine learning and deep learning technologies are increasing at a fast pace with respect to the domain of healthcare and medical sciences. These technologies sometimes even out perform medical doctors by producing results that might not be easily notable to a human eye. Polyp recognition and segmentation is one such technology which helps doctors identify polyps from colonoscopic images. 
Data Overview 
CVC-Clinic database consists of frames extracted from colonoscopy videos. The dataset contains several examples of polyp frames &amp; corresponding ground truth for them.
The Ground Truth image consists of a mask corresponding to the region covered by the polyp in the image.&nbsp; The data is available in both .png and .tiff formats
Aim 
To segment the polyps from colonoscopy images
Tech Stack 
Language used : Python
Deep learning library used : Pytorch
Computer vision library used : OpenCV
Other python libraries :&nbsp; Scikit-learn, pandas, numpy, albumentations etc. 
Approach 

Data Understanding :

Understanding the essence of the dataset 

Understanding evaluation metrics:

Understanding the metrics that are going to be used for evaluating the predictions

Unet Architecture :

Understanding Unet architecture and why is it preferred widely in building deep learning models with respect to medical sciences.

Unet ++ :

Understanding Unet++ and how is it different from Unet

Environment Setup :

Setting up a working environment for the project

Data Augmentation :

Creating new data by making modifications on the existing data

Model building :

Building Unet ++ model using pytorch

Model Training

Training the model. ( A GPU might be required since model training takes a really long time in CPUs)

Model Prediction

&nbsp;"
71,Image Segmentation using Mask R-CNN with Tensorflow,"Business Objective
Fire is one of the deadliest risks any person can encounter in his life. In a concise time frame, fire can wreck an area, say in a forest, hut, house, building, etc. Fire can lead to the loss of expensive property, and in the worst possible scenario, it can also lead to the loss of human life. Detecting fire thus becomes a significant concern.
&nbsp;
Our project will carry out fire detection by adopting an RGB (Red, Green, Blue) model based on chromatic and disorder measurement for extracting fire pixels and smoke pixels. Our main aim is to locate the position where the fire is present, which will help the authorities take proper measures to avoid any loss. To prevent this, we will build an early fire detection using image segmentation with the help of the mrcnn model. 
&nbsp;
Image Localization helps to detect the location of a single object in any given image. In our case, image localization can locate the fire in a given image.
We will be using image segmentation, in which we group a similar set of pixels, i.e., divide the image into segments and thus make use of the essential segments. Hence image segmentation is used in this project as it gives us the desired location of our object in the image. 
&nbsp;
Further, we will use the Mask RCNN model to train and build predictions over our input images. Mask RCNN is a deep neural network algorithm that is used for solving segmentation problems. 
&nbsp;
This project aims to build a deep neural network model that will give the best accuracy in detecting fire in an image.
&nbsp;
Data Description&nbsp;
&nbsp;

In this case, we use 20 fire images for the training set and ten fire images for the validation set, and one image for test data. These images can be JPG, PNG, TIF formats.
Via_project.json is the annotation file containing the region of interest marked.

&nbsp;
Aim
This project aims to make predictions over images and detect mask and bounding boxes around the area of interest, i.e., mask and bounding around the fire in a given image.&nbsp;
Tech stack

Language - Python
Libraries &ndash; numpy, keras, tensorflow, mrcnn, scikit-image, etc (as mentioned in the requirements.txt file) 

Approach

Install the requirements.txt file.
Import the required libraries.
Using the VGG Annotator, create annotations and save them in the JSON files. Then creating a class to add these annotations and use them. (Link for the VGG Annotator - https://www.robots.ox.ac.uk/~vgg/software/via/via_demo.html)
Define LoadDatatset class for loading of image data and annotations for train and inference data.
The setting of the root directory for the project and the path of the weights file.
Define config class for setting configuration for training on the dataset. 
Define the LoadBackBone for train and inference data class, for loading the backbone pre-trained weights to extract features.
Model Building (Using Mask RCNN)


Overtraining data images
Over validation data images.


Predict the mask and then plot it over the test data images.
"
72,Digit Recognition using CNN for MNIST Dataset in Python,"Business Overview
&nbsp;
The Modified National Institute of Standards and Technology dataset popularly known as MNIST dataset is widely used as a standard dataset in deep learning. &nbsp;The dataset was released in 1999. The classic dataset serves as a basis for testing most of the classification algorithms. In the emerging field of machine learning the MNIST dataset remains one of the reliable resources to test the new techniques. The training dataset contains 60,000 and the testing dataset contains 10,000 grayscale images of handwritten digits between 0 to 9 each of 28x 28 pixels. 
&nbsp;
&nbsp;
Aim: To correctly identify handwritten digits from the MNIST dataset. 
&nbsp;
&nbsp;
Data Description 
The Modified National Institute of Standards and Technology dataset popularly known as MNIST dataset is widely used as a standard dataset in deep learning. &nbsp;The dataset was released in 1999. The training dataset contains 60,000 and the testing dataset contains 10,000 grayscale images of handwritten digits between 0 to 9 each of 28x 28 pixels. 
&nbsp;
&nbsp;
Tech Stack

Language : Python
Libraries : pandas, numpy, tensorflow, matplotlib, seaborn, scikit-learn

&nbsp;
&nbsp;
Approach 
&nbsp;

Data visualization

&nbsp;

Data preprocessing

Reshaping data
One hot encoding
Feature Scaling


&nbsp;

CNN model creation and building on training data

&nbsp;

Model validation

Confusion matrix
Classification report
Loss over training and validation dataset
Accuracy over training and validation dataset


&nbsp;

Model prediction over individual images

&nbsp;"
73,Build a Face Recognition System in Python using FaceNet,"Introduction to Face Recognition System Project
Artificial intelligence that trains computers to interpret and understand the visual world is known as computer vision. Computer vision tries to replicate the functions of the human visual system to identify and process different objects in images or videos.&nbsp; It is one of the most powerful techniques used extensively to detect and label objects in images or videos. The recent inventions in artificial intelligence have outshined humans in detecting and labeling objects.
&nbsp;
Image transformation, Medical image analysis, Human pose estimation are among the numerous uses of computer vision. Object Classification, Identification, Verification, Detection, Landmark Detection, Segmentation, Recognition are some of the most popular computer vision applications. In recent years Face recognition, which tries to identify the faces of a person given an image or video and then classify them, has gained popularity. Nowadays, many phones come with a face unlocking feature that uses face recognition in the back end. One of the popular social media applications, Facebook, is trying to make its algorithm more and more robust where faces will be identified and tagged when multiple people are there in an image. When you enable your camera, you see a box surrounding a face, it is possible because of the face recognition code running in the background. Entering company premises also requires a biometric impression which is popular. Still, some are trying to go for face recognition.
What is FaceNet Model?
The FaceNet model is a facial recognition model released by a team of Google researchers in 2015 and is based upon two previously-launched models for image classification, ZF-Net&nbsp;and Inception. The model performs extraordinarily well on popular benchmark datasets, including Labeled Faces in the Wild (LFW) and Youtube Face Database.&nbsp;

Image Source: FaceNet Research Paper
&nbsp;
The image above represents the architecture of the FaceNet model. It has a deep neural network of 128 layers that produces an output of 128x1 embeddings. This output is then passed through the last layer of Triplet loss to evaluate the loss function. The basic principle of this model is that the vectors in the embedding space that correspond to the image of one person should have a small Euclidean distance with respect to each other. But, this distance should be large for vectors that correspond to images of different people.

What are Haar Cascade Classifiers?
A discussion on a face recognition project in machine learning is incomplete without introducing a haar cascade classifier. It is a classification algorithm used by computer vision engineers to perform object detection and is based on the principle of edge detection using haar-like features. The inspiration for their standard implementation in a face detection system project stems from the fact that a human face has gradients of light and dark in images; for example, the bone of our nose shines relatively brighter in an image than the region around it.&nbsp;
&nbsp;

Image Source: opencv.org
&nbsp;
Thus, using patterns that assist in detecting edges in an image, one can identify the location of a face in it. These patterns are referred to as haar-like features (shown in the image above). These features span different regions of an image to look for the presence of a face in the input image. In this project, you will use the haar cascade classifier to determine the pixel location of a face in the image and then use the FaceNet model for performing face recognition. Using these two tools, you will learn to make a face recognition system in Python.
Face Recognition using Facenet in Python Project Overview

Aim of the FaceNet Python Project
This project is one of the basic ML projects aiming to extract faces from images and identify/classify a person's face in images and videos. This face recognition python project will help you understand how to extract frames from a video, train using faces, and identify where the classified person is located in a video or an image. Challenges to overcome are mainly due to inefficient capturing of a face from a person's image and classifying it correctly if located. Our main objective is to extract weights/embeddings from pre-trained weights/models and classify them using a Machine Learning (ML) or a Deep Learning Model Technique using images.
Python Face Recognition Data Description
In this face recognition with FaceNet project, we have used a video from a popular sitcom show, Friends. You can find the video on https://www.youtube.com/watch?v=NzOTuh63eVs. The snapshots of five characters, Rachel, Chandler, Phoebe, Monica, and Ross, are taken from the video. We get the train and the test images by extracting frames per second and then extracting faces from each frame. For implementing face verification in Python, we have 35 images, consisting of 7 images per person in the training dataset and 15 images in total for the test dataset.&nbsp;
Tech Stack for the Python Facenet Project


Language: Python (Version 3.6.2)


Libraries: OpenCV, scikit-learn, NumPy, os, pytube, scikit_image, skimage, Keras, TensorFlow


OpenCV for loading images and videos


Haar Cascade Algorithm for face extraction&nbsp;


Extracting embeddings from a pre-trained FaceNet Model


Convolution Neural Networks for training purposes


Dense Neural networks to classify images


VGGFace Architecture&nbsp;


Matplotlib for plotting images


Numpy for array and math operations


Tensorflow and Keras for loading and building models


Skimage for resizing images


Imutils for video reading


os for operating folders


Face Recognition in Python: Solution Approach


Downloading video from YouTube using python


Extracting frames per second from the video


Face extraction using with Haar cascade Algorithm


Modifying every image as per Facenet model requirements


Extracting embedding and normalizing as per the model requirements


Fitting SVM model and predictions on test data


Visualizing normal embedding using TSNE


Model testing on frames and predicting faces


Prediction on videos


Creating VGG Face model architecture


Data preparation for modeling


Model training and predictions


Methodology for Performing Face Detection in Python


The images of each character are obtained from the Youtube video and stored in a separate folder.


Each face from the respective folder is read and resized using the required format for the model requirements.


Then pixels extracted are required to go specific pre-processing to extract meaningful information from weights or a model.


Depending on the requirement, the extracted embeddings or weights are sent as input to an ML/DL model.


Faces from a frame or a video are to be extracted using Haar Cascade Object Detection for face extraction.


Model training for identifying faces extracted from Haar Cascade and labeling if it meets certain thresholds and others.


Label a person&rsquo;s name in frames or a video.


FAQs for Project on Face Recognition using Python
How is Python used in Face Recognition?
Python offers an easy face detection application through its Computer Vision library, cv2, and has TensorFlow for the FaceNet model&rsquo;s implementation. The cv2 library has cascade classifiers that quickly identify faces in an image.
Which algorithm is used for Face Recognition?
There are plenty of Face Recognition algorithms used for face recognition like FaceNet, Eigenfaces, Fisherfaces, Local Binary Patterns Histograms (LBPH), etc."
74,Locality Sensitive Hashing Python Code for Look-Alike Modelling,"Business Objective
Let&rsquo;s say you wish to generate brand awareness of your product and increase its sale; online advertising is the easiest way to do it. Online advertising is considered one of the most effective and efficient ways for businesses of all sizes to expand their reach, find new customers, and diversify their revenue streams.
Online advertising is the art of using the internet as a powerful platform to deliver marketing messages to an intended audience. Social media is one the most popular online pastimes for people worldwide, and advertisers have evolved their strategies to target consumers on social media sites like Facebook, Instagram, Twitter, etc. It helps in attracting website traffic and brand exposure, which in turn helps in increasing sales. Hence, online adverting is designed in such a way as to persuade the targeted customer to make a purchase.&nbsp;
But, when these ads get advertised, not all people will be interested in the product/service, and only those interested click on the ads. Click rate is a term that helps us find the percentage of people who have watched your ad online and have ended up clicking it. Hence the goal of online advertising is to reach maximum and relevant users.&nbsp;
So, to find these relevant users/customers, we make use of the Lookalike model.&nbsp;
Lookalike models are models used to build larger audiences from smaller segments to create reach for advertisers. The more significant users reflect the benchmark characteristics of the original users, which are known as the seed users.&nbsp;
In this project, we will build a Lookalike model that will help us find similar and relevant customers with the help of the LSH &ndash; Locality Sensitive Handling algorithm. This algorithm will hence improve the click rate.
&nbsp;
Data Description&nbsp;
&nbsp;
The dataset used is from a company called &lsquo;Adform&rsquo;. This dataset is from a particular online digital online campaign. This ad was shown to several thousand people and recorded whether the people clicked the ad or not. As the dataset is vast, only a subset of the dataset was considered. Following is the description of the data:
The file is gzipped and each line corresponds to a single record, serialized as JSON. The JSON has the following fields:


""l"": The binary label indicating whether the ad was clicked (1) or not (0).


""c0"" - ""c9"": Categorical features which were hashed into a 32-bit integer.


Here, c6 and c9 have multiple values perusers and the rest have single values per user.


To know more about the dataset, access the following link:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TADBY7


Aim
&nbsp;
To build a lookalike model to find similar users using the Locality Sensitive Hashing algorithm and find an increase in click rate w.r.t the default click rate.
&nbsp;
Tech stack&nbsp;


Language - Python


Libraries - scikit-learn, pandas, numpy, pickle, yaml, datasketch ,


Approach&nbsp;

Importing the required libraries and packages
Open the config.yaml file. (This is a configuration file that can be edited according to your dataset) 
Read the JSON file
Clean the JSON file


Reset index in the data
Convert list to integers
Remove rows above certain threshold values
Replace empty values with an empty list if any
Store the cleaned file
Calculate feature counts for scoring 


Model Training 


Create a MinHashForest Model
Create an LSH graph object
Train the model
Save the model to a pickle file


Seed set Extension


Read the saved model
Read the seed set data
Retrieve the neighbors of seed set from LSH graph
Calculate the default click rate
Score the neighbors
Create and store the extension file
Find the increased click rate.
"
75,Forecasting Business KPI's with Tensorflow and Python,"Business Objective
Branding is one of the most important marketing strategies which helps a brand to grow. There are different ways these marketing strategies are carried out. For example, holdings, television ad breaks, during a cricket match, etc.&nbsp; Branding helps the brands stand out in a crowded market and hence they intend to spend resources on brand awareness in order to reach the targeted customers.
Return on investment (ROI) is a&nbsp;performance measure used to evaluate the efficiency or&nbsp;profitability&nbsp;of an investment made. For example, let&rsquo;s say Pepsi company launched a new flavor of the drink and started advertising it via commercial breaks on television. The company invests some sort of amount in this branding of their drink and the return on investment will be nothing but the number of sales of the drink.
In our project, we are checking our brand exposure by calculating some KPI metrics on an IPL match video. Our main aim is to find the number of times a brand logo is shown during the match (total appearances), the largest and smallest area percentage, and the total area of the logo per count for every logo during the match.
Data Description&nbsp;
For our case study, we will be using a video clip of an IPL match played between CSK and RCB teams. This dataset is downloaded from YouTube and is 2 minutes 35 seconds long.&nbsp; Further processing of this data will convert the video clip into frames. Using annotators, we will convert the data into XML files which will further be converted into CSV files on basis of our key performance metrics.
Aim
To find the KPI metrics for each brand logo, such as the number of appearances of the logo, the area, frames, and shortest and longest area percentage, for the given input video clip.
Tech stack

Language - Python
Libraries &ndash; TensorFlow, pillow, opencv, matplotlib, NumPy, uuid

Approach&nbsp;


Download the input video&nbsp;






From YouTube


Using python (youtube_downloader.py)






Use the annotation tool (LabelImg) to convert the images into XML files


Set up TensorFlow for object detection i.e git clone


Convert the XML files to CSV files (the CSV files contain the width, height, depth, xmin, ymin, xmax, ymax per image)


Convert the CSV file into tfrecords file (for train and test)


Download the base model from TensorFlow model zoo1 (eg: ssd resnet 50fpn coco) and unzip the file.


From the folder (ssd resnet 50fpn coco) open the pipeline.config file and make the changes as shown in video


Now open the model&rsquo;s folder, open the research folder, followed by the object detection folder, and open the legacy folder. In the legacy, folder open the train.py file and start the training.


Once the training is completed, the results are saved in ckpt files.


The results of the training can be monitored on the tensor board by using the events file generated along with the ckpt files.


Freeze the model - Cktp has 3 files, data, index, and meta, and these three files have to be combined into one file and this is called freezing of the model.&nbsp;


From the models/research/object_detection folder opens the export_inference_graph.py. Perform the command as shown in the video. This generates the frozen_inference_graph.pb which will be used for further inferences.


The frozen_inference_graph.pb, the labels.txt, and the test_video.mp4 are the final inputs on basis of which we will measure the KPI metrics.


Make predictions on the test images using the predict.py file


Tweak visualization utils to return box classes scores&nbsp;&nbsp;


Process the video into frames


Finally, run the video_processing.ipynb to generate the output i.e find the KPI metrics for each brand logo.

"
76,Build OCR from Scratch Python using YOLO and Tesseract,"Introduction to OCR in Python Project
People have been relying on paper invoices for a very long time, but these days, all have become digital, and so have invoices. Reconciling digital invoices is a laborious job as it requires employees to spend hours browsing through several invoices and noting things down in a ledger. But, what if we told you we could automate this, and you can save on those human hours spent as a business? Yes, it is possible because of the incredible data science tools like YOLO and Tesseract that one can use to create OCR in Python. OCR stands for optical character recognition, and in this project, we will explain how to build OCR from scratch in Python.&nbsp;

Building OCR from Scratch in Python Project Overview
This machine learning project deals with training the YOLO object detection model using the dataset of digital invoices. The model is trained to identify three essential classes from the invoices, Invoice number, Billing Date, and Total amount. After that, you will use Tesseract for performing OCR in python.
Tech Stack
Language: Python
Object detection: YOLO V4
Text Recognition: Tesseract OCR
Environment: Google Colab
Application
Any business currently going through all the bills manually to jot it down in a ledger can use this project.

Topics Covered in this YOLO-OCR Project
Below we have mentioned in detail all the data science tools and techniques that you will use to implement the solution of this project.
Computer Vision using OpenCV
OpenCV is one of Python&rsquo;s most popular computer vision and image processing libraries. Before serving any image to the object detection model YOLO, it must be processed, and for that purpose, you will use OpenCV. Additionally, for visualizing the testing results of the YOLO model, one relies on various functions of the OpenCV library.&nbsp;
Coding with Google Colab
Google Colab is an application hosted by Google in the cloud that allows its users to build executable programs in Python. In this YOLO character recognition project, you will learn to use Colab notebooks to implement the complete solution. You will learn how to link the darknet OCR framework for training the YOLO v4 model, execute terminal commands in colab notebooks, and do many more exciting tasks. Colab uses the power of Graphical Processing Units (GPU) for performing this task at a much faster speed than CPU tasks. You will also learn how to change the runtime in Colab and set it to GPU for faster execution
Text Detection using YOLO
YOLO v4 is an object detection model developed by Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao where YOLO stands for &lsquo;You only look once&rsquo;. Its quirky name comes from the algorithm identifying all the objects in an image by looking at it only once. And that is one of the primary reasons why the algorithm can detect objects faster than the RCNN algorithms. This project will use the YOLO algorithm to build a custom OCR with Python. The reason behind building a custom OCR model is that YOLO only knows how to identify 80 predefined classes of the COCO dataset. Thus, this project will guide you through transfer learning to create a YOLO-text-recognition model using the invoices dataset. As specified already, this custom OCR system will identify the three objects from the invoice images: invoice number, Billing Date, and Total amount and create a bounding box around them once the respective entities have been identified.
OCR using Tesseract
With YOLO, the system will recognize the vital text classes from the invoices but to decode the information in the text; one must utilize Optical Character Recognition (OCR). Tesseract OCR is a tool that quickly scans text and converts it into digital data. In this project, you will learn how to use Tesseract OCR for creating a custom OCR in Python.
Step-by-Step Instructions on How to Build Python OCR
Here is a step-by-step guide on building OCR from scratch in Python -
&nbsp;


Setting up and Installation to run Yolov4


Downloading AlexeyAB's famous repository, we will adjust the Makefile to enable OPENCV and GPU for darknet and then build darknet.


Downloading pre-trained YOLOv4 weights


YOLOv4 has already been trained on the coco dataset, with 80 classes that it can predict. We will take these pre-trained weights to understand how they result in some test images.


Creating display functions to display the predicted class.


Here, you will learn how to use OpenCV for visualizing object detection results of the YOLO model.


Data collection and Labeling with LabelImgThis YOLO OCR project aims to train YOLO to learn three new classes; you will create&nbsp; a new dataset for training and validation. You will create this new dataset with the help of the Labellmg tool that will annotate the image with three classes, and YOLO will then use these annotations during training.


Configuring Files for Training - This step involves configuring custom .cfg, obj.data, obj.names, train.txt and test.txt files.




Configuring all the needed variables based on class in the config file


Creating obj.names and obj.data files


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. obj.names: Classes to be detected

&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. obj.data:

&nbsp;


Configuring train.txt and test.txt




Download pre-trained weights for the convolutional layers


YOLO's object detection model has already been trained on the COCO dataset for 80 different classes. One can download these weights and then fine-tune them accordingly with the help of their custom dataset. The great part about this is the fact that even with fewer data points, by just adding a couple of layers of learning on top of existing ones, the model can learn and adapt to the new classes.


Training Custom Object Detector


Evaluating the model using Mean Average precision


Predict image classes and save the coordinates separately


Detecting text from the predicted class




Importing pytesseract and setting environment variable (for windows only, for Unix it is already set) for English trained data


Getting the list of predicted files from the directory


Using tesseract pre-trained LSTM model to extract the text


Fine-tuning the LSTM model. (Please note that fine-tuning the model will only be required if the extracted text is inaccurate to that shown in the image)


FAQs on How to Build an OCR in Python
1) How to train Tesseract OCR python?
Follow the below steps to train Tesseract-OCR in Python:


Install Tesseract from the website Home &middot; UB-Mannheim/tesseract Wiki &middot; GitHub.


Use jTessBoxEditor for merging train data to .tiff format.


Generate .box files from the .tff files to create the training dataset.


Use the above dataset for training the algorithm.


2) How to Install Tesseract OCR in windows python?&nbsp;
Visit the website https://github.com/UB-Mannheim/tesseract/wiki#tesseract-at-ub-mannheim and download the windows installer for tesseract depending on whether your system is 32-bit/64-bit. After that, run the downloaded file and wait until the system extracts all the necessary files. Click on &lsquo;Next&rsquo; and accept the license agreement to install. If you are interested in OCR for languages other than English, tick the additional language tools box and select the language of your choice. After that, click on the &lsquo;Next&rsquo; button and wait for the installation process to get over. Finally, click on &lsquo;Finish&rsquo; to complete the installation process."
77,AWS Project-Website Monitoring using AWS Lambda and Aurora,"What is Website Monitoring?
&nbsp;
Website Monitoring is defined as the term used for any activity which involves testing a website or any web service for the availability, performance, or function. It&rsquo;s the process of testing and also verifying that the end-users can interact with the website or the web application as expected. The Website Monitoring service checks and verifies the website is up and working as expected and website visitors can use the site without facing any difficulties as expected.
&nbsp;
&nbsp;
Data Pipeline:
&nbsp;
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches. Right from extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into query worthy format, visualization of KPI's including Orchestration of the above process is data pipeline.
&nbsp;
&nbsp;
Agenda of the project:
&nbsp;

The agenda of the project involves Real-time monitoring of websites using AWS services. 
We first launch an EC2 instance on AWS and install Amazon Kinesis in it. 
Then, Data analytics streams are created in amazon Kinesis for real-time streaming of data. Then after launching AWS EC2 instances, an Identity Access Management(IAM) role is assigned to the EC2 instance. 
It is followed by doing Log streaming to Kinesis data streams, followed by the Creation of Kinesis Analytics. It is followed by the creation of Amazon Aurora MySQL followed by using Amazon SNS service and Secret Manager.
 Further, AWS Lambda code is run for end-to-end testing and Amazon DynamoDB is run with a selection of Keys. 
It is followed by the creation of AWS Lambda for loading in Amazon DynamoDB, Loading DynamoDB with order logs, and finally Real-time streaming the data in the Kinesis Analytics.

&nbsp;
&nbsp;
Architecture:
&nbsp;
1) Amazon EC2 acts as website backend generating server logs. 
2) Kinesis Datastreams reads the server logs in real-time and pushes them to Kinesis Data Analytics for doing the above computation(more than 15 orders per 15 seconds).
 3) The Second stream in Data Analytics is created that actually notes such floods for the past 1 minute and then sends only messages to 2nd Data stream if the trend is noted continuously for 1 min. This step is purely added to reduce the number of SNS messages received in case of a spike.
 4) Second data stream is used to receive such alarm records and trigger lambda. 
5) Lambda triggers SNS notification which in turn delivers an SMS message. It saves the copy of all the error messages in Aurora MySQL for aggregated views in the future.
&nbsp;
&nbsp;

&nbsp;
&nbsp;
&nbsp;
Data Analysis:&nbsp;
&nbsp;

From the given website, PUTTY is downloaded to run Linux shell in Windows for running website monitoring using Amazon Kinesis.
AWS EC2 instance is created and PUTTY shell is connected to EC2 instance. Amazon Kinesis is downloaded in the EC2 instance followed by the addition of python files and dataset.
The Amazon Kinesis data streams are creating in the AWS console followed by attaching Identity Access Management(IAM) to the EC2 instances.
The Amazon Kinesis analytics is created for performing analytics of website real-time streaming. Amazon Aurora MySQL is created using a relational database.
The AWS Lambda is created and its code is executed for end-to-end testing so as to enable smooth analytics.
The Amazon DynamoDB is created for the NoSQL database and keys are selected in it. Amazon DynamoDB is loaded in AWS Lambda.
Finally, Amazon DynamoDB is loaded with order logs, and data is analyzed in real-time streaming in the Amazon Kinesis Data Analytics application.
"
78,Time Series Python Project using Greykite and Neural Prophet,"Business Overview
A sequence of data points collected often at a constant time interval of a given entity is known as time series. The measure question asked by any business is &ldquo;how did the past influence the future?&rdquo;. Forecasting is a process by which the future observation is estimated by using historical data. A statistical method that is used to analyze the data taken over time to forecast the future is known as time series forecasting. It is used to analyze time-based patterns in data and hence to determine a good model to forecast future behavior. Basically time series connect the past, present, and future.
From&nbsp; Supply chain, Stocks to weather, biomedical monitoring forecasting is used everywhere. There are two main use cases in forecasting. The first being store sales prediction to manage inventory demand and plan ahead accordingly. The second is ride-hailing demand for pricing and supply chain management. One of the importance of forecasting is if a holiday comes over how one should plan store sales to get maximum sales hence the profit. In this project, we will use Walmart store sales data to predict store sales. Greykite, a python library developed by Linkedin, and the famous Neural Prophet model developed by Facebook are used to forecast the demand.&nbsp;&nbsp;&nbsp;
Aim
To predict future demand/sales using historical data and other related features.&nbsp;
Dataset
https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data
Data Description
The dataset used is Walmart store sales data. Walmart is an American multinational retail corporation that operates a chain of hypermarkets, department stores, and Grocery stores. The dataset provided is historical sales data for 45 Walmart stores located in different regions. &nbsp;Each store contains many departments. Four different datasets are being provided which are discussed below.

Stores.csv: This file contains information about the 45 stores, indicating the type and size of the store.
Train.csv: This is the historical training data, which covers 2010-02-05 to 2012-11-01.
Test.csv: This file is identical to train.csv, except for the weekly sales which have to be predicted.&nbsp;
&nbsp;Features.csv: This file contains additional data related to the store, department, and regional activity for the given dates.&nbsp;

The basic information about the features available in the data is as follows.

Store - the store number
Date - the week
Dept - the department number
Temperature - the average temperature in the region
Fuel_Price - the cost of fuel in the region
MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011 and is not available for all stores all the time. Any missing value is marked with an NA.
CPI - the consumer price index
Unemployment - the unemployment rate
IsHoliday - whether the week is a special holiday week
Weekly_Sales -&nbsp; sales for the given department in the given store

Tech Stack

Language: Python
Libraries: greykite, neural prophet, sci-kit learn, pandas, pandas_profiling, matplotlib, datetime, plotly, seaborn, numpy

Approach&nbsp;

Exploratory data analysis (EDA)

Inference about features
Data visualization using pandas profiling




Data cleaning (outlier/missing values)&nbsp;

Missing value imputation
Outlier detection




Feature Engineering

Extracting day, month, and year from date&nbsp;
Mapping




Time series component analysis

Trend
Seasonality




Model building on training data

Silverkite
Neural Prophet




Model validation

Mean Absolute Percent Error
RMSE




Forecasting using trained models
"
79,Abstractive Text Summarization using Transformers-BART Model,"Introduction to Text Summarization using Transformers
Summarization has closely been and continues to be a hot research topic in the data science arena. Summarization is a technique that reduces the size of a document while preserving its meaning. It is one of the most researched areas among the Natural Language Processing (NLP) community.
&nbsp;
Summarization techniques are categorized into two classes based on whether the exact sentences are considered as they appear in the original text. New sentences are generated using NLP techniques, extractive, and abstractive summarization.&nbsp;
Extractive Text Summarization
The most meaningful sentences in an article are selected and arranged comprehensively in extractive summarization. In other words, the summarized sentences are extracted from the article without any modifications.
Abstractive Text Summarization
An NLP task aims to generate a concise summary of a source text. Abstractive summarization does not simply copy essential phrases from the source text but also potentially develops new relevant phrases, which can be seen as paraphrasing.
&nbsp;
Abstractive summarization has several applications in different domains such as,


Science and R&amp;D


Books and literature.&nbsp;


Financial research and legal documents analysis


Meetings and video conferencing&nbsp;


Programming languages, etc



BART for Summarization of Text Data
BART stands for Bidirectional and Auto-Regressive Transformer. Its primary features are a bidirectional encoder from BERT and an autoregressive decoder from GPT. The encoder and decoder are united using the seq2seq model to form the BART algorithm. Let us look at it in more detail.
BART Model Architecture
To understand the BART transformer, one needs to closely look at BERT and GPT. BERT performs the Masked Language Modelling with the help of its bidirectional transformer and predicts the missing values. On the other hand, GPT uses its autoregressive decoder to predict the next token in a sentence. Merging both of these results in the BART model, as depicted in the image below.
&nbsp;

BART Pre-training
There are five primary methods for training BART with noisy text:


Token Masking: Randomly, a small number of input points are masked.


Token Deletion: Certain tokens from the document are deleted.


Text Infilling: Multiple tokens are replaced with a single mask token.


Sentence Permutation: Sentences are identified with the help of &lsquo;.&rsquo; and are then shuffled for training.


Document Rotation: A token is randomly selected, and the sequence is rotated so that the document begins with the chosen token.


These strategies augment the dataset and make the BART model better understand the natural language.
BART Fine-Tuning&nbsp; Down Stream Tasks&nbsp;
Depending on the task one wants to perform using BART, they can fine-tune the model as discussed in the section below:


Sequence classification: To perform sequence classification using BART, we feed the same input to the encoder and the decoder. The final decoder token's final hidden state is fed into a new multi-class linear classifier.


Token classification: For solving classification problems using BART,&nbsp; the complete document is passed into the encoder and decoder, and the top hidden state of the decoder is used as a representation for each word. One then uses this representation for the classification of tokens.


Sequence generation: As an autoregressive decoder is a part of the BART model&rsquo;s architecture, we can use it for sequence generation problems. The input at the encoder acts as the input, and the decoder generates the output autoregressively.


Machine translation: Unlike other state-of-the-art models, BART combines both an encoder and a decoder, making it suitable for English translation. To add a new set of encoder parameters (learn using bitext) to the model and use BART as a single pre-trained decoder for machine translation.


Abstractive Text Summarization using Transformers Project&nbsp; Overview&nbsp;
This project aims to build a BART model that will perform abstractive summarization on a given text data.
&nbsp;

Dataset for Text Summarization using BART
The data used is from the curation base repository, which has a collection of 40,000 professionally written summaries of news articles, with links to the articles themselves.
The data was downloaded in the form of a CSV file and has the following features:


Article titles &ndash; title for the texts


Summaries &ndash; Summary for each text


URLs &ndash; the URL links


Dates


Article content &ndash; content under each article&nbsp;


Aim of the Bart Text Summarization Project
The BART model will perform abstractive text summarization in Python on a given text data.
Tech Stack used in the Abstractive Text Summarization Python Code


Language - Python


Libraries - pandas, sklearn, PyTorch, transformers, PyTorch Lightning&nbsp;


Environment &ndash; Google Colab


BART Summarization Project: Solution Approach


Import the dataset from the dataset library and load a subset of the data. (To get an overview of the summarized data)


Clone the repository.


Download the article titles, summaries, URLs, and dates (CSV file)


Create a new environment, install the requirements and scrape the data.


Change the runtime to GPU.


Import the required packages and libraries.


Create a class function for the dataset.


Create a class function for the BART data loader.


Create an abstractive summarization model class function.


Create a BART tokenizer&nbsp;


Define the data loader&nbsp;


Read and prepare the data.


Perform train test split.&nbsp;


Create the main class that runs the &lsquo;BARTForConditionalGeneration&rsquo; model and tokenizer as an input.


Define the trainer class and then fit the model.


Perform the BART summarization using the pre-trained model.


Understand the concept behind the BART evaluation metric &ndash; Rouge.


For running the web application:




Create a new environment


Install the requirements.txt file


Go to the output folder.


Run the app.py&nbsp;


Go to localhost 5000 port.


Give a particular article link for scrapping.


Summary for the provided article gets generated.


FAQs on Transformers-based Text Summarization
1) Is BART better than BERT?
Unlike BERT, BART leverages the functionalities of both encoders and decoders by combining them in a sequence-to-sequence fashion. It can thus learn original text better and performs better than BERT &nbsp;in most cases.
2) How many parameters does the BART model have?&nbsp;
The BART model contains 406M parameters and 1024 hidden layers. The model has been fine-tuned using CNN, a news summarization dataset."
80,Word2Vec and FastText Word Embedding with Gensim in Python,"Business Objective 
The biggest challenge in the NLP (Natural Language Processing) domain is to extract the context from text data, and word embeddings are the solution that represents words as semantically meaningful dense vectors. They overcome many of the problems that other techniques like one-hot encodings and TFIDF have.
Embeddings boost generalization and performance for downstream NLP applications even with fewer data. So, word embedding is the feature learning technique where words or phrases from the vocabulary are mapped to vectors of real numbers capturing the contextual hierarchy.
General word embeddings might not perform well enough on all the domains. Hence, we need to build domain-specific embeddings to get better outcomes. In this project, we will create medical word embeddings using Word2vec and FastText in python.
Word2vec is a combination of models used to represent distributed representations of words in a corpus. Word2Vec (W2V) is an algorithm that accepts text corpus as an input and outputs a vector representation for each word. FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.
This project aims to use the trained models (Word2Vec and FastText) to build a search engine and Streamlit UI.
Data Description&nbsp;
We are considering a clinical trials dataset for our project based on Covid-19. The link for this dataset is as follows:
Link:https://dimensions.figshare.com/articles/dataset/Dimensions_COVID-19_publications_datasets_and_clinical_trials/11961063
There are 10666 rows and 21 columns present in the dataset. The following two columns are essential for us,

Title
Abstract

Aim
The project aims to train the Skip-gram and FastText models for performing word embeddings and then building a search engine along with a Streamlit UI.
Tech stack

Language - Python
Libraries and Packages - pandas, numpy, matplotlib, plotly, gensim, streamlit, nltk.

Environment &ndash; Jupyter Notebook
Approach 

Importing the required libraries
Reading the dataset
Pre-processing




Remove URLs
Convert text to lower case
Remove numerical values
Remove punctuation.
Perform tokenization
Remove stop words
Perform lemmatization 
Remove &lsquo;\n&rsquo; character from the columns




Exploratory Data Analysis (EDA)&nbsp; 




Data Visualization using word cloud




Training the &lsquo;Skip-gram&rsquo; model
Training the &lsquo;FastText&rsquo; model
Model embeddings &ndash; Similarity 
PCA plots for Skip-gram and FastText models
Convert abstract and title to vectors using the Skip-gram and FastText model
Use the Cosine similarity function
Perform input query pre-processing
Define a function to return top &lsquo;n&rsquo; similar results &nbsp;
Result evaluation
Run the Streamlit Application
"
81,Deep Learning Project- Real-Time Fruit Detection using YOLOv4,"Introduction to YOLO&nbsp;Fruit Detection Project
In the rapid development of technology, significant concerns are given to the food we consume. One of the most cost-demanding factors in the agriculture industry is skilled labor. The industry is moving towards automation to decrease work costs and increase quality. Robotic harvesting can provide potential solutions to many such problems the industry faces. There are numerous challenging tasks to be fulfilled by the upcoming technology, one of them being an accurate fruit detection system. Different technologies have been used for fruit recognition using emerging computer vision technologies.

The particular project discusses building a robust model for fruit detection. There can be many advanced use cases for this, and some of them are:


You are working in a warehouse where lakhs of fruits come in daily, and if you try to separate and package each of the fruit boxes manually, it will require a lot of workforce. So, you can build an automated system that can detect fruits and separate them for packaging.


You are the owner of a vast orchid. If you want to harvest fruits from it manually, it will require a lot of workforce. You can build a robot or a self-driving truck that can detect fruits on specific trees and harvest them for you.


&nbsp;

Real-Time Custom Object Detection using YOLOv4 Project Overview
Here we give you a general layout of the YOLO v4 object detection project solution.
Aim of the YOLOv4 Object Detection Project
To build a robust fruit detection system using the YOLOv4 object detection model.
Fruit Object Detection Dataset
Suppose you are a newbie in data science and are looking for machine learning projects for beginners. In that case, we suggest you first work with Google&rsquo;s Open Images Dataset and use the default YOLO v4 model. After completing that, you should move ahead with building a custom object detection model using YOLO v4. The dataset can be created by manually downloading the image using google search or by clicking the photographs of different fruits available.
Tech Stack involved in this Object Detection Project


Language: Python


Object detection: YOLOv4


Data annotation: LabelImg


Environment: Google Colab


Fruit Detection Python Code Roadmap


Data Collection and Labeling with LabelImg


To create a custom object detector, we need an excellent dataset of images and labels so that the sensor can efficiently train to detect objects.
We can do this in two ways.


Using Google's Open Images Dataset


We can gather thousands of images and their auto-generated labels within minutes. Explore that dataset here!


Creating your dataset and then labeling it manually


We will create a dataset manually by collecting images from google image scraper or manually clicking them and then marking them using an image annotation tool, LabelImg.


Building a YOLOv4 Object Detector with Darknet in the Cloud




Enabling GPU within your notebook


Cloning and Building Darknet


We are downloading AlexeyAB's famous repository and adjusting the Makefile to enable OPENCV and GPU for darknet and then build darknet.


Demo on Pretrained model


YOLOv4 is trained on the coco dataset, which has 80 classes that it can predict. We will take these pre-trained weights to see how it results on some of the images.


Customize YOLOv4 with the different command-line flags




Threshold Flag


Output Bounding Box Coordinates


Don&rsquo;t Show Image


Multiple Images at Once




Preparing dataset for training Yolo


Labeled Custom Dataset


Custom .cfg file




Edit the yolov4.cfg for your custom model training using the google colab editor&nbsp;


obj.data and obj.names files


Create a new file within a code or text editor called obj.names where you will have one class name per line
Example for multiclass obj.names file:

You will also create an obj.data file and fill it in accordingly.



train.txt and test.txt file


The train.txt and test.txt files hold the relative paths to all our training and validation images.


Train Your Custom Object Detector


Training Yolo model from a checkpoint


Model Evaluation using Mean Average Precision


Predictions on images


Predictions on Video


Data Science Concepts to explore in this YOLOv4 Object Detection Project
Below you find the details of data science concepts that are discussed in this object detection project.
Data Preprocessing
This project will build a system that can perform fruit detection using OpenCV code. And for that, you will first need to prepare the dataset for an object detection model like YOLO v4. This project will help you learn how to create that dataset and split it for training the model. Additionally, you will know how to split the dataset efficiently for testing and training purposes.
Object Detection
Unlike image classification, object detection involves labeling parts of an image where the specific object is located. This project will teach you how to build a YOLOv4-based custom object detection model. You will explore the architecture and working of the YOLOv4 model in detail. Additionally, the project will teach you the reasoning behind the quirky acronym YOLO that stands for &lsquo;You only look once&rsquo;. Furthermore, you will learn how to download the pre-trained version of the model and tweak it to make it capable of fruit detection for your customized dataset. The project will also cover the evaluation of an object detection model using various statistical tools. Lastly, this project will guide you on implementing the object detection system over each frame of a live video.
Transfer Learning
YOLO v4 is a pre-trained model that data scientists widely use for completing object detection tasks. It can only identify 20 objects, but one can build their customized object detection model using transfer learning. In this project, you will learn how to use your dataset and annotate it using the LabelImg tool. The project will help you understand the significance of data annotation for YOLOv4 transfer learning.
FAQs on Custom Object Detection using YOLOv4
1) What objects can YOLO detect?
The YOLO model can detect 20 types of objects by default which include:


Person


Animals: bird, cat, cow, dog, horse, sheep


Vehicles: airplane, bicycle, boat, bus, car, motorbike, train.


Others: bottle, chair, dining table, potted plant, sofa, tv/monitor.


2) Can YOLO detect small objects?
The YOLO object detection model doesn&rsquo;t perform well when detecting a small object. The problem is that the feature map YOLO use has a very low resolution and the small object features get too small to be detectable.&nbsp; However, one can improve the receptive field for YOLO by using image processing methods such as tiling."
82,Create Your First Chatbot with RASA NLU Model and Python,"Business Objective 
In simple terms, a chatbot is a computer program that simulates and processes human conversation, either written or spoken, allowing humans to interact with digital devices as if they were communicating with a real person. 
Chatbots can be as simple as a program that can answer a simple query with a single-line response or as sophisticated as digital assistants that learn and evolve to deliver increasing levels of personalization as they gather and process information.
There are two main types of chatbots:

Rule-based chatbots&nbsp;

In a rule-based type of bot, communication is through a pre-defined rule. User input must conform to these pre-set rules to get an answer. Example:&nbsp;A chatbot that answers basic customer inquiries like the status of one&rsquo;s delivery, returns request process, tracking information, etc.&nbsp;

AI-based chatbots

AI chatbots make use of machine learning to understand the context and intent of a question before formulating a response. These chatbots generate their own answers to more complicated questions using natural-language responses. The more you use and train these bots, the more they learn and the better they operate with the user.
The working of an AI-driven chatbot depends on two main concepts, the intent and the entity.&nbsp;

Intent - Intention or purpose of the user in the conversational flow.
Entity - A data point or value which you can extract from a conversation

In our case study, we will use the Rasa NLU model to build the chatbot. 
Rasa is an open-source machine learning framework for automated text and voice-based conversations. Understand messages, hold conversations, and connect to messaging channels and APIs.
We will build an AI-based chatbot using an E-Commerce business case.
Data Description&nbsp;
The data can be curated from the following two data curation websites,

Rasa NLU trainer: https://rasahq.github.io/rasa-nlu-trainer/
Chatito: https://rodrigopivi.github.io/Chatito/
The business case we will be considering is E-Commerce,
The intents consist of product_info, ask_price, cancel_order
The entities consist of product, location and order_id

Aim
To build an AI chatbot using the Rasa NLU model.
Tech stack 
Language - Python
Libraries &ndash; pandas, matplotlib, rasa, pymongo, tensorflow, spacy 
Approach 

Perform data curation to obtain the data
Importing the required packages and libraries 
Import the data
Create a function to convert the data into training and testing dataframes
Convert the dataframes to JSON files
Exploratory Data Analysis (EDA) &ndash; 


Data Visualization


Create configuration files (.yaml) for spacy and TensorFlow.
Model Building


Define a function to train the Rasa NLU model.


Model Evaluation


Define a function to perform a model evaluation on test data


Train the data using spacy as the pipeline
Train the data using TensorFlow as the pipeline
Plot confusion matrix for both the models
Interpreting the model
Install MongoDB and import pymongo
Create an IntentFlow class
Create a ContextManager class
Create a function for processing a message.
Test the chatbot
"
83,"Deploying auto-reply Twitter handle with Kafka, Spark and LSTM","Business Overview:
Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on enabling computers to interpret text and spoken words in the same manner that humans do. Several NLP jobs help the computer understand what it imbibes by breaking down human text in ways that make sense.&nbsp;
One such task is social media sentiment analysis, which obtains hidden data insights from social media platforms. Natural language processing (NLP) has become an indispensable commercial tool. Sentiment analysis extracts attitudes and emotions in reaction to products, promotions, and events by analyzing the language used in social media postings, comments, and reviews. Companies may utilize this data for various purposes, including product design, customer service, and advertising campaigns. These tools can be clubbed with NER (named entity recognition), a technique for recognizing words or sentences as valuable entities. The whole process can be automated and deployed on the cloud using data ingestion and streaming services with distributed processing.&nbsp;
The objective of this Twitter Support project is to listen to live tweets with required tags and publish them to a Kafka topic which will be consumed using Spark Stream and passed through an NLP pipeline for further processing and replying. The tweets will then be classified based on sentiment and query category using machine learning models like LSTM before responding to them with a custom ticked ID using Flask and tweepy API.
Aim:
This project automatically replies to query-related tweets with a trackable ticket ID generated based on the query category predicted using deep learning models. The whole process is automated with tweepy API and Big Data techniques with final deployment on AWS.
Data Description:
For the training purposes in this project, we have used an &ldquo;airline tweets&rdquo; dataset, which includes necessary fields like airline names as tags, actual text, sentiment class, i.e., positive, negative, or neutral and a topic class which can be one of these:

Baggage Issue
Customer Experience&nbsp;
Delay and Customer Service
Extra Charges
Online Booking
Reschedule and Refund
Reservation Issue
Seating Preferences

Tech Stack:
➔ Language: Python3
➔ Services: Confluent Kafka, Spark
➔ Libraries: tweepy, Flask, kafka, spacy, sklearn, keras, numpy, pyspark, nltk, matplotlib, os

Tweepy for fetching tweets and replying.
Flask for setting up API for a reply.
Kafka for data ingestion.
Sklearn for model evaluation and encoding.
Nltk for preprocessing.
Spacy for named entity recognition.
Pandas for data manipulation and analysis.
Matplotlib for plotting graphs.
Numpy for array and math operations.
Tensorflow and Keras for loading and building models and Embeddings.
Pyspark for UDFs and Streaming.
Os for operating folders.

Approach :

Dataset is first processed by removing Stopwords, mentions, URLs, etc., and Lemmatization.
Tokenizing and Embedding words to text sequences.
&nbsp;A sequential model is trained on the processed data, which includes LSTM for Sentiment Classification.
Topic labelling using Latent Dirichlet Allocation (LDA).
Data is trained on a sequential model including LSTM for Topic Classification.
Named Entity Extraction.
Saving all models and embeddings for runtime usage.
Sink enriched data to a parquet file.
Reply to the tweet using Flask and tweepy.

Architecture Diagram:
"
84,Orchestrate Redshift ETL using AWS Glue and Step Functions,"Business Overview
Extract, Transform, and Load, or ETL, is a data integration process that integrates data from various sources into a single, consistent data store put into a data warehouse or other destination system.
ETL was developed to integrate and load data for calculation and analysis as databases become more popular in the 1970s. It eventually became the dominant way of processing data for data warehousing operations.
Data Analytics and Machine Learning work-streams rely on ETL for their basis. ETL cleanses and organizes data using a set of business rules to meet particular business intelligence requirements, such as monthly reporting. Still, it may also handle complex analytics to enhance back-end operations or end-user experiences. An organization's ETL is frequently used to:&nbsp;

Retrieve data from legacy systems.
To improve data accuracy and reliability, clean the data.
Update a target database with data.

It is prevalent to use Redshift as a data-warehousing tool in the AWS cloud. However, there are quite some ways to orchestrate the loading, unloading and querying Redshift. In this project, we use in-house AWS tools to orchestrate end-to-end loading and deriving business insights. Since it uses in-house tools, the availability and durability of the solution are guaranteed by AWS.
Tech Stack
➔ Language: Python3, SQL
➔ Services: Amazon Redshift, AWS Glue, AWS Step Function, VPC, QuickSight
➔ Libraries: boto3, sys
Amazon Redshift
Amazon Redshift is a fully managed petabyte-scale cloud data warehouse service. It also includes Redshift Spectrum that runs SQL queries directly against structured or unstructured data in Amazon S3 without loading them into the Redshift cluster. Redshift lets us run complex, analytic queries against structured data and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage like SSD, and massively parallel query execution. It is an OLAP solution to store petabytes of information without owning Infrastructure (Paas).
AWS Glue
A serverless data integration service makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. Also, you pay storage costs for Data Catalog objects.
AWS Step Functions
This service is a workflow(or State Machine) of steps(or tasks) where the output of one step acts as an input to the next. Each step in an application executes in order, as defined by business logic. Step Functions manages sequencing, error handling, retry logic, state management, and parameter passing with its built-in operational controls, removing a significant operational burden from developers. We can connect via AWS CLI, AWS SDK, APIs, or Console.
VPC
Amazon Virtual Private Cloud (Amazon VPC) is a service that lets us launch AWS resources in a logically isolated virtual network we define. We have complete control over the virtual networking environment, including selecting our IP address range, creating subnets, and configuring route tables and network gateways. There are no additional charges for creating and using the VPC itself. However, usage charges for other Amazon Web Services like Amazon EC2, Elastic IP addresses still apply at published rates for those resources, including data transfer charges. All environments are set up in the AWS cloud. If a user does not create VPC, they are bound to use the default VPC for most products.
QuickSight
Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. It is the first BI service to offer pay-per-session pricing, where you only pay when your users access their dashboards or reports, making it cost-effective for large scale deployments. It can connect to various sources like Redshift, S3, Dynamo, RDS, files like JSON, text, CSV, TSV, Jira, Salesforce, and on-premises oracle SQL-server.
Data Description
One of Amazon's most recognizable products is Amazon Customer Reviews. Millions of Amazon consumers have posted over a hundred million reviews to express their thoughts and explain their experiences with items on the Amazon.com website during the past two decades. This makes Amazon Customer Reviews a valuable resource for academics working in disciplines like Natural Language Processing, Information Retrieval, and Machine Learning. This dataset was created specifically to reflect a sample of customer evaluations and opinions, variance in product perception across geographic locations, and promotional purpose or bias in reviews.
The Parquet dataset, which is one of two accessible formats, including TSV, is partitioned (split into subfolders) on S3 by ""product category"" to increase further query speed. This enables queries that include a WHERE clause on ""product category"" only to read data from that category.
Approach&nbsp;

The state machine starts a series of AWS Glue Python Shell jobs, each with parameters for obtaining database connection information from AWS Secrets Manager and a SQL file from S3.
The database connection information is used by each execution of the AWS Glue Python Shell task to connect to the Amazon Redshift cluster and submit the queries in the SQL file.

Task 1:&nbsp;The cluster utilizes Amazon Redshift Spectrum to read data from S3 and load it into an Amazon Redshift table


Task 2:&nbsp;The cluster executes an aggregation query and exports the results to another Amazon S3 location via UNLOAD.

The state machine may send a notification to an Amazon Simple Notification Service (SNS) topic in the case of pipeline failure.
Users can query the data from the cluster or retrieve report output files directly from S3/Redshift using QuickSight.

Architecture Diagram:
&nbsp;
"
85,Multi-Class Text Classification with Deep Learning using BERT,"Business Objective 
The most abundant data in the world today is in the form of texts. Text is a rich source of information, but extracting insights from it can be complex and time-consuming as they are in an unstructured format. Hence, having a powerful text-processing system is critical and is more than just a necessity. There are various algorithms designed for performing the text classification today, BERT being one of the most popular.
&nbsp;
Bidirectional Encoder Representations from Transformers or BERT is a prevalent NLP model from Google known for producing state-of-the-art results in a wide variety of NLP tasks. The importance of Natural Language Processing (NLP) is profound in the artificial intelligence domain.
&nbsp;
The BERT algorithm is built on top of breakthrough techniques such as seq2seq models and transformers. The seq2seq model is a network that converts a given sequence of words into a different sequence and is capable of relating the words that seem more important. 
&nbsp;
This project will cover in detail the application of the BERT base model concerning text classification. We will witness how this state-of-the-art Transformer model can achieve extremely high-performance metrics for a large corpus of data comprising more than 100k+ labelled training examples.
&nbsp;

Data Description&nbsp;
&nbsp;
For our case study, we will be using the datasets from the hugging face library.
The BERT model will be built on the AG News dataset. 

AG News (AG&rsquo;s News Corpus) is a sub dataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes 
The four classes are: World, Sports, Business, Sci/Tech
The AG News contains 30,000 training and 1,900 test samples per class.&nbsp;

&nbsp;
Aim
&nbsp;
The project aims at building, training and fine-tuning the BERT model with respect to classification on the AG News dataset.
&nbsp;
&nbsp;Tech stack 
&nbsp;

Language - Python
Libraries &ndash; ktrain, transformers, datasets, numpy, pandas, tensorflow, timeit

&nbsp;
Environment
&nbsp;

Jupyter Notebook

&nbsp;
&nbsp;Approach 

Checking the hardware acceleration settings.
Installing the required libraries 
Checking for the available dataset from the hugging face library
Importing the required dataset
Loading the train and test data
Creating dataframe objects for train and test data.
Performing data pre-processing 
Creating the BERT model.
Compile the BERT model.
Training the BERT model on some defined hyperparameters.
Evaluating the performance metrics
Saving the model.
"
86,How to deal with slowly changing dimensions using snowflake?,"What is Slowly Changing Dimensions?
The terms ""facts"" and ""dimensions"" are used in data warehousing. A fact is a piece of numerical data, such as a sale or clicks. Facts are stored in fact tables, linked to various dimension tables via foreign keys that act as companion tables to the facts. Dimension Attributes are the different columns in a dimension table that provide descriptive features of the facts.
A Slowly Changing Dimension (SCD) stores and maintains both current and historical data across time in a data warehouse. It is regarded as one of the essential ETL jobs for monitoring the history of dimension records, and it has been implemented. Customer, geography, and employee are examples of such dimensions.
SCD may be approached in a variety of ways. The most popular ones are:
Type 0: This is a passive method. When the dimensions change in this approach, no particular action is taken. Some dimension data can be kept the same as when initially entered, while others may be replaced.
Type 1:&nbsp;The new data overwrites the previous data in a Type 1 SCD. As a result, the existing data is lost because it is not saved elsewhere. This is the most common sort of dimension one will encounter. To make a Type 1 SCD, one does not need to provide further information.
Type 2:&nbsp;The complete history of values is preserved in a Type 2 SCD. The current record is closed when the value of a particular attribute changes. With the updated data values, a new record is generated, which then becomes the current record. Each record's adequate time and expiry time are used to determine the period during which the record was active.
Type 3:&nbsp;For some chosen dimensions, a Type 3 SCD maintains two copies of values. The previous and current values of the chosen attribute are saved in each record. When the value of any of the chosen attributes changes, the latest value is recorded as the current value, and the previous value is saved as the old value in a new column.
In this project, we use Snowflake Data Warehouse to implement different SCDs. Snowflake offers all sorts of services to build an efficient Data warehouse with ETL capability and support for various external data partners.
Data Pipeline:&nbsp;
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches. The data pipeline is extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into the query-worthy format, and visualizing KPIs, including Orchestrating the above process.
Dataset Description:
In this project, we use the faker library from Python to generate records of users and store the records in CSV format with the name, including the current system time.
The data includes the following parameters:


Customer_id


First_name


Last_name


Email


Street


State


Country


Tech Stack:
➔ Languages: Python3, JavaScript, SQL
➔ Services: NiFi, Amazon S3, Snowflake, Amazon EC2, Docker
NiFi
Apache NiFi is a data logistics platform that automates data transfer across systems. It gives real-time control over data transportation from any source to any destination, making it simple to handle.
Docker
Docker is a containerization platform that is available as an open-source project. It allows developers to bundle programs into containers, standardized executable components that combine application source code with the OS libraries and dependencies needed to run that code in any environment.
Amazon EC2
In the Amazon Web Services Cloud, the Amazon Elastic Compute Cloud (Amazon EC2) offers scalable computing capability. The user will not have to buy hardware upfront if Amazon EC2 is used. Amazon EC2 allows developers to launch multiple virtual servers based on usage, set security and networking, and manage storage.
Amazon S3
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
Snowflake
Snowflake is a data storage, processing, and analytics platform that blends a unique SQL query engine with a cloud-native architecture. Snowflake delivers all the features of an enterprise analytic database to the user. Snowflake components include:

Warehouse/Virtual Warehouse
Database and Schema
Table
View
Stored procedure
Snowpipe
Stream
Task

Approach:

Test data created using faker library and saved in CSVs.
Data is ingested using NiFi and pushed to Amazon S3.
A Snowpipe automation tool loads new data from S3 to the staging table.
Data manipulation language changes are recorded using Snowflake streams in the staging table to decide the operation to be performed.
Based on the changes, Tasks and stored procedures are triggered to implement SCD Type-1 and Type-2.

Architecture:
"
87,"Build a Real-Time Dashboard with Spark, Grafana, and InfluxDB","Business Overview:
A time-series metric includes a collection of data points that appear in a specific order over a particular timeframe. In this context, a ""metric"" refers to the data recorded at each time interval. A time-series metric has two primary characteristics:&nbsp;


It may be assigned a numerical value.


It refers to the fact that the measure evolves with time.


Each numeric data point relates to a timestamp as well as one or more identified dimensions. Time series data is commonly used to analyze website traffic, pricing fluctuations, demographic data, user clicks, and IT operations since it captures changes over time. As one might guess, because this time series data is frequently gathered in short periods, the data builds quickly. As a result, having a database optimized for time series data is critical, which is why they've been so popular in recent years.
In this project, we build a real-time e-commerce users analytics Dashboard. By consuming different events such as user clicks, orders, demographics, create a dashboard that gives a holistic view of insights such as how a campaign is performing country level, gender basis orders count, real-time purchase insights.
&nbsp;
Data Pipeline:
A data pipeline is a mechanism that allows data to be transferred from one system to another. The data may be modified or not, and it could be processed in real-time (or streaming) rather than in batches. The data pipeline includes everything from harvesting or capturing data using various tools to storing raw data, cleaning, verifying, and transforming data into a query-worthy format to visualizing KPIs and orchestrating the above process.
&nbsp;
Dataset Description:
&nbsp;The batch data consists of 100,000 auto-generated user demographic data points, including the following features:


Id


Age


Gender


State


Country


While the stream data is based on user purchase events and is produced every 1 second along with a timestamp when joined with batch data. This data includes the following features:


Id


campaignID


orderID


total_amount


units


tags- click/purchased


&nbsp;
Tech Stack:
➔ Language: Java8, SQL
➔ Services: Kafka, Spark Streaming, MySQL, InfluxDB, Grafana, Docker, Maven
&nbsp;
Apache Kafka:
&nbsp;Apache Kafka is a distributed data storage designed for real-time data intake and processing. Streaming data is created continuously by hundreds of data sources, which generally transmit data records simultaneously. A streaming platform must cope with the continual input of data and process it sequentially and gradually.
Kafka is most used to create real-time streaming data pipelines and applications that react to data streams. It mixes communications, storage, and stream processing to allow historical and real-time data to be stored and analyzed.
&nbsp;
Spark Streaming:
Spark Streaming is a Spark API service that enables data engineers and scientists to handle real-time data from various sources, including Kafka, Flume, Amazon Kinesis, etc. Data may be delivered to file systems, databases, and live dashboards once it has been analyzed.
&nbsp;
MySQL:
MySQL is a SQL (Structured Query Language) based relational database management system. Data warehousing, e-commerce, and logging applications are just a few of the uses of the platform.
&nbsp;
InfluxDB:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
InfluxData developed InfluxDB, an open-source time-series database. It's developed in Go and designed to store and retrieve time series data quickly and reliably in domains including operations monitoring, application metrics, Internet of Things sensor data, and real-time analytics.
&nbsp;
Grafana:
Grafana is a web application for interactive visualization and analytics that is open source and cross-platform. When linked to supported data sources, it displays charts, graphs, and alerts on the web for mainly time series data.
&nbsp;
Approach:


User purchase events in Avro format are produced via Kafka.


Spark Streaming Framework does join operations on batch and real-time events of user purchase and demographic type.


MySql Holds the demographic data such as age, gender, country, etc.


Spark Streaming Framework consumes these events and generates a variety of points suitable for time series and dashboarding.


Kafka connect pushes the events from the Kafka streams to influxDB.


Grafana connects to different sources like influxDB, MySQL and populates the graphs.

"
88,Build a Multi Touch Attribution Machine Learning Model in Python,"Introduction to Multi-Touch Attribution Model In Python Project
Marketing is a technique of getting potential clients or customers interested in one&rsquo;s products and services. Businesses can reach out to their customers via various marketing channels. It looks simple by definition but is indeed a much more challenging task. Advertisers use various online marketing channels to reach consumers, and they typically want to know how much each channel contributes to their marketing success. This is what is known as multi-channel attribution.
&nbsp;
In many cases, advertisers approach this problem using simple marketing attribution models that help them understand the importance of each marketing channel. A few attribution models are as follows:


Single Touch attribution models like First Touch Conversion, Last Touch Conversion


Multi-Touch attribution models like Linear Touch Conversion


Probabilistic models like Markov chains, etc


&nbsp;
We will try building all these machine learning models in our project to understand how these models will help us find which channels will lead us to more conversions.
&nbsp;

&nbsp;
Project Overview: How to build a multi-touch attribution model in Python?
Here is a brief overview of the marketing attribution model in the python project.
Aim of the Attribution Modeling using Machine Learning Project
&nbsp;
In this project, we aim to improve the advertising ROI by quantifying the actual value of the multi-faceted advertising campaigns. This will enable business stakeholders to make decisions based on the millions of converting click paths by isolating the impact of every touchpoint. The project aims at building multiple attribution models on the given dataset to discover channels leading to greater customer conversions.
Tech Stack for Implementing Multi-Touch Attribution
Language - Python
Libraries &ndash; NumPy, matplotlib, seaborn, itertools, gekko, pandas-profiling
Dataset for Attribution Modeling in Python
The data is in the form of a CSV file with a data size of 586737 rows and six columns.
The columns are as follows:
Cookie - Anonymous customer-id
Time - Date and time when the visit took place
Interaction - Categorical variable indicating the type of interaction that took place
Conversion - indicating whether a conversion took place, 0: not converted, 1: converted
Conversion value - Value of the potential conversion event
Channel (target variable) - The marketing channel that brought the customer to our site&nbsp;
Machine Learning Attribution Modelling Python Code Overview
&nbsp;Importing the required dependencies and libraries.


Import the dataset.


Exploratory Data Analysis (EDA) &ndash;




Create EDA report using pandas profiling python module




Building Single Touch Attribution Models




Last Touch Attribution model


First Touch Attribution model


Last nondirect Touch Attribution model




Building Multi-Touch Attribution Models




Linear Attribution model


Position based (U-shaped) Attribution model


Position Decay Attribution model




Probabilistic Attribution model




Markov Attribution model


Shapley Value model




Results




Tables &ndash; an average of all the models


Graphs &ndash; plot the models




Build a Budget Optimization Engine


Concepts Covered in this Attribution Modeling Python Project
This multi-touch attribution model data science project covers the following marketing and data science concepts -
Exploratory Data Analysis
This project covers the implementation of Python data analysis libraries like NumPy, Pandas, etc. You will learn how to perform analysis at the granular level and understand how customer conversions are performed through different channels. The project will teach you how to handle categorical and numerical data simultaneously. You will use statistical tools like correlations to deeply understand the relationships among other variables. Additionally, you will learn how to prepare an HTML report to display the stats of each variable in the dataset. You can use this report as a template for other datasets. Lastly, this project will also assist you in dealing with duplicate values in a dataset.
Single Touch Attribution Models
There are three single-touch attribution models that you will study in this project.&nbsp;


Last-touch attribution model: It gives the utmost importance to the channel the customer interacted with the last with, just before conversion.


The first touch attribution model considers the customer's first interaction with the channel, responsible for conversion.


Last nondirect Touch Attribution model: This model considers the second-last channel the customer interacts with, before conversion, as the most crucial channel.


The project will guide how to implement these models in the Python programming language.

Multi-Touch Attribution Model
With digital marketing campaigns in the picture, relying on single-touch models for attribution modeling becomes difficult. That is why analysts resort to multi-touch attribution analysis. Unlike single-touch models, the multi-touch model assumes all the channels that a customer interacts with, play a critical role in conversions. This project will cover the following three multi-touch attribution models:


Linear Attribution Model: This model gives equal importance to all the customer-channel interactions.


Position based (U-shaped) Attribution model: This model assumes that the first and the last customer-channel interactions play vital roles in conversions


Position Decay Attribution model: This model gives the highest weightage to the latest customer-channel interaction, and the importance decays as it traverses previous interactions.


This project will guide you through implementing all three models by defining their functions in Python.
Probabilistic Attribution Model
The probability attribution model relies on statistical and machine learning techniques for analyzing the contribution of all the customer-channel interactions. This project will teach you the following two models:


The Markov Attribution model uses the Markov-chain algorithm to assign weights to all the channels that lead to conversions.


Shapley Value model: This model uses Shapley value (cooperative game theory) to quantify the influence of each channel.


Budget Optimization
Finally, the project will teach you to determine which model will work best for your specific problem. After that, you will learn how to optimize the marketing budget, that is, understand which channels should be allocated with what percentage of the budget to drive maximum conversions.
FAQs on Multi-touch Attribution Models
What is the best multi-touch attribution model?
The best multi-touch attribution model will be the one that works the best for your dataset. A multi-touch attribution model is a model which changes from business to business, lineage to lineage. It solely depends on which level of touch you are attributing a conversion. Some of the famous multitouch attribution models are:&nbsp;


Linear Multi-Touch Marketing Attribution Model


U-Shaped Multi-Touch Marketing Attribution Model


Time Decay Multi-Touch Marketing Attribution Model


W-Shaped Multi-Touch Marketing Attribution Model


How to Build a Multi-Touch Attribution model?


Choose the multi-touch attribution model that you believe will suit your dataset by analyzing which channels were prominent in serving as the point of conversion.


&nbsp;If you conclude a probabilistic model works the best, then follow the steps for a Shapely model or any other Position Decay model.


&nbsp;If you deduce that the first and last steps of the conversion process are the most important, then will follow the steps for a Position Based attribution model.


Use a programming language like Python to define a function that takes your data as the input. The function must filter the conversions from the data, store the corresponding cookie IDs, aggregate the click counts and then distribute them according to the chosen model.


Once the model has been designed, you can use it to analyze the influence of each channel.

"
89,Build CNN for Image Colorization using Deep Transfer Learning,"Business Objective 
Image colorization is the process of taking an input grayscale (black and white) image and then producing an output colorized image that represents the semantic colors and tones of the input. In image colorization, a color is assigned to each pixel of a target grayscale image.
Image colorization technique is helpful for many applications such as Medical Microscope, Medical Imagery, Denoising and recreating old Images, Night Vision Camera, etc.
In our use case, we will be using a fully automated data-driven technique called autoencoders for image colorization. Autoencoders are a specific type of feedforward neural network where the input is the same as the output. The VGG16 model will be used as a feature extractor. VGG16 is a classic neural network used as a backbone for many computer vision tasks
This project aims to build a convolutional neural network that will best convert the grayscale images to RGB images.
Data Description&nbsp;
Here, we will be using a landscape image dataset which consists of approximately,

Training - 7000 images (RGB images)
Testing - 5 images (Gray scale images)

Aim
To build a model using keras, that can best convert a grayscale image to a coloured image.
Tech stack 

Language - Python
Libraries - numpy, pandas, tensorflow, keras

Approach 

Mount the drive for loading the input data images.
Import the necessary libraries.
Import and initialize the VGG16 model
Initialise the ImageDataGenerator to rescale the images.
Convert the RGB to LAB format
Create a sequential model and check its summary
Compile the model with the appropriate optimizer, loss, and performance metric
Fit the model and train it on 2000 epochs with a defined batch size
Save the model
Using the saved model, make predictions on the test images
&nbsp;Check the predicted test images
"
90,FEAST Feature Store Example for Scaling Machine Learning,"Business Overview
Feast (Feature Store) is an operational data system for managing and serving machine learning features to models in production. Feast is able to serve feature data to models from a low-latency online store (for real-time prediction) or from an offline store (for scale-out batch scoring or model training). Some of the problems solved by feast are


Consistent access to data in modeling


Deploying new features into production


Point-in-time correct data for models


Reusability of the features across the project


One of the main problems is the consistency of the data between training and production. Feast helps to achieve this in a very simple way. In this project, we used customer churn prediction problem to get hands-on training in Feast.&nbsp;&nbsp;
Reference: docs.feast.dev
Aim
To predict Customer churn using a Feature store Feast&nbsp;
Data Description
The dataset used is customer churn data with 8 features. The dataset contains information about 891 customers.&nbsp;
Features:


Created_at


Customer_id


Churned


Category


Sex


Age


Order gmv


Credit type


Tech Stack

Language: Python

Libraries: feast, pandas, sklearn, flask, pickle&nbsp;


Approach


Feast installation and setup


Offline store and Retrieval


Online store and Retrieval


Training data creation using feast


Model training


Random Forest


Gradient Boosting




Real time predictions using Feast


Interactive model deployment using Feast

"
91,OpenCV Project for Beginners to Learn Computer Vision Basics,"Business Objective 
OpenCV (Open-Source Computer Vision Library) is an open-source library that includes several hundreds of computer vision algorithms. OpenCV has a modular structure, which means that the package includes several shared or static libraries.
OpenCV is a huge open-source library for computer vision, machine learning, and image processing. It can process images and videos to identify objects, faces, or even the handwriting of a human. When integrated with various libraries, such as &ldquo;NumPy,&rdquo; a highly optimized library for numerical operations, the number of weapons increases in your Arsenal, i.e., whatever operations one can do in NumPy can be combined with OpenCV.
&nbsp;
Data Description
In this project, we will use three sample images (jpg) and a video (mp4) as our input data and perform various operations on top of it.&nbsp;
&nbsp;
Aim
The project aims to understand OpenCV and build some applications of how to use the OpenCV library.
&nbsp;
Tech stack 

Language - Python
Libraries - numpy, matplotlib, cv2(OpenCV)

&nbsp;
Approach 

Importing the required libraries.
Perform reading, writing, and displaying an image.
Perform arithmetic operations like addition and subtraction on images
Create a function for color spacing and conversion
Create a function for Image thresholding
Create a function for Image smoothing
Create a function for Morphological Transformation
Perform edge detection of an image with Canny Edge Detection
Perform template matching and multi-scale template matching
Create a function for Hough Transformation
Perform video processing
Perform Harris Corner detection
Perform feature Detection and Extraction using SIFT
Create a function for feature matching with Flann and Brute force
Perform face and eye detection
"
92,Deep Learning Project for Text Detection in Images using Python,"Business Objective 
Text detection is the process of detecting the text present in the image. Several applications include solving the captcha, identifying vehicles by reading their license plates, etc.
Convolutional neural networks are deep learning algorithms that are very powerful for the analysis of images. On the other hand, Recurrent Neural Networks (RNNs) are used for sequential data such as text. RNNs are ideal for solving problems where the sequence is more important than the individual items themselves.
In our case, to perform image processing and sequence prediction, we are going to use both CNN and RNN networks. We will apply convolutions on the images, and on these convolutions, we will run the recurrent neural network model to predict the text in the picture. This model is called Convolutional Recurrent Neural Network (CRNN).
This model is best used for images with a single line of text in them, so that we will build this model on images with single-line texts.
This project aims to build a convolutional recurrent neural network that can detect the text from a given image.
Data Description&nbsp;
We will be using the TRSynth100K dataset. This dataset contains around, 100k images along with their labels in a text file.
Aim
To build a CRNN network that can predict the single-line text in a given image.
Tech stack 

Language - Python
Libraries &ndash; cv2, torch, numpy, pandas, albumentations, matplotlib, pickle

Approach 

Importing the required libraries
Download the required dataset
Pre-processing




Find null values and replace those missing values with string &ldquo;null&rdquo;
Create a data frame with image path and corresponding labels (save as csv file)
Create a mapping from characters to integer and save in pickle format(char2int.pkl)
Create a mapping from integer to character and save in pickle format (int2char.pkl)




Define configurations paths
Training the model




Split the data into train and validation
Create train and validation datasets
Define the loss function
Create the model object
Define the optimizer
Training loop
Save the trained model




Predictions




Select image for prediction
Apply augmentations
Take the model output
Apply softmax and take label predictions
Use the &lsquo;ph&rsquo; string character and convert integer predictions to string
Display the output


"
93,Classification Projects on Machine Learning for Beginners - 1,"Overview
The Classification algorithm is a type of supervised machine learning technique used to categorize a set of data into classes. For a given example of input data, a classification algorithm assigns the most probable class label. An easy-to-understand example is classifying email as spam or non-spam. There are several use cases of classification in real-world scenarios. This project aims to give you the basic idea related to different algorithms used for classification.&nbsp;

Aim
To predict license status for the given business.

Data Description
The dataset used is a licensed dataset. It contains information about 86K different businesses over various features. The target variable is the status of license which has five different categories.

Tech Stack


Language: Python


Libraries:&nbsp; pandas, scikit_learn, category_encoders, numpy, os, seaborn, matplotlib


&nbsp;
&nbsp;
Approach


Data Description


Exploratory Data Analysis


Data Cleaning


Missing Value imputation


Outlier Detection




Data Imbalance


Data Encoding


Model Building


KNN classifier


Naive Bayes algorithm


Logistic Regression


Decision Tree classifier




Classification Metrics


Precision


Recall


F1 score


Accuracy


Macro average


Weighted average




Feature importance

"
94,Build Classification Algorithms for Digital Transformation[Banking],"Business Objective 
Bank XYZ has a growing customer base where the majority of them are liability customers (depositors) vs. borrowers (asset customers). The bank is interested in expanding the borrower&rsquo;s base rapidly to bring in more business via loan interests.
A campaign that the bank ran in the last quarter showed an average single-digit conversion rate. In the last town hall, the marketing head mentioned that digital transformation is the core strength of the business strategy, how to devise effective campaigns with better target marketing to increase the conversion ratio to double-digit with the same budget as per the last campaign.
As a data scientist, you are asked to develop a machine learning model to identify potential borrowers to support focused marketing.
&nbsp;
Data Description&nbsp;
The dataset has 2 CSV files,

Data1 - 5000 rows and 8 columns
Data 2 - 5000 rows and 7 columns

&nbsp;
Aim
Build a machine learning model to perform focused digital marketing by predicting the potential customers who will convert from liability customers to asset customers.
&nbsp;
Tech stack 

Language - Python
Libraries &ndash; numpy, pandas, matplotlib, seaborn, sklearn, pickle, imblearn

&nbsp;
Approach 

Importing the required libraries and reading the dataset.




Merging of the two datasets
Understanding the dataset




Exploratory Data Analysis (EDA) &ndash;




Data Visualization




Feature Engineering




Dropping of unwanted columns
Removal of null values
Checking for multi-collinearity and removal of highly correlated features




Model Building




Performing train test split
Logistic Regression Model
Weighted Logistic Regression Model
Naive Bayes Model
Support Vector Machine Model
Decision Tree Classifier
Random Forest Classifier




Model Validation




Accuracy score
Confusion matrix
Area Under Curve (AUC)
Recall score
Precision score
F1-score




Handling the unbalanced data using imblearn.
Hyperparameter Tuning (GridSearchCV)




For Support Vector Machine Model




Creating the final model and making predictions
Save the model with the highest accuracy in the form of a pickle file.
"
95,Snowflake Real Time Data Warehouse Project for Beginners-1,"Business Overview
Snowflake's Data Cloud is based on a cutting-edge data platform delivered as a service (SaaS). Snowflake provides data storage, processing, and analytic solutions that are quicker, easier to use, and more versatile than traditional options.
Snowflake isn't based on any current database technology or large data software platforms like Hadoop. Snowflake, on the other hand, combines a brand-new SQL query engine with cutting-edge cloud architecture. Snowflake gives users all the features and capabilities of an enterprise analytic database, plus a lot more.


There is no hardware to choose, install, configure, or manage (virtual or actual).


There isn't much to install, set up, or maintain in terms of software.


Snowflake oversees ongoing maintenance, administration, updates, and tweaking.


Snowflake is entirely based on cloud infrastructure. Except for optional command-line clients, drivers, and connectors, all components of Snowflake's service operate on public cloud infrastructures.
Snowflake's computing demands are met by virtual compute instances, and data is stored permanently via a storage service. Snowflake isn't compatible with private cloud environments (on-premises or hosted).
The architecture of Snowflake is a mix of classic shared-disk and shared-nothing database designs. Snowflake employs a central data repository for persistent data, like shared-disk architectures, available from all compute nodes in the platform. Snowflake executes queries utilizing MPP (massively parallel processing) compute clusters, wherein each node in the cluster stores a part of the whole data set locally, like shared-nothing architectures. This method combines the simplicity of a shared-disk design with the speed and scale-out advantages of a shared-nothing architecture.
&nbsp;Snowflake components include:


Warehouse/Virtual Warehouse


Database and Schema


Table


View


Stored procedure


Snowpipe


Stream


Task


Time Travel


Objective of the Snowflake Real-time Analytics Project
In this Snowflake real-time project, you will learn about the Snowflake data warehouse architecture and how it differs from the traditional data warehouses. You will learn how to implement the Snowflake data warehouse architecture to build a data warehouse for business applications. Also, this Snowflake project will introduce you to various modern technologies like SnowSQL, Quicksight, etc. You will be loading bulk data from the web interface, from SnowSQL, and a cloud provider (Amazon S3 in this case).

Tech Stack:
➔ Languages: SQL
➔ Services: Amazon S3, Snowflake, SnowSQL, QuickSight, SnowPipe
SnowSQL
SnowSQL is a next-generation command-line client for connecting to Snowflake and running SQL queries, and performing all DDL and DML actions, such as loading and unloading data from database tables. You will work with SnowSQL to load the data for this Snowflake real-time data warehousing project. It is a &lsquo;customer_detail&rsquo; database that consists of various customer details like first name, last name, address, etc. Initially, it loads the data in the stage (PIPE_CLI_STAGE), from where Snowflake loads it into the table for further processing.
SnowPipe
SnowPipe is a managed service of Snowflake that mainly helps in loading streaming data, i.e., it allows Snowflake to load data as soon as data is available in the stage. SnowPipe helps build a Snowflake data warehouse for businesses involving transaction data or any such data that needs immediate processing. This data warehousing Snowflake project involves creating a SnowPipe for loading the sample dataset in a streaming manner into the table.
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location. This Snowflake project entails loading data (Tesla&rsquo;s actual stock data) in Amazon S3, which is considered an external stage here.&nbsp;
QuickSight
Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. It can connect to various sources like Redshift, S3, Dynamo, RDS, files like JSON, text, CSV, TSV, Jira, Salesforce, and on-premises oracle SQL-server. It is the first BI service to offer pay-per-session pricing, where you only pay when your users access their dashboards or reports, making it cost-effective for large-scale deployments. This Snowflake dwh project shows you how to leverage AWS Quicksight for visualizing various patterns in the dataset. It uses Quicksight to create a dashboard using the Tesla stock data in Snowflake. Once you connect Quicksight with Snowflake, you can create the dashboard by loading and analyzing the dataset. For the Tesla stock price data, the most appropriate graph type is a Line chart as it shows the rise and fall of the stock prices aptly. Depending on the business requirements, you can also create different dashboards such as Vertical bar charts, Area line charts, etc. Once you are satisfied with the dashboard, you can publish it
Architecture

Although Snowflake doesn&rsquo;t need many optimization techniques, there are a few points to keep in mind-


There must be a separate dedicated virtual warehouse for handling different workloads.


The size of these virtual warehouses must be increased depending on the pattern of the queries.


Try to maximize cache usage in case of re-execution of queries, and cut down on computation costs.


FAQs


Why is Snowflake so valuable?


Snowflake provides collaborative data exchange, which distinguishes it from other frameworks. It has infinite, seamless scalability across Amazon Web Services (AWS) and Microsoft Azure and other clouds as well.


How does Snowflake optimize queries?


Snowflake uses various methods to optimize query performance, such as maximizing cache usage, using the Search Optimization service, clustering tables, etc.
"
96,MLOps Project for a Mask R-CNN on GCP using uWSGI Flask,"Business Overview
87% of Data Science Projects never make it to production - VentureBeat
Machine learning operations are widely known as MLOps include various technologies, processes, and practices that automate deployment, monitoring, and management of machine learning models in production. Many organizations are turning towards machine learning and Artificial intelligence. MLOps advocates automation and monitoring at all steps of the ML system. In this project, we aim to provide hands-on experience in MLOps by using cloud computing. Google cloud platform is used as a cloud provider. We would advise you to have a basic understanding of Image Segmentation using Mask R-CNN with Tensorflow before jumping into this project.&nbsp;
Aim
To provide an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable machine learning models by using Google Cloud Platform(GCP)
Tech Stack
➔ Language: Python
➔ Services: GCP, uWSGI, Flask, Kubernetes, Docker&nbsp;
➔ Libraries: TensorFlow, mrcnn, matplotlib, os, flask"
97,OpenCV Project to Master Advanced Computer Vision Concepts,"Business Objective 
OpenCV (Open-Source Computer Vision Library)&nbsp;is an open-source library that includes several hundreds of computer vision algorithms. OpenCV has a modular structure, which means that the package includes several shared or static libraries.
OpenCV is a huge open-source library for computer vision, machine learning, and image processing. It can process images and videos to identify objects, faces, or even the handwriting of a human. When integrated with various libraries, such as &ldquo;NumPy,&rdquo; a highly optimized library for numerical operations, the number of weapons increases in your Arsenal, i.e., whatever operations one can do in NumPy can be combined with OpenCV.
It is advised to visit the basic OpenCV project OpenCV Project for Beginners to Learn Computer Vision Basics before starting.
&nbsp;
Data Description
In this project, we will be using sample images and videos as our input data and perform various algorithms on top of it.&nbsp;
&nbsp;
Aim
The project aims at performing some complex techniques and algorithms using the OpenCV library.
&nbsp;
Tech stack 

Language - Python
Libraries - numpy, matplotlib, cv2(OpenCV)

&nbsp;
Approach 

Importing the required libraries.
Implement Background subtraction.
Perform the Meanshift algorithm.
Perform the Camshift algorithm.
Implement the Lucas Kanade Optical Flow algorithm
Implement the Franeback Dense Flow algorithm.
Perform High Dynamic Range (HDR) imaging
Implement Epipolar Geometry using SIFT and Stereo images.
Implement Depth Map on Stereo images.
Perform Colour Quantization using Clustering.
Perform Image De-noising
"
98,Build a Scalable Event Based GCP Data Pipeline using DataFlow,"Business Overview
There is a continual need to give huge volumes of data to teams in a data-driven company. Many tools are available to assist you with your requirements and wants. Choosing the appropriate mechanism may be complicated and overwhelming at times. The key idea to remember is that there is no one-size-fits-all tool or architecture, and it all depends on your needs.
For data ingestion, many businesses are opting for Event-Driven Architecture (EDA). EDA is a software design paradigm used in application development. It enables enterprises to track and recognize significant business moments and then act on them immediately. EDA differs from a conventional request-driven system in that services must wait for a response before moving on to the next job. Many application architectures are increasingly event-driven in a modern economy fuelled by high digital transaction volume.
These pipelines can provide the agility, scalability, context, and responsiveness required for performant digital business applications. The event-driven design also needs low coupling, making it a suitable match for distributed application architectures.
There are producers and consumers in event-driven architecture. Producers unearth events and craft a message. The event is subsequently transmitted to event consumers through an event channel, where it is processed. An event processing system then performs a response to the event message, which results in creating an activity downstream.
This project will create an event-driven data pipeline using the Google Cloud Platform's serverless features. It will be based on some of the development methods that are frequently used in businesses.
&nbsp;
Dataset Description
The dataset used in this project will be a Covid-19 dataset(COVID-19 Cases.csv) from data.world, which consists of a few of the following attributes:


people_positive_cases_count


county_name


case_type


data_source


&nbsp;
Tech Stack
➔ Language: Python3.7
➔ Services: Cloud Composer, Google Cloud Storage (GCS), Pub-Sub, Cloud Functions, BigQuery, BigTable
&nbsp;
Cloud Composer
Cloud Composer is a workflow orchestration service based on Apache Airflow that is completely managed. Cloud Composer pipelines are easily constructed as directed acyclic graphs (DAGs) using Python. One-click deployment provides quick access to an extensive library of connectors and numerous graphical representations of your process in operation, making debugging a breeze. Your jobs will stay on track if your directed acyclic graphs are automatically synchronized.
&nbsp;
Google Cloud Storage
Google Cloud Storage is a web-based file storage service that allows you to store and access files on the Google Cloud Platform infrastructure. The service combines Google's cloud's speed and scalability with sophisticated security and sharing features. It provides Infrastructure as a Solution (IaaS), similar to Amazon's S3 online storage service.
&nbsp;
Pub/Sub
To ingest and disseminate data, Pub/Sub is utilized in streaming analytics and data integration pipelines. It works equally well as messaging-oriented middleware for service integration or as a queue to parallelize operations.
Pub/Sub allows you to construct event producers and consumers systems, known as publishers and subscribers. Publishers connect with subscribers asynchronously by disseminating events rather than synchronous remote procedure calls (RPCs). It is comparable to Apache Kafka.
&nbsp;
Cloud Functions
Google Cloud Functions is a serverless execution environment used to develop and connect cloud applications. Cloud Functions allows you to build simple, one-time functions related to events generated by your cloud infrastructure and services. When an event being monitored fires, your function is called. Your code runs in a completely controlled environment. There is no need to set up Infrastructure or manage servers.
&nbsp;
BigQuery
BigQuery is a fully managed corporate data warehouse that lets you collect and analyze your data with built-in capabilities such as machine learning, geographic analysis, and business analytics. BigQuery's serverless design enables you to do research using SQL queries while managing no infrastructure. The scalable, distributed analytical engine in BigQuery allows you to query terabytes in seconds and petabytes in minutes.
&nbsp;
BigTable
Cloud Bigtable is a sparsely filled table with the ability to expand to billions of rows and thousands of columns, allowing you to store terabytes or even petabytes of data. Each row has a single value that is indexed, and this value is known as the row key. Bigtable is suitable for storing massive volumes of single-keyed data with extremely low latency. It has a high read and write throughput with minimal latency, making it an excellent data source for MapReduce processes.
&nbsp;
Architecture&nbsp;
&nbsp;

&nbsp;"
99,Recommender System Machine Learning Project for Beginners-1,"Business Overview
A recommendation system is a type of algorithm designed to recommend or suggest things to the user based on many different factors. The recommendation system deals with a large amount of data and filters it out based on user&rsquo;s preferences and interests. With the rise of Youtube, Netflix, Amazon, etc., recommendation systems have taken a crucial place. Recommender systems are critical in many industries as they can help to generate a large amount of revenue. This series of projects aims to introduce recommendation systems and several algorithms used for recommendation.
This project mainly focuses on the basics of the recommendation system and a brief introduction to the different algorithms. Implementation of a Rule-based recommendation system has also been covered.&nbsp;
Aim
To understand the basics of a recommendation system and build a rule-based recommender system.
Data Description
The dataset is a transnational data set containing all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts with maximum wholesaler customers. The dataset can be found at The UCL machine learning repository. The dataset contains information about 541910 customers over eight attributes. The eight attributes are InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country.
Tech Stack


Language: Python


Libraries:&nbsp; pandas, numpy, seaborn, matplotlib


Approach


Data Description


Exploratory Data Analysis


Data Cleaning


Rule-based Recommendation system


Popular items Globally


Popular items countriwise


Popular item monthwise


Buy again



"
100,Time Series Project to Build an Autoregressive Model in Python,"Business Objective 
A time series is simply a series of data points ordered in time. In a time-series, time is often the independent variable, and the goal is usually to make a forecast for the future.
Time series data can be helpful in for many applications in day-to-day activities like:

Tracking daily, hourly, or weekly weather data
Monitoring changes in application performance
Medical devices to visualize vitals in real-time

Autoregression (AR) modelling&nbsp;is one of the techniques used for&nbsp;time-series&nbsp;analysis. Autoregression is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step.
In this project, we will build an autoregression model and also check different parameters such as presence of white noise, stationarity of the data, seasonality, etc.
(The autoregression modelling is the first time series project from the list of projects)
Data Description&nbsp;
The dataset attached in the following CSV showcases the readings of 3 sensors of a chiller. The file contains data from one chiller, and the sensors give out one value at every hour of the day.
The data has 1895 rows and 5 columns. Following are the variables:

Time: At what time the reading was taken (timestamp)
IOT_Sensor_Reading: The reading of the sensor at the above-mentioned timestamp
Error_Present: The error which may or may not be present while taking the reading
Sensor 2: The reading from the subordinate sensor
Sensor_Value: The final value to be predicted

Aim
The aim of this project is to build an auto regression model on the given dataset.
Tech stack 

Language - Python
Libraries - pandas, numpy, matplotlib, scipy.stats, pylab, statsmodels, seaborn

Approach 

Import the required libraries and read the dataset.
Perform descriptive analysis
Exploratory Data Analysis (EDA) -




Data Visualization (Q-Q plot)




Pre-processing




Convert date from string format to date format
Set time as the index column
Setting desired frequency
Handle missing data using forward filling, backward filling, and mean filling.




Check for white noise
Create a random walk model
Perform Stationarity tests




Augmented Dickey-Fuller test
KPSS test




Seasonal decomposition plot
Plot an Autocorrelation plot (ACF)
Plot a Partial Autocorrelation plot (PACF)
Perform Autoregression Modelling (ARMA)
Log Likelihood test
Calculating Rolling window
Calculating Expanding window
"
101,Build a Moving Average Time Series Forecasting Model in Python,"Business Objective 
A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable, and the goal is usually to make a forecast for the future.
Time series data can be helpful for many applications in day-to-day activities like:&nbsp;

Tracking daily, hourly, or weekly weather data
Monitoring changes in application performance
Medical devices to visualize vitals in real-time

Moving averages are a simple and common type of smoothing used in time series analysis and forecasting. Calculating a moving average involves creating a new series where the values comprise the average of raw observations in the original time series.
A moving average requires that you specify a window size called the window width. This defines the number of raw observations used to calculate the moving average value. The &lsquo;moving&rsquo; part in the moving average refers to the window defined by the window width is slid along the time series to calculate the average values in the new series. Two main types of moving average are used, namely, Centred and Trailing Moving Average.
In the last part of the time series, we have covered Autoregression modelling. In this project we will explore what is Moving Average Smoothing technique.
(Moving average is the second project in our list of time series projects, you can refer to the previous project through this link :&nbsp;Time Series Project to Build an Autoregressive Model in Python)
&nbsp;
Data Description&nbsp;
The dataset attached in the following csv showcases the readings of 3 sensors of a chiller installed in North America. For data anonymity, the values have been transformed using a certain algorithm. The file contains data from one chiller which is located in Brazil and the sensors give out 1 value at every hour of the day. The raw data is at second level but as that is not actionable this data has been grouped at an hour level.
The data has 1895 rows and 5 columns. Following are the variables:

Time: At what time the reading was taken (timestamp)
IOT_Sensor_Reading: The reading of the sensor at the above-mentioned timestamp
Error_Present: The error which may or may not be present while taking the reading
Sensor 2: The reading from the subordinate sensor
Sensor_Value: The final value to be predicted

&nbsp;
Aim
This project aims to build a moving average smoothing on the given dataset.
&nbsp;
Tech stack 

Language - Python
Libraries - pandas, numpy, matplotlib, seaborn, statsmodels, sklearn

&nbsp;
Approach 

Import the required libraries and read the dataset
Perform descriptive analysis
Exploratory Data Analysis (EDA) -




Data Visualization (Q-Q plot)




Feature Engineering
Perform resampling on data




Upsampling
Downsampling




Handling the missing data (Interpolate)




Linear Interpolation
Polynomial / Spline Interpolation




Perform data transformation




Square Root Transformation
Log Transformation
Box-Cox Transformation




Rolling window Statistics
Expanding window Statistics
Moving Average Smoothing
Use the Performance Metric RMSE
"
102,Recommender System Machine Learning Project for Beginners-2,"Business Overview&nbsp;
While surfing any of the eCommerce platforms, you must have landed upon the frequently bought together section. Many of the business use this to increase the sales which eventually increases revenue. Did you ever wonder how do they do it? The technique to understand customer purchasing patterns based on the historical data is known as Market Basket Analysis, also known as Association analysis. In other words, Market Basket Analysis allows retailers to identify relationships between the items that people buy. As a part of series of Recommender system projects, this project covers a wide variety of Market basket analysis with the implementation of various rules in Python. An extensive Exploratory Data Analysis (EDA) has also been discussed.
If you haven&rsquo;t already visited, here is the first project of the series https://www.projectpro.io/project-use-case/recommendation-system-project-for-beginners
&nbsp;
Aim
To carry out Exploratory Data Analysis and implement Market Basket Analysis.

Data Description&nbsp;
The dataset is a transnational data set containing all the transactions occurring between the years 2010 to 2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts with maximum wholesaler customers. The dataset contains information about 500K customers over eight attributes.
&nbsp;
Tech Stack


Language: Python


Libraries:&nbsp; pandas, numpy, seaborn, matplotlib, collections, mlxtend, wordcloud, networkx



Approach


Data Description


Exploratory Data Analysis


Customer insights


Date time analysis


Free items


Item level analysis


Wordcloud




Data Cleaning


Missing values detection


Outlier detection and treatment




Market Basket Analysis


Top 10 first choices


Trending items


Deals of the day




Apriori algorithm


Implementation by using mlxtend


Visualization using networkx



"
103,Text Classification with Transformers-RoBERTa and XLNet Model,"&nbsp;
Business Objective
In Part 1 of the Transformer series lectures - Multi-Class Text Classification with Deep Learning using BERT), we have done an in-depth analysis of NLP.
We started from simpler NLP models like&nbsp;Bag of Words (BOW), TF-IDF&nbsp;and moved towards word embedding models&nbsp;like Word2Vec, Glove,&nbsp;and then to&nbsp;simple/bi-directional RNNs. Finally, we came across complex&nbsp;Transformer architecture and witnessed the power of&nbsp;BERT, one of the highest performing State-of-the-Art Transformer models.&nbsp;
In Part 2 of this series, we will discuss&nbsp;two more novel architectures&nbsp;which worked on improving over BERT performances using different training &amp; optimization techniques.
These are:

RoBERTa: A Robustly Optimized BERT Pretraining Approach
XLNet: Generalized Autoregressive Pretraining for Language Understanding

We will analyse the architectures of these two models, study their training and optimization techniques and finally use them to&nbsp;classify Human Emotions&nbsp;into separate categories.
&nbsp;
Data Description&nbsp;
Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. This dataset is taken from the hugging face library.
The dataset comprises of three data categories,

Train - 16000 rows and 2 columns
Validation - 2000 rows and 2 columns
Test - 2000 rows and 2 columns

&nbsp;
Aim
The project aims at building two models, namely RoBERTa and XLNet to perform classification on the human emotion dataset.
&nbsp;
Tech stack 

Language - Python
Libraries - datasets, numpy, pandas, matplotlib, seaborn, ktrain, transformers, tensorflow, sklearn

&nbsp;
Environment

Jupyter Notebook
Google Colab Pro (Recommended)

&nbsp;
Approach 

Install the required libraries
Load the &lsquo;emotion&rsquo; dataset
Read the dataset across all the three categories
Convert dataset object to data-frame and create a new feature
Data Visualization




Histogram plots




RoBERTa model




Create a RoBERTa model instance.
Split the train and validation data
Perform Data Pre-processing
Compile RoBERTa in a K-train learner object
Find optimal learning rate
Fine-tune the RoBERTa model on the dataset
Check for performance metrics
Save the RoBERTa model
Use the RoBERTa model on the test data and check for the performance.




Understand the Autoregressive and Autoencoder models
XLNet model




Load the required libraries
Create an XLNet model instance
Split the train and validation data
Perform Data Pre-processing
Compile XLNet in a K-train learner object
Find optimal learning rate
Fine-tune the XLNet model on the dataset
Check for performance metrics
Save the XLNet model
Use the XLNet model on the test data and check for the performance



&nbsp;"
104,Time Series Forecasting Project-Building ARIMA Model in Python,"Business Objective 
A time series is simply a series of data points ordered in time. In a time-series, time is often the independent variable, and the goal is usually to make a forecast for the future.
Time series data can be helpful for many applications in day-to-day activities like:&nbsp;

Tracking daily, hourly, or weekly weather data
Monitoring changes in application performance
Medical devices to visualize vitals in real-time

Auto-Regressive Integrated Moving Average (ARIMA) model is one of the more popular and widely used statistical methods for time-series forecasting. ARIMA is an acronym that stands for Auto-Regressive Integrated Moving Average. It is a class of statistical algorithms that captures the standard temporal dependencies unique to time-series data.
The model is used to understand past data or predict future data in a series. It&rsquo;s used when a metric is recorded in regular intervals, from fractions of a second to daily, weekly or monthly periods.
ARIMAX (Auto-Regressive Integrated Moving Average Exogenous) is an extension of the ARIMA model, and similarly, SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with Exogenous factors) is also an updated version of the ARIMA model. We will see how to implement these two models as well.
We have already covered the concepts of Time Series Project to Build an Autoregressive Model in Python and Build a Moving Average Time Series Forecasting Model in Python
In this project, we will be implementing the ARIMA model on the given dataset.
(ARIMA modelling is the third project in our list of time series projects, you can refer to the previous project through this link : Build a Moving Average Time Series Forecasting Model in Python)
Data Description&nbsp;
The dataset is &ldquo;Call centers&rdquo; data. This data is at month level wherein the calls are segregated at domain level as the call centre operates for various domains. There are also external regressors like no of channels and no of phone lines which essentially indicate the traffic prediction of the inhouse analyst and the resources available.
The total number of rows are 132 and number of columns are 8:

Month, healthcare, telecom, banking, technology, insurance, no of phonelines and no of channels.

Aim
This project aims to build an ARIMA model on the given dataset.
Tech stack 

Language - Python
Libraries - pandas, numpy, matplotlib, seaborn, statsmodels, scipy

Approach 

Import the required libraries and read the dataset
Perform descriptive analysis
Exploratory Data Analysis (EDA) -




Data Visualization




Check for white noise
Check for Random Walk
Perform Stationarity tests




Augmented Dickey-Fuller test
KPSS test




Seasonal decomposition plot
Holt Winter Exponential Smoothing




Create and fit the model
Make predictions on the model
Plot the results




ARIMA model




Create models with varying lag values
Compare these models using log-likelihood and AIC values
Check with the LLR test
ACF Plots of residuals




ARIMAX model




Create a model
ACF plots of residuals




SARIMAX model




Create a model
ACF plots of residuals


"
105,Build a Multi Class Image Classification Model Python using CNN,"Business Objective 
Image classification helps to classify a given set of images as their respective category classes. There are many applications of image classification today, one of them being self-driving cars. An image classification model can be built that recognizes various objects, such as vehicles, people, moving objects, etc., on the road to enable autonomous driving.
There are three main classes of input images in this project, and we need to build a model that can correctly identify a given image. To achieve this, we will be using one of the famous machine learning algorithms used for image classification, i.e., Convolutional Neural Network (or CNN).
Data Description
We will be using a dataset of images categorized into three types, namely the driving license, social security, and others.
The training and testing data each contains these three subfolders. There are around 50 images in each subfolder of testing data, while approximately 200 images in each subfolder of training data.
Aim
To build a sequential model that can perform multiclass classification on a given set of data images.
Tech stack 

Language - Python
Libraries - numpy, matplotlib, tensorflow, cv2

Approach 

Importing the required libraries.
Load and read the data images
Data Visualization




Count plot




Data pre-processing




Create train and validation data
Normalize the data
Reshape the data images




Data augmentation




Using ImageDataGenerator




Model Training




Create a sequential model
Add convolution, maxpool,dropout layers
Add the softmax activation function (As this is a multiclass classification problem)
Pass the optimizer parameter
Compile the model
Fit and train the model
Check for the predictions
Save the model in h5 format.




Inferencing the model




Prediction on the test data


"
106,NLP Project for Beginners on Text Processing and Classification,"Business Overview&nbsp;
Have you ever wondered how the machine deals with the text we write, the sentence we speak? How the machine takes essential decisions based on just the text? Natural Language Processing, often abbreviated as NLP, gives the ability to machines to understand, read, and get meaningful insights from human language. Basically, NLP is the automatic handling of human languages.&nbsp; Nowadays, NLP is prospering due to the large availability of data and computational power. NLP has dug down its routes from healthcare, media, finance to human resources. It is growing with each coming day.&nbsp;
In this series of projects, we will introduce NLP and associated techniques in a very lucid manner. This project aims to give you a brief overview of text preprocessing and building a binary classification model on processed data.&nbsp;

Aim
To understand the basic text preprocessing and build a classification model.

Data Description&nbsp;
The dataset contains more than a thousand reviews about an application openly available to the public. The data includes reviews and sentiment, i.e., is the review positive or negative with various other variables.&nbsp;&nbsp;

Tech Stack


Language: Python


Libraries:&nbsp; pandas, seaborn, matplotlib, sklearn, nltk



Approach


Data Description and visualization


Introduction to NLTK library


Data Preprocessing


Conversion to lower case


Tokenization


Stopwords removal


Punctuation removal


Stemming




Bag of Words


Binary


Non-binary


N-grams




TF-IDF


Model Building and Accuracy&nbsp;


Predictions on new reviews

"
107,"Retail Analytics Project Example using Sqoop, HDFS, and Hive","Business Overview
Retail analytics is the process of delivering analytical data on inventory levels, supply chain movement, customer demand, sales, and other important factors for marketing and procurement choices. Demand and supply data analytics may be utilized to manage procurement levels as well as make marketing decisions. Retail analytics provides us with precise consumer insights and insights into the organization's business and procedures, as well as the scope and need for development.
Aside from inventory management, many retailers employ analytics to determine customer patterns and shifting preferences by merging various sources. Businesses may discover developing trends, and better predict them by combining sales data with a range of criteria. This is strongly related to marketing functions, which also benefit from analytics.
Companies may use retail analytics to strengthen their marketing strategies by better grasping individual preferences and gaining more detailed data. They may design strategies that focus on people and have a greater success rate by combining demographic data with information such as purchasing patterns, preferences, and purchase history.
In this, we will be utilizing Walmart store sales data to perform analysis and answer the following questions:
&nbsp;


Which store has a minimum and maximum sales?


Which store has a maximum standard deviation?


Which store/s has an excellent quarterly growth rate in Q3'2012?


Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together.


&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
The Walmart Store sales dataset consists of 6436 data points, including the following parameters:


Store&nbsp;&nbsp;


Date


Weekly_Sales&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;


Holiday_Flag&nbsp;


Temperature&nbsp;


Fuel_Price&nbsp; &nbsp;


CPI&nbsp; &nbsp;&nbsp;


Unemployment


&nbsp;
Tech Stack
➔ Language: SQL, Bash
➔ Services: AWS EC2, Docker, MySQL, Sqoop, Hive, HDFS
&nbsp;
AWS EC2
A virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture is known as an Amazon EC2 instance. The Amazon Elastic Compute Cloud (EC2) service allows corporate customers to run application applications in a computer environment. Using Amazon EC2 eliminates the need to invest in hardware up front, allowing users to create and deploy apps quickly. Amazon EC2 allows users to launch as many or as few virtual servers as they want, set security and networking, and manage storage.
&nbsp;
Docker
Docker is a containerization platform that is a free source. It allows developers to bundle programs into containers. These standardized executable components combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment.
&nbsp;
MySQL
MySQL is a SQL (Structured Query Language) based relational database management system. Data warehousing, e-commerce, and logging applications are just a few of the uses of the platform.
&nbsp;
Sqoop&nbsp; &nbsp;
Sqoop is a tool for transferring data between Hadoop and relational database servers. It is used to import data from relational databases such as MySQL and Oracle into Hadoop HDFS, Hive, and export data from the Hadoop file system to relational databases.
&nbsp;
Hive
Apache Hive is a fault-tolerant distributed data warehousing solution that enables massive-scale analytics. Using SQL, Hive allows users to read, write, and manage petabytes of data.
Hive is based on Apache Hadoop, an open-source system for storing and processing massive information. As a result, Hive is tightly linked with Hadoop and is built to handle petabytes of data fast. The ability to query massive datasets with a SQL-like interface, using Apache Tez or MapReduce, distinguishes Hive.
&nbsp;
Approach


Containers for all the services are spun up using Docker.


Setup for MySQL is performed for Table creation using the dataset.


Data is imported using Sqoop into Hive.


Data transformation is performed for analysis and reporting.


&nbsp;
Architecture&nbsp;
"
108,MLOps on GCP Project for Autoregression using uWSGI Flask,"Business Objective
&nbsp;
Today, many organizations are turning towards machine learning and Artificial intelligence. AI-based applications promise to deliver new levels of competitiveness, intelligence, and automation for businesses. MLOps is a means of continuous delivery and deployment of these machine learning models. &nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management. &nbsp;
We aim to create the MLOps project by using the google cloud platform (GCP). Here, GCP is used as a cloud provider. But first, we would suggest you go through the former project&nbsp;Time Series Project to Build an Autoregressive Model in Python&nbsp;before starting with this project.
&nbsp;
Aim
&nbsp;
To create an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable machine learning models using Google Cloud Platform (GCP) for the Time Series AutoRegressor Project.
&nbsp;
&nbsp;
Tech stack 

Language - Python
Services - GCP, uWSGI, Flask, Kubernetes, Docker&nbsp;

&nbsp;
Approach 
This repository contains the code files involved in creating an automated MLOps Pipeline on GCP (Google Cloud Platform)
&nbsp;
Steps:


Clone the repository


Place your model (pickle file) inside the output folder


Once you made the changes, create a new repository and commit the changes. From here on, this will be your source repository. Proceed with the below steps
&nbsp;
CLOUD BUILD TRIGGER


In your GCP console, create a new cloud build trigger.


Point the trigger to your source repository


GOOGLE KUBERNETES ENGINE (GKE)


From the console launch a Kubernetes cluster


Connect to the cluster and create the following two files


deployment.yaml


service.yaml


Copy the code for both files from ""Kubernetes Files"" folder in the cloned repository


Execute the following commands


kubectl apply -f deployment.yaml


kubectl apply -f service.yaml




Get the name of the deployment with the following command


kubectl get deployments




CLOUD PUB/SUB


Create a Pub/Sub topic with the name&nbsp;cloud-build


Provide a subscription for the topic, which is to trigger a cloud function


CLOUD FUNCTIONS


From Pub/Sub console launch the cloud function window


Provide the following Environment variables through the GUI console


PROJECT&nbsp;(project name)


ZONE&nbsp;(Region in which the project is deployed ex.uscentral-1)


CLUSTER&nbsp;(Name of the Kubernetes cluster created earlier)


DEPLOYMENT&nbsp;(Name of the deployment inside the Kubernetes cluster)




Copy the program code and requirements.txt files for the cloud function from&nbsp;cloud-function-trigger&nbsp;folder


Configure the Entrypoint for the cloud function as&nbsp;onNewImage


Deploy the function


After successful deployment, make a commit to the source repository and the following will happen in sequence
&nbsp; &nbsp;Cloud Build will push message to Pub/Sub upon successful build&nbsp;
&nbsp; &nbsp;Pub/Sub will trigger the cloud function&nbsp;
Cloud function will deploy the new image on Kubernetes&nbsp;
To test the deployment, check the logs on Kubernetes cluster using the following command&nbsp;


kubectl get pods&nbsp;


kubectl logs&nbsp;&nbsp;


The deployment will reflect in the logs as well as in the endpoints"
109,GCP Project to Explore Cloud Functions using Python Part 1,"Business Overview
Google Cloud is a collection of physical assets, such as computers and hard disk drives, and virtual resources, such as virtual machines (VMs), housed in Google data centers worldwide. This resource distribution has various advantages, including redundancy in a failure and decreased latency by putting resources closer to customers. This release also presents some guidelines for combining resources.
GCP offers a web-based graphical user interface for managing Google Cloud projects and resources. If a user prefers to work at the command line, the gcloud command-line tool can handle most Google Cloud activities.
In this project, we will explore the following services of GCP:


Cloud Storage


Compute Engine


PubSub&nbsp;


&nbsp;
Tech Stack
➔ Language: Python3
➔ Services: Cloud Storage, Compute Engine, Pub/Sub
&nbsp;
Cloud Storage
Cloud Storage is a service that allows users to store their data on the Google Cloud. An object is an immutable piece of data that consists of a file in any format. Objects can be stored in containers known as buckets. All buckets are related to a project, and the user may organize their projects into organizations. After starting a project, users may create Cloud Storage buckets, upload things to the buckets, and get objects. Users can also give rights to make data accessible to certain domains or for specific use cases such as establishing a website.
&nbsp;
Compute Engine
Compute Engine provides size, performance, and value, allowing users to deploy huge compute clusters on Google's infrastructure effortlessly. There are no initial costs, and hundreds of virtual CPUs may be run on a system that provides speedy consistent performance.
&nbsp;
Pub/Sub
Pub/Sub allows services to interact asynchronously and is used in streaming analytics and data integration pipelines to ingest and disseminate data. It, like Kafka, allows users to design systems of event producers and consumers, known as publishers and subscribers. Publishers connect with subscribers asynchronously by disseminating events rather than synchronous remote procedure calls (RPCs). Publishers transmit events to the Pub/Sub service without considering how or when these events will be handled. Pub/Sub then sends events to any services that need to respond to them. This type of asynchronous integration improves the overall flexibility and robustness of the system.
"
110,MLOps AWS Project on Topic Modeling using Gunicorn Flask,"Business Overview
Machine learning operations are widely known as MLOps include various technologies, processes, and practices that automate deployment, monitoring, and management of machine learning models in production. Many organizations are turning towards machine learning and Artificial intelligence. MLOps advocates automation and monitoring at all steps of the ML system. In this project, we aim to provide hands-on experience in MLOps by using cloud computing. Amazon Web Services(AWS) is used as a cloud provider. We would advise you to have a basic understanding of NLP Project on LDA Topic Modelling Python using RACE Dataset before jumping into this project.&nbsp;
&nbsp;
&nbsp;
Aim
To provide an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable machine learning models by using Amazon Web Services(AWS)
&nbsp;
&nbsp;
Tech Stack
➔ Language: Python
➔ Libraries: Flask, gunicorn, scipy, nltk, tqdm, numpy, joblib, pandas, scikit_learn, boto3
➔ Services: Flask, Docker, AWS, Gunicorn&nbsp;
&nbsp;
&nbsp;
Prerequisites
It is advisable to have a basic knowledge of the following services to get an understanding of the project.


Flask


AWS s3


Aws ECR


AWS ECS


AWS EC2 Load balancer


AWS Code commit


AWS Code Build


AWS Code Deploy


AWS Code Pipeline

"
111,Time Series Project to Build a Multiple Linear Regression Model,"Business Objective 
A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable, and the goal is usually to make a forecast for the future.
Time series data can be helpful for many applications in day-to-day activities like:&nbsp;

Tracking daily, hourly, or weekly weather data
Monitoring changes in application performance
Medical devices to visualize vitals in real-time

Linear regression is widely used in practice and adapts naturally to even complex forecasting tasks. In this project, we will deal with the multiple linear regression model. The aim of the multiple linear regression is to model a dependent variable (output) by independent variables (inputs).
In this series of time series projects, we have already covered three major topics, Time Series Project to Build an Autoregressive Model in Python Build a Moving Average Time Series Forecasting Model in Python and Time Series Forecasting Project-Building ARIMA Model in Python.
In this project, we will be implementing the Multiple linear regression model on the given dataset.
(Multi linear regression model is the fourth project in our list of time series projects, you can refer to the previous project through this link :Time Series Forecasting Project-Building ARIMA Model in Python)
&nbsp;
Data Description&nbsp;
We will be using &ldquo;Call_centres&rdquo; data. This data is at the month level wherein the calls are segregated at the domain level as the call center operates for various domains. There are also external regressors like no of channels and no of phone lines which essentially indicate the traffic prediction of the inhouse analyst and the resources available.
There are about 130 rows and 8 columns in the dataset

Month, healthcare, telecom, banking, technology, insurance, no of phonelines and no of channels.
The multiple linear regression model will be built using three variables, banking (dependent variable), no of phonelines, and no of channels (independent variables)

&nbsp;
Aim
This project aims to build a Multiple linear regression model on the given dataset
&nbsp;
Tech stack 

Language - Python
Libraries - pandas, numpy, matplotlib, scipy, scikit learn, gplearn

&nbsp;
Approach 

Import the required libraries and read the dataset
Data pre-processing




Setting date as the index
Setting frequency as month




Exploratory Data Analysis (EDA) -




Data Visualization




Check for normality




Density plots
Q-Q plots




Multiple linear regression model




Train test split
Train the model
Fit the model
Make predictions
Plot the results




Residual analysis




Remove autocorrelation with varying lag values
Check for the normality of the variables
Train and fit the model
Make predictions and plot the results




Symbolic regression model




Create a model
Train the model
Fit the model
Make predictions and plot the results



&nbsp;"
112,Deep Learning Project for Beginners with Source Code Part 1,"Overview
If you are an experienced person or a fresher in the field of data science, you must have already heard about the term Deep learning. Deep learning takes its inspiration from the structure and function of the brain. In simple terms, deep learning tries to draw conclusions similar to humans if given a logical structure to analyze the data. Self-driving cars are phenomenal examples of deep learning. As the world is moving towards automation it becomes important to understand the key concepts behind it. In this series of projects, we try to give you basic ideas related to deep learning and its different algorithms.&nbsp;
&nbsp;
Aim
To introduce the Deep Neural Network and its implementation

Data Description
The dataset used is a licensed dataset. It contains information about 86K different businesses over various features. The target variable is the status of license which has five different categories.

Tech Stack


Language: Python


Libraries: pandas, seaborn, numpy, matplotlib, scikit learn, h2o, tensorflow



Approach


Data Description


Exploratory Data Analysis


Data Cleaning


Preparing data for analysis


Base model building using h2o&nbsp;


Building deep neural network model


Predictions on test data


Model deployment using flask gunicorn

"
113,MLOps on GCP Project for Moving Average using uWSGI Flask,"Business Objective 
Today, many organizations are turning towards machine learning and Artificial intelligence. AI-based applications promise to deliver new levels of competitiveness, intelligence, and automation for businesses. MLOps is a means of continuous delivery and deployment of these machine learning models. &nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management. &nbsp;
We aim to create the MLOps project by using the google cloud platform (GCP). Here, GCP is used as a cloud provider. But first, we would suggest you go through the former project&nbsp;Build a Moving Average Time Series Forecasting Model in Python&nbsp;before starting with this project.
&nbsp;
Aim
To create an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable machine learning models using Google Cloud Platform (GCP) for the Time Series Moving Average Project.
&nbsp;
Tech stack 

Language - Python
Services - GCP, uWSGI, Flask, Kubernetes, Docker&nbsp;

&nbsp;
Approach 
Steps:

Clone the repository

Once you made the changes, create a new repository and commit the changes. From here on, this will be your source repository. Proceed with the below steps
CLOUD BUILD TRIGGER

In your GCP console, create a new cloud build trigger.
Point the trigger to your source repository

&nbsp;
GOOGLE KUBERNETES ENGINE (GKE)

From the console launch a Kubernetes cluster
Connect to the cluster and create the following two files
deployment.yaml
service.yaml
Copy the code for both files from ""Kubernetes Files"" folder in cloned repository
Execute the following commands

kubectl apply -f deployment.yaml
kubectl apply -f service.yaml


Get the name of the deployment with the following command

kubectl get deployments



CLOUD PUB/SUB

Create a Pub/Sub topic with the name cloud-build
Provide a subscription for the topic, which is to trigger a cloud function

CLOUD FUNCTIONS

From Pub/Sub console launch the cloud function window
Provide the following Environment variables through the GUI console

PROJECT&nbsp;(project name)
ZONE&nbsp;(Region in which in the project is deployed ex.uscentral-1)
CLUSTER&nbsp;(Name of the kubernetes cluster created earlier)
DEPLOYMENT&nbsp;(Name of the deployment inside the kubernetes cluster)

Copy the program code and requirements.txt files for the cloud function from&nbsp;cloud-function-trigger&nbsp;folder
Configure the Entrypoint for the cloud function as&nbsp;onNewImage
Deploy the function

After successful deployment, make a commit to source repository and the following will happen in sequence 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cloud Build will push message to Pub/Sub upon successful build
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pub/Sub will trigger the cloud function
Cloud function will deploy the new image on Kubernetes 
To test the deployment, check the logs on kubernetes cluster using the following command 

kubectl get pods
kubectl logs 

The deployment will reflect in the logs as well as in the endpoints
&nbsp;"
114,A Hands-On Approach to Learn Apache Spark using Scala,"
Business Overview
&nbsp;
Apache Spark is a distributed processing solution for large data workloads that is open-source. For quick analytic queries against any quantity of data, it uses in-memory caching and efficient query execution. It offers code reuse across many workloads&mdash;batch processing, interactive queries, real-time analytics, machine learning, and graph processing&mdash;and provides development APIs in Java, Scala, Python, and R.
&nbsp;
Hadoop MapReduce is a programming technique that uses a parallel, distributed method to handle extensive data collections. Developers do not have to worry about job distribution or fault tolerance when writing massively parallelized operators. The sequential multi-step procedure required to perform a task, however, is a difficulty for MapReduce. MapReduce gets data from the cluster, conducts operations, and publishes the results to HDFS at the end of each phase. Due to the latency of disk I/O, MapReduce tasks are slower since each step involves a disk read and write. By doing processing in memory, lowering the number of steps in a job, and reusing data across several concurrent processes, Spark was built to solve the constraints of MapReduce. With Spark, data is read into memory in a single step, operations are executed, and the results are written back, resulting in significantly quicker execution. Spark additionally reuses data by employing an in-memory cache to substantially accelerate machine learning algorithms that execute the same function on the same dataset several times.
&nbsp;
&nbsp;
Tech Stack
&nbsp;
➔ Language: Scala, SQL
&nbsp;
➔ Services: Apache Spark, IntelliJ
&nbsp;
&nbsp;

Approach
&nbsp;

Using Docker

&nbsp;



Implementing RDD Transformation and Action functions



&nbsp;

Using IntelliJ

&nbsp;



Using IntelliJ to setup SBT for Scala-Spark project



&nbsp;



Spark Analysis for the given dataset



&nbsp;
&nbsp;
Dataset Description
&nbsp;
Fitness Tracker data is used to perform transformations and gain insights. Few parameters included in this data are:

Platform
Activity
Heartrate
Calories
Time_stamp
"
115,GCP MLOps Project to Deploy ARIMA Model using uWSGI Flask,"Business Objective 
MLOps is a set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently. Today, many organizations are turning towards machine learning and Artificial intelligence. AI-based applications promise to deliver new levels of competitiveness, intelligence, and automation for businesses. MLOps is a means of continuous delivery and deployment of these machine learning models. &nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management. &nbsp;
We aim to create the MLOps project by using the google cloud platform (GCP). Here, GCP is used as a cloud provider. But first, we would suggest you go through the former project Time Series Forecasting Project-Building ARIMA Model in Python&nbsp;before starting with this project.
&nbsp;
Aim
To create an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable machine learning models using Google Cloud Platform (GCP) for the Time Series ARIMA Project.
&nbsp;
Tech stack 

Language - Python
Services - GCP, uWSGI, Flask, Kubernetes, Docker&nbsp;

&nbsp;
Approach 
Steps:

Clone the repository
Place your model file inside the&nbsp;output&nbsp;folder

Once you made the changes, create a new repository and commit the changes. From here on, this will be your source repository. Proceed with the below steps
&nbsp;
CLOUD BUILD TRIGGER

In your GCP console, create a new cloud build trigger.
Point the trigger to your source repository

GOOGLE KUBERNETES ENGINE (GKE)

From the console launch a Kubernetes cluster
Connect to the cluster and create the following two files
deployment.yaml
service.yaml
Copy the code for both files from ""Kubernetes Files"" folder in cloned repository
Execute the following commands

kubectl apply -f deployment.yaml
kubectl apply -f service.yaml

Get the name of the deployment with the following command

kubectl get deployments


CLOUD PUB/SUB

Create a Pub/Sub topic with the name&nbsp;cloud-build
Provide a subscription for the topic, which is to trigger a cloud function

CLOUD FUNCTIONS

From Pub/Sub console launch the cloud function window
Provide the following Environment variables through the GUI console

PROJECT&nbsp;(project name)
ZONE&nbsp;(Region in which in the project is deployed ex.uscentral-1)
CLUSTER&nbsp;(Name of the kubernetes cluster created earlier)
DEPLOYMENT&nbsp;(Name of the deployment inside the kubernetes cluster)

Copy the program code and requirements.txt files for the cloud function from&nbsp;cloud-function-trigger&nbsp;folder
Configure the Entrypoint for the cloud function as&nbsp;onNewImage
Deploy the function

After successful deployment, make a commit to source repository and the following will happen in sequence * Cloud Build will push message to Pub/Sub upon successful build * Pub/Sub will trigger the cloud function * Cloud function will deploy the new image on Kubernetes * To test the deployment, check the logs on kubernetes cluster using the following command *&nbsp;kubectl get pods&nbsp;*&nbsp;kubectl logs &nbsp;* The deployment will reflect in the logs as well as in the endpoints
&nbsp;"
116,BERT Text Classification using DistilBERT and ALBERT Models,"Business Objective
In Part-1&nbsp;of the Transformer series, we have discussed the&nbsp;techniques of NLP, starting from&nbsp;Bag of words&nbsp;to&nbsp;Transformer architecture. We finally discussed BERT&nbsp;which is one of the&nbsp;State-of-the-Art Transformer&nbsp;models for downstream NLP tasks (Multi-Class Text Classification with Deep Learning using BERT)
In Part-2 of the series, we came to know the limitations of BERT and the ways to improve it. We then explored the concepts of Auto Regression, Auto Encoding and two new models, RoBERTa and XLNet which further improved on BERT performance significantly by changes in training techniques and architectures. (Text Classification with Transformers-RoBERTa and XLNet Model)
In Part-3 of this series, we will deal with the&nbsp;short comings&nbsp;of all these models in terms of&nbsp;Memory Optimization, Prediction Latency &amp; Space usage. We will understand new techniques and architecture modifications that help to solve these issues while deploying a model to production and study in detail two new models, which are:

ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
DistilBERT: A distilled version of BERT: smaller, faster, cheaper, and lighter

And see how these optimize space and memory with minor changes in prediction accuracy.
Finally, we will have a comparative study across all the five transformer models over the three series.
&nbsp;
Data Description&nbsp;
The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in classes.txt. From each of these 14 ontology classes, we randomly choose,

40,000 training samples and 5,000 testing samples.
Therefore, the total size of the training dataset is 560,000 and the testing dataset 70,000.

There are three columns in the dataset,

Title: the title of the document
Content: the body of the document
Label: one of the 14 possible topics.

&nbsp;
Aim
The project aims at building two models, namely ALBERT and DistilBERT to perform classification on the DBpedia ontology dataset.
&nbsp;
Tech stack 

Language - Python
Libraries -datasets, numpy, pandas, matplotlib, ktrain, transformers, tensorflow, sklearn

&nbsp;
Environment

Jupyter Notebook
Google Colab Pro (Recommended)

&nbsp;
Approach 

Install the required libraries
Load the &lsquo;DBpedia&rsquo; dataset
Load train test data
Data pre-processing




Assign column names to the dataset
Append and save the dataset
Drop redundant columns
Add text length column for visualization




Perform data visualization




Histogram plots




ALBERT model




Check for hardware and RAM availability S
Import necessary libraries
Data interpretations
Create an ALBERT model instance.
Split the train and validation data
Perform Data Pre-processing
Compile ALBERT model in a K-train learner object
Fine-tune the ALBERT model on the dataset
Check for the performance on validation data
Save the ALBERT model




DistilBERT model




Check for hardware and RAM availability
Import necessary libraries
Data interpretations
Create a DistilBERT model instance.
Split the train and validation data
Perform Data Pre-processing
Compile DistilBERT model in a K-train learner object
Fine-tune the DistilBERT model on the dataset
Check for the performance on validation data
Save the DistilBERT model




Similarly create a BERT model using the DBpedia dataset for comparative study
Follow the above steps for creating a BERT model on the &lsquo;Emotion&rsquo; dataset
Follow the above steps for creating an ALBERT model on the &lsquo;Emotion&rsquo; dataset
Follow the above steps for creating a DistilBERT model on the &lsquo;Emotion&rsquo; dataset
Save all the generated model

&nbsp;"
117,Classification Projects on Machine Learning for Beginners - 2,"Overview
In machine learning, Classification is one of the most widely used techniques with various applications. For sentiment analysis, spam detection, risk assessment, churn prediction, and medical diagnosis classification have served as a very simple yet powerful method. In this project, we aim to give you hands-on experience and theoretical explanations of various ensemble techniques. You can find the first project of this series here.&nbsp;&nbsp;

Aim
Understanding various Ensemble techniques and implementing them to predict license status for the given business.

Data Description
The dataset used is a licensed dataset. It contains information about 86K different businesses over various features. The target variable is the status of the license, which has five different categories.

Tech Stack


Language: Python


Libraries: pandas, scikit_learn, category_encoders, numpy, os, seaborn, matplotlib, hyperopt, xgboost



Approach


Data Description


Exploratory Data Analysis


Data Cleaning


Missing Value imputation


Outlier Detection




Data Imbalance


Data Encoding


Model Building


Random Forest


AdaBoost


XGBoost




Feature importance


Hyperparameter tuning


Random search optimization


Grid search optimization


Bayesian optimization



"
118,Build an optimal End-to-End MLOps Pipeline and Deploy on GCP,"Business Overview
Data Science has become a significant part of solving real-world problems in all domains. Many of the industries are investing in leveraging business value with the help of data science. Continues integration of data, and Continues deployment of models also known as CI-CD, assists the continuous operation of data science models in production.&nbsp;
While there are various ways to deploy the ML pipeline in production, industrialists still struggle to take advantage of such services from the wide variety of services available. In this project, we aim to deploy a machine learning model developed in the project Loan Eligibility Prediction using Gradient Boosting Classifier on the Google cloud platform(GCP) with a minimal number of services.&nbsp;
&nbsp;
&nbsp;
&nbsp;
Aim
To build optimal MLops pipeline on Google cloud platform to deploy loan eligibility prediction model in production

Tech Stack
➔ Language: Python
➔ Libraries: Flask, gunicorn, scipy, xgboost, joblib, seaborn, fancyimpute, scikit_learn
➔ Services: Flask, Docker, GCP, Gunicorn&nbsp;
&nbsp;
&nbsp;
Prerequisites
It is advisable to have a basic knowledge of the following services to better understand the project.


Flask


Docker


Cloud Build


Cloud Run


Cloud Source Repository

"
119,Build a Logistic Regression Model in Python from Scratch,"Business Objective 
Predicting a qualitative response for observation can be referred to as classifying that observation since it involves assigning the observation to a category or class. Classification forms the basis for Logistic Regression. Logistic Regression is a supervised algorithm used to predict a dependent variable that is categorical or discrete. Logistic regression models the data using the sigmoid function.
Churned Customers are those who have decided to end their relationship with their existing company. In our case study, we will be working on a churn dataset.
XYZ is a service-providing company that provides customers with a one-year subscription plan for their product. The company wants to know if the customers will renew the subscription for the coming year or not.
&nbsp;
Data Description&nbsp;
This data provides information about a video streaming service&nbsp;company, where they want to predict if the customer will churn or not. The CSV consists of around 2000 rows and 16 columns.
&nbsp;
Aim
Build a logistics regression learning model on the given dataset to determine whether the customer will churn or not.
&nbsp;
Tech stack 

Language - Python
Libraries - numpy, pandas, matplotlib, seaborn, sklearn, pickle, imblearn, statsmodel

&nbsp;
Approach 

Importing the required libraries and reading the dataset.
Inspecting and cleaning up the data
Perform data encoding on categorical variables
Exploratory Data Analysis (EDA)




Data Visualization




Feature Engineering




Dropping of unwanted columns




Model Building




Using the statsmodel library




Model Building




Performing train test split
Logistic Regression Model




Model Validation (predictions)




Accuracy score
Confusion matrix
ROC and AUC
Recall score
Precision score
F1-score




Handling the unbalanced data




With balanced weights
Random weights
Adjusting imbalanced data
Using SMOTE




Feature Selection




Barrier threshold selection
RFE method




Save the model in the form of a pickle file.
"
120,Deploy Transformer BART Model for Text summarization on GCP,"Business Objective 
Text summarization is the process of automatically generating natural language summaries from an input document while retaining the important points.&nbsp; Abstractive Summarization is a task in NLP that aims to generate a concise summary of a source text. Unlike the extractive summarization technique, abstractive summarization does not simply copy essential phrases from the source text but also potentially come up with new relevant phrases, which can be seen as paraphrasing.
This ML model deployment project makes two initial assumptions, first that there is an ML model built (Transformer model in our case), and second there is an interface available for the model (Flask app)
We aim to create the ML model deployment project by using the google cloud platform (GCP). Here, GCP is used as a cloud provider. But first, we would suggest you go through the former project Abstractive Text Summarization using Transformers-BART Model before starting this project.
&nbsp;
Aim
To deploy the machine learning model for the &lsquo;Abstractive Text Summarization using Transformers-BART Model&rsquo; project on Google Cloud Platform (GCP)
&nbsp;
Tech stack 

Language - Python
Services - GCP, Docker&nbsp;

&nbsp;
Approach 
Three components for ML model deployment

Source repository
Cloud build
Virtual Machine

&nbsp;
Steps:

Cloud Source Repository




Create a new repository or Connect to an external repository
Clone the repository




Git commands




Upload the necessary files to the repository in zip format
Unzip the folder and move to the git folder
Follow steps (git commands) in order to add the files to the cloud source repository




CLOUD BUILD TRIGGER




In your GCP console, create a new cloud build trigger.
Run the trigger




Create a docker file




Go to Google Container Repository (GCR)&nbsp;




Virtual Machine creation




Create a VM instance




Server deployment 1 (server-based deployment




Connect to VM instance
Follow the steps given in the &lsquo;setup-new-vm.sh&rsquo; file
Once the above steps are successfully completed, go to the particular URL (copy the external IP code)
This will direct you to the abstractive summarization page, where we can extract the summarization of any news article.




Server deployment 2 (docker deployment)




Connect to VM
Follow the steps given in the &lsquo;install-docker.sh&rsquo; file
Pull the docker image from the container and run it on the VM
Once the above steps are successfully completed, go to the particular URL (copy the external IP code)
This will direct you to the abstractive summarization page, where we can extract the summarization of any news article


"
121,Build a Multi ClassText Classification Model using Naive Bayes,"Business Overview&nbsp;
Natural Language Processing, often abbreviated as NLP, gives the ability to machines to understand, read, and get meaningful insights from human language. In the last project of this series, we understood how to perform text preprocessing and classification using Logistic regression which can be found here. While we dealt with binary classification, many of the fields are concerned about multiclass classification. This project aims to give you a brief overview of text classification where there are more than two classes available and build a classification model on processed data using the Naive Bayes algorithm. This project also explains the working of the Naive Bayes algorithm and related terminology.

Aim
To understand the Naive Bayes algorithm and build a multiclass classification model.

Data Description&nbsp;
The dataset contains more than two million customer complaints about consumer financial products. Amongst the various available columns, we have a column that contains the actual text of the complaint and one column containing the product for which the customer is raising the complaint.

Tech Stack


Language: Python


Libraries:&nbsp; pandas, seaborn, matplotlib, sklearn, nltk



Approach


Introduction to Naive Bayes algorithm


Data Description and visualization


Data Preprocessing


Conversion to lower case


Tokenization


Stopwords removal


Punctuation removal




Model Building and Accuracy&nbsp;


Predictions on new reviews

"
122,Skip Gram Model Python Implementation for Word Embeddings,"Business Overview&nbsp;
After typing a few words in the search box or a message, we often get automatic suggestions for using the following words. One might wonder how the mobile phone or computer knows what to suggest? All thanks to the world's leading technology, which gives machines power to learn millions and billions of data points, in this case, documents. In Natural Language Processing, word embeddings refer to the text's numerical representation, which is understandable by machines. One must have heard about Word2Vec, Count vectorizing, which are some well-known algorithms for word embeddings. In this project, we give some basic ideas about word2vec and one hot encoder. This project is mainly concerned with creating word embeddings by implementing the continuous skip-gram algorithm discussed in &ldquo;Distributed Representations of Words and Phrases and their Compositionality&rdquo; by Mikolov et. al.
Aim
To understand the continuous skip-gram algorithm and build a model to create word embeddings over a set of documents
Data Description&nbsp;
The dataset contains more than two million customer complaints about consumer financial products. Amongst the various available columns, we have a column that contains the actual text of the complaint and one column containing the product for which the customer is raising the complaint.
Tech Stack


Language: Python


Libraries:&nbsp; pandas, torch, collections,&nbsp; nltk, numpy, pickle, os, re


Prerequisite


Classes in python


The torch framework


Multiclass Text Classification using Naive Bayes in Python


Approach


Introduction to the continuous skip-gram algorithm


Data Description&nbsp;


Data Preprocessing


Missing values treatment


Conversion to lower case


Punctuation removal


Digits removal


Additional spaces removal


tokenization




Building Data loader


Building Skip-gram model using the torch framework


Model training


Word embedding to get word vectors

"
123,Model Deployment on GCP using Streamlit for Resume Parsing,"Business Objective 
The resume parser application can take in millions of resumes parse the needed fields and categorize them. We use popular python libraries like Spacy and OCR text classifications. In this project, for ML model deployment, there are two initial assumptions, first that there is an ML model built and, second, there is an interface available for the model (Streamlit app)
We aim to create the ML model deployment project by using the google cloud platform (GCP). Here, GCP is used as a cloud provider. But first, we would suggest you go through the former project, Resume parsing with Machine learning - NLP with Python OCR and Spacy, before starting this project.
&nbsp;
Aim
To deploy the machine learning model for the &lsquo;Resume parsing with Machine learning - NLP with Python OCR and Spacy&rsquo; project on Google Cloud Platform (GCP)
&nbsp;
Tech stack 

Language - Python
Services - GCP, Docker

&nbsp;
Approach 
Three components for ML model deployment

Source repository
Cloud build
Virtual Machine

&nbsp;
Steps:

Cloud Source Repository




Create a new repository or Connect to an external repository
Clone the repository




Git commands




Upload the necessary files to the repository in zip format
Unzip the folder and move to the git folder
Follow steps (git commands) in order to add the files to the cloud source repository




CLOUD BUILD TRIGGER




In your GCP console, create a new cloud build trigger.
Run the trigger




Create a docker file




Go to Google Container Repository (GCR) (Deployed images are stored)




Virtual Machine creation




Create a VM instance




Server deployment 1 (server-based deployment




Connect to VM instance
Follow the steps given in the &lsquo;setup-new-vm.sh&rsquo; file
Once the above steps are successfully completed, go to the particular URL (copy the external IP code and port)
This will direct you to the resume parse page, where we can add any resume document and extract the details.




Server deployment 2 (docker deployment)




Connect to VM
Follow the steps given in the &lsquo;install-docker.sh&rsquo; file
Pull the docker image from the container and run it on the VM
Once the above steps are successfully completed, go to the particular URL (copy the external IP code and port)
This will direct you to the resume parse page, where we can add any resume document and extract the details.


"
124,Azure Stream Analytics for Real-Time Cab Service Monitoring,"Business Overview
The process of analyzing and measuring data as soon as it enters the database is referred to as real-time analytics.&nbsp; Thus, users gain insights or may draw conclusions as soon as data enters their system. Businesses can react quickly using real-time analytics. They have the ability to grasp opportunities and avert issues before they occur.
Batch-style analytics, on the other hand, might take hours or even days to provide findings. As a result, batch analytical systems frequently produce only static insights based on lagging indications. Real-time analytics insights may help organizations stay ahead of the competition. These pipelines for streaming data generally follow a 3 step process, i.e., Ingest, Analyze and Deliver.
In this project, we will address the problem of Real-Time cab service monitoring. Usage of cab services like Uber, Ola, etc., are increasing day by day, and they have become a part of the regular commute. The architecture diagram given is a replica of how these companies monitor their real-time booking service via the Power BI dashboard.
&nbsp;
Data Pipeline
A data pipeline is a method of moving information from one system to another. The information may or may not be updated, and it may be processed in real-time (or streaming) rather than in batches. The data pipeline includes everything from gathering data through multiple ways to storing raw data, cleaning, verifying, and transforming data into a query-able format, showing KPIs, and overseeing the entire process.
&nbsp;
Tech Stack
➔Language: SQL, C#
➔Services: Azure VM, Docker, Azure Stream Analytics, Azure Event Hubs, Blob Storage, Cosmos DB, Power BI
&nbsp;
Azure VM
Azure Virtual Machines (VM) are one of several forms of scalable, on-demand computing resources offered by Azure. VMs are typically used when you want greater control over the computing environment than the other options provide. An Azure VM provides you virtualization flexibility without the need to purchase and maintain the actual hardware that runs it. You must still maintain the VM by configuring, patching, and installing the software that runs on it.
&nbsp;
Azure Stream Analytics
Azure Stream Analytics is a real-time analytics and sophisticated event processing engine for analyzing and processing large amounts of rapid streaming data from various sources at the same time. Information gathered from a variety of input sources, including devices, sensors, clickstreams, social media feeds, and apps, may be used to identify patterns and correlations. These patterns may be used to start workflows and trigger activities like issuing alarms, sending data into a reporting platform, or storing altered data for later use.
&nbsp;
Azure Event Hubs
Azure Event Hubs is a large data streaming platform and event ingestion service. It can receive and process millions of events per second. Data delivered to an event hub may be converted and saved using any real-time analytics provider or batching/storage adapters.
&nbsp;
Power BI
Power BI is a set of software services, applications, and connections that work together to transform disparate data sources into coherent, visually engaging, and interactive insights. Your data might be in the form of an Excel spreadsheet or a combination of hybrid cloud-based and on-premises data warehouses. Power BI makes it simple to connect to your data sources, visualize and find what's essential, and share it with anyone or whomever you want.
&nbsp;
Approach


We use Event hubs as the ingestion point for cab booking/drop ride details. We simulate real-time data using Azure VM running a generator code.


We use stream analytics to join some reference data like customer details and cab owner details which are maintained in Blob storage. The joined data is used to derive multiple trends like average commission per km, routes with max bookings(to identify trends)


We will keep a copy of the data in cosmos No-SQL DB, which will feed the power BI dashboards for business users' understanding.


Since we are using real-time streaming and analytics service, we might have to monitor it and trigger it in case of input overload via email service to service admin.


&nbsp;
Architecture&nbsp;
"
125,Build Time Series Models for Gaussian Processes in Python,"Business Objective 
A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable, and the goal is usually to make a forecast for the future.
Time series data can be helpful for many applications in day-to-day activities like:&nbsp;

Tracking daily, hourly, or weekly weather data
Monitoring changes in application performance
Medical devices to visualize vitals in real-time

Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression. Gaussian probability distribution functions summarize the distribution of random variables, whereas Gaussian processes summarize the properties of the functions, e.g., the parameters of the functions. Gaussian processes can be used as a machine learning algorithm for classification predictive modeling
&nbsp;
We have already covered the concepts of Autoregression modeling, Moving Average Smoothing techniques, ARIMA model and Multiple linear Regression.
In this project, we will be implementing the Gaussian model on the given dataset.
(Guassian model is the fifth project in our list of time series projects, you can refer to the previous project through this link : Time Series Project for Multiple Linear Regression in Python )
&nbsp;
Data Description&nbsp;
The dataset is &ldquo;Call-centres&rdquo; data. This data is at the month level wherein the calls are segregated at the domain level as the call centre operates for various domains. There are also external regressors like no of channels and no of phone lines which essentially indicate the traffic prediction of the inhouse analyst and the resources available.
The total number of rows are 132 and number of columns are 8:

Month, healthcare, telecom, banking, technology, insurance, no of phonelines and no of channels.

&nbsp;
Aim
This project aims to build a Gaussian model on the given dataset.
&nbsp;
Tech stack 

Language - Python
Libraries - pandas, numpy, matplotlib, seaborn, sklearn, scipy

&nbsp;
Approach 

Import the required libraries and read the dataset
Perform descriptive analysis
Data pre-processing




Converting date to numeric
Setting date as the index




Exploratory Data Analysis (EDA) -




Data Visualization




Check for normality




Density plots
QQ-plots




Gaussian process




Initiate kernels
Perform train-test split
Create a Gaussian process regressor model
Fit the model
Generate predictions
Plot the results




Difference




Create a residual column (difference)
Check for normality
Train test split
Initiate kernel
Create a gaussian model
Fit the model
Generate predictions on test data
Plot the results


"
126,Build ARCH and GARCH Models in Time Series using Python,"Business Objective 
A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable, and the goal is usually to make a forecast for the future.
Time series data can be helpful for many applications in day-to-day activities like:&nbsp;

Tracking daily, hourly, or weekly weather data
Monitoring changes in application performance
Medical devices to visualize vitals in real-time

The ARCH (Autoregressive Conditional Heteroskedasticity) process introduced by Engle (1982) explicitly recognizes the difference between the unconditional and the conditional variance, allowing the latter to change over time due to past errors.
Autoregressive: The current value can be expressed as a function of the previous values, i.e., correlated.&nbsp;
Conditional: This informs that the variance is based on past errors.&nbsp;
Heteroskedasticity: This implies the series displays unusual variance (varying variance).
&nbsp;
As the name suggests, the GARCH is just the generalized version of the ARCH model. This generalization is expressed in including past variances as well as past squared residuals to estimate current (and subsequent) variances. The generalization comes from the fact that including a single past variance would (in theory) contain in itself the explanatory power of all other previous squared error terms. On the other hand, GARCH is a better fit for modeling time series data when the data exhibits heteroskedasticity but also volatility clustering. It serves as a sort of ARMA equivalent to the ARCH, where we&rsquo;re including both past values and past errors (albeit squared).
&nbsp;
We have already covered the concepts of Autoregression modelling, Moving Average Smoothing techniques, ARIMA model, Multiple linear regression, and Gaussian process in the series till now.&nbsp;
In this project, we will be implementing the ARCH, GARCH models on the given dataset.

(ARCH-GARCH model is the sixth project in our list of time series projects, you can refer to the previous project through this link : Build Time Series Models for Gaussian Processes in Python )
&nbsp;
Data Description&nbsp;
The dataset is &ldquo;Call-centres&rdquo; data. This data is at month level wherein the calls are segregated at domain level as the call centre operates for various domains. There are also external regressors like no of channels and no of phone lines which essentially indicate the traffic prediction of the inhouse analyst and the resources available.
The total number of rows are 132 and number of columns are 8:

Month, healthcare, telecom, banking, technology, insurance, no of phonelines and no of channels.

&nbsp;
Aim
This project aims to build ARCH and GARCH models on the given dataset.
&nbsp;
Tech stack 

Language - Python
Libraries - pandas, numpy, matplotlib, seaborn, statsmodels, scipy, arch

&nbsp;
Approach 

Import the required libraries and read the dataset
Perform descriptive analysis
Data pre-processing




Setting date as Index
Setting frequency as month




Exploratory Data Analysis (EDA) -




Data Visualization




Perform train test split
Calculating returns and volatility
ARCH model




Install libraries
Build ARCH models with varying parameters
Build higher-lag ARCH models




GARCH model




Build a GARCH model




Forecasting the results




Forecast results on the best model


"
127,AWS MLOps Project to Deploy Multiple Linear Regression Model,"Business Objective 
Deployment is the method by which you integrate a machine learning model into an existing production environment to make practical business decisions based on data.
MLOPS is a means of continuous delivery and deployment of a machine learning model.&nbsp;&nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management.&nbsp;
In this project, we aim to create the MLOps project for the time series multiple linear regression model (Time Series Project for Multiple Linear Regression in Python) on the AWS cloud platform (Amazon Web Services) that is cost-optimized. Cost-optimized model deployment means that we will be using minimal services.&nbsp;
&nbsp;
Aim
To create an MLOps project using the Amazon Web Services (AWS) platform to deploy time series multiple linear regression model in production.
&nbsp;
Tech stack 

Language - Python
Libraries - Flask, pickle
Services - Flask, AWS, Docker, Lightsail, EC2

&nbsp;
Approach 

Model creation




Save the model in a pickle format(.pkl)




Flask app




Create a flask file




EC2 machine creation




Create an instance on the AWS management console
Launch the instance
Install the &lsquo;putty&rsquo; tool on your local for login




EC2 and Docker setup




Refer to the steps present in the &lsquo;install-docker.sh&rsquo; file




AWS CLI installation




Refer to the steps from &lsquo;install-aws-cli.sh&rsquo; file




Lightsail installation




Refer to the steps from &lsquo;install-lightsail-cli.sh&rsquo;




Upload files into the EC2 machine

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Method1



Upload the code file in zip format on AWS console (cloud shell)



&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Method 2



Create an S3 storage bucket
Copy the object URL, get the URL on the EC2 machine
Unzip the bitbucket folder




Deployment




Follow the order of installation, from &lsquo;lightsail-deployment.md&rsquo; file


"
128,Deploying Machine Learning Models with Flask for Beginners,"Business Objective:
A proper machine learning cycle does not stop with just model training and validation. Model deployment plays a crucial role in serving the model to the end-user and helps in the continuous evaluation of the model's performance. This project is based on an initial assumption that a Machine learning model has been built using python, and a user interface has been created for the model using Flask or Streamlit.

Project description:
In this project, an already built machine learning model has been utilized to demonstrate model deployment in the Google Cloud Platform. The model used for deployment is trained during the development of the Machine Learning Project to Forecast Rossman Sales.
Rossmann operates over 3,000 drug stores in 7 European countries. The goal is to predict their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. Click on this link to know about the project in detail.

Aim:
To deploy a machine learning model to the cloud (Google Cloud Platform) using both traditional approach and containerized approach

Approach:
Generally, there are two ways in which a machine learning model can be deployed into the cloud environment. They are


Traditional Approach


Dockerized Approach


In the traditional approach, we usually rent a server from the cloud, create an environment on the server, push the interface that we have built using Flask / Streamlit to that server and finally expose the necessary components for model deployment. Whereas in the dockerized approach, we generally package all the model code and create a docker image containing the app's configurations."
129,MLOps Project on GCP using Kubeflow for Model Deployment,"Business Overview
Kubernetes, also known as K8s, is an open-source system for automating deployment, maintenance, and scaling containerized applications. Whereas to make the deployments of Machine learning workflows on Kubernetes simple, portable and scalable Kubeflow is used by many of the developers. However, many industrialists still find it difficult to understand how to design, build and connect the components in order to create the kubeflow pipeline.
In this project, we give a brief introduction to kubeflow and other Google Cloud services used. The project mainly focuses on developing and deploying the model developed in Deep Learning Project for Text Detection in Images using Python.&nbsp;

Aim
To develop and deploy a deep learning model on Google Cloud Platform using Kubeflow

Tech Stack
➔ Language: Python
➔ Libraries: tqdm, torch, opencv, albumentations, numpy, pandas, kfp, sklearn
➔ Services: Flask, Docker, GCP, Kubernetes, Kubeflow&nbsp;

Prerequisites
It is advisable to have a basic knowledge of the following services to better understand the project.


Flask


Docker


Cloud Build


Cloud Run


Cloud Source Repository


Basics of Kubernetes


Deep Learning Project for Text Detection in Images using Python

"
130,AWS Snowflake Data Pipeline Example using Kinesis and Airflow,"Business Overview
Snowflake's Data Cloud is based on a cutting-edge data platform delivered as a service (SaaS). Snowflake provides data storage, processing, and analytic solutions that are quicker, easier to use, and more versatile than traditional options.
Snowflake isn't based on any current database technology or large data software platforms like Hadoop. Snowflake, on the other hand, combines a brand-new SQL query engine with cutting-edge cloud architecture. Snowflake gives users all the features and capabilities of an enterprise analytic database, plus a lot more.


There is no hardware to choose, install, configure, or manage (virtual or actual).


There isn't much to install, set up, or maintain in terms of software.


Snowflake oversees ongoing maintenance, administration, updates, and tweaking.


Snowflake is entirely based on cloud infrastructure. Except for optional command-line clients, drivers, and connectors, all components of Snowflake's service operate on public cloud infrastructures.
Snowflake's computing demands are met by virtual compute instances, and data is stored permanently via a storage service. Snowflake isn't compatible with private cloud environments (on-premises or hosted).
The architecture of Snowflake is a mix of classic shared-disk and shared-nothing database designs. Snowflake employs a central data repository for persistent data, like shared-disk architectures, available from all compute nodes in the platform. Snowflake executes queries utilizing MPP (massively parallel processing) compute clusters, wherein each node in the cluster stores a part of the whole data set locally, like shared-nothing architectures. This method combines the simplicity of a shared-disk design with the speed and scale-out advantages of a shared-nothing architecture.
In this project, we will create a data pipeline starting from the EC2 logs to storage in Snowflake and S3 post-transformation and processing through Airflow DAGs. This is the second project in the Snowflake project series, the first project being an introduction to different Snowflake components and their uses.
&nbsp;
Dataset used
Two CSV files are used for project implementation with various fields-


customers.csv


orders.csv


&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process. In this project, two streams of data are added to Snowflake and S3 processed stage through Airflow DAG processing and transformation- customers data and orders data.
&nbsp;
Tech Stack:
➔ Languages: SQL, Python3
➔ Services: Amazon S3, Snowflake, Amazon MWAA, Amazon Kinesis Firehose, AWS EC2
&nbsp;
Amazon MWAA
Amazon Managed Workflows for Apache Airflow (MWAA) is a managed orchestration solution for Apache Airflow1 that makes setting up and operating end-to-end data pipelines in the cloud at a scale much easier. Apache Airflow is an open-source application for authoring, scheduling, and monitoring process and task sequences known as ""workflows"" in a programmatically controlled manner. Using Managed Workflows, you can develop workflows with Airflow and Python without having to worry about scalability, availability, or security of the underlying infrastructure.
&nbsp;
AWS EC2
A virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture is known as an Amazon EC2 instance. The Amazon Elastic Compute Cloud (EC2) service allows corporate customers to run application applications in a computer environment. Using Amazon EC2 eliminates the need to invest in hardware up front, allowing users to create and deploy apps quickly. Amazon EC2 allows users to launch as many or as few virtual servers as they want, set security and networking, and manage storage.
&nbsp;
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
&nbsp;
Amazon Kinesis Firehose
Amazon Kinesis Data Firehose is a fully managed service that delivers real-time streaming data to Amazon S3, Amazon Redshift, and other destinations. Along with Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics, Kinesis Data Firehose is part of the Kinesis streaming data platform. The user does not need to create apps or manage resources while using Kinesis Data Firehose. Simply set up the data producers to transmit data to Kinesis Data Firehose, and the data will be sent to the designated destination automatically. The data can also be transformed before being delivered using Kinesis Data Firehose.
&nbsp;
Architecture&nbsp;

&nbsp;"
131,GCP Project-Build Pipeline using Dataflow Apache Beam Python,"Business Overview
Google Cloud is a collection of physical assets, such as computers and hard disk drives, and virtual resources, such as virtual machines (VMs), housed in Google data centers worldwide. This resource distribution has various advantages, including redundancy in a failure and decreased latency by putting resources closer to customers. This release also presents some guidelines for combining resources.
GCP offers a web-based graphical user interface for managing Google Cloud projects and resources. If a user prefers to work at the command line, the G-Cloud command-line tool can handle most Google Cloud activities.
This is the third project in the GCP Roadmap project series, the previous projects utilize services such as PubSub, Compute Engine, Cloud Storage, and BigQuery. In this project, we will explore GCP Dataflow with Apache Beam:
&nbsp;
Tech Stack
➔ Language: Python3
➔ Services: Cloud Storage, Dataflow, Apache Beam, BigQuery, G-Cloud SDK, Pub/Sub
&nbsp;
Cloud Storage
Cloud Storage is a service that allows users to store their data on the Google Cloud. An object is an immutable piece of data that consists of a file in any format. Objects can be stored in containers known as buckets. All buckets are related to a project, and the user may organize their projects into organizations. After starting a project, users may create Cloud Storage buckets, upload things to the buckets, and get objects. Users can also give rights to make data accessible to certain domains or for specific use cases such as establishing a website.
&nbsp;
BigQuery
Google Bigquery is a Cloud Datawarehouse powered by Google, which is Serverless, highly scalable, and cost-effectively designed for making data-driven business decisions quickly. It offers both the batch and&nbsp;streaming insertion capabilities and is integrated with Tensorflow as well to perform machine learning using SQL like dialects.
&nbsp;
Pub/Sub
Pub/Sub allows services to interact asynchronously and is used in streaming analytics and data integration pipelines to ingest and disseminate data. It, like Kafka, allows users to design systems of event producers and consumers, known as publishers and subscribers. Publishers connect with subscribers asynchronously by disseminating events rather than synchronous remote procedure calls (RPCs). Publishers transmit events to the Pub/Sub service without considering how or when these events will be handled. Pub/Sub then sends events to any services that need to respond to them. This type of asynchronous integration improves the overall flexibility and robustness of the system.
&nbsp;
Apache Beam
Apache Beam is a batch and streaming data processing unified programming model. It offers many APIs for interacting with various data sources and processing data using various backends, such as Spark or Dataflow. As a result, the data may be stored elsewhere, and computation can be performed on it in a serverless manner or on a specified backend.
&nbsp;
Dataflow
Google Cloud Dataflow is a cloud-based data processing service that can handle batch and real-time data streaming. It allows users to build processing pipelines for integrating, preparing and analyzing massive data sets, which is typical of big data processing.
&nbsp;
Approach


Read JSON encoded messages from the GCS file, transforms the message data, and write the results to BigQuery.


Read JSON encoded messages from Pub/Sub, transforms the message data, and write the results to BigQuery

"
132,Azure Text Analytics for Medical Search Engine Deployment,"Business Context:
 We all must have wondered that if we search for a particular word in google, it does not show just the results that contain the very same word but also shows results that are very closely related to it. For example, if we search for the term &lsquo;medicine&rsquo; in google, you can see results that not just include the word &lsquo;medicine&rsquo; but also terms such as &lsquo;health,&rsquo; &lsquo;pharmacy&rsquo;, &lsquo;WHO&rsquo; and so on. So, google somehow understands that these terms are closely related to each other. This is where word embeddings come into the picture. Word embeddings are nothing but numerical representations of words in a sentence depending on the context. In this project, we will be learning how word embeddings work and how can we build a smart search engine, particularly for medical science using word embeddings.&nbsp;

Aim :
To develop a machine learning application that can understand the relationship and pattern between various words used together in the field of medical science, create a smart search engine for records containing those terms, and finally build a machine learning pipeline in azure to deploy and scale the application.

Tech Stack:


Language: Python


Packages: NLTK, Scikit-Learn, Pandas, Numpy, Streamlit


Cloud: Azure, Azure Data Factory, Azure Blob Storage, Azure Databricks


Code Management: Git, Github, Docker, Dockerhub



Prerequisites:


Word2Vec and FastText Word Embedding with Gensim in Python

"
133,GCP Project to Learn using BigQuery for Exploring Data,"Business Overview
Google Cloud is a collection of physical assets, such as computers and hard disk drives, and virtual resources, such as virtual machines (VMs), housed in Google data centers worldwide. This resource distribution has various advantages, including redundancy in a failure and decreased latency by putting resources closer to customers. This release also presents some guidelines for combining resources.
GCP offers a web-based graphical user interface for managing Google Cloud projects and resources. If a user prefers to work at the command line, the G-Cloud command-line tool can handle most Google Cloud activities.
This is the second project in the GCP Roadmap project series, the first project utilizes services such as Pub/Sub, Compute Engine, and Cloud storage. In this project, we will explore GCP BigQuery.
&nbsp;
Tech Stack
➔ Language: Python3
➔ Services: Cloud Storage, BigQuery, G-Cloud SDK
&nbsp;
Cloud Storage
Cloud Storage is a service that allows users to store their data on the Google Cloud. An object is an immutable piece of data that consists of a file in any format. Objects can be stored in containers known as buckets. All buckets are related to a project, and the user may organize their projects into organizations. After starting a project, users may create Cloud Storage buckets, upload things to the buckets, and get objects. Users can also give rights to make data accessible to certain domains or for specific use cases such as establishing a website.
&nbsp;
BigQuery
Google BigQuery is a Cloud Datawarehouse powered by Google, which is serverless, highly scalable, and cost-effectively designed for making data-driven&nbsp;business decisions quickly. It offers both the batch and streaming insertion capabilities and is integrated with Tensorflow as well to perform machine learning using SQL-like dialects."
134,AWS MLOps Project for Gaussian Process Time Series Modeling,"Business Objective 
Deployment is the method by which you integrate a machine learning model into an existing production environment to make practical business decisions based on data.
MLOps is a means of continuous delivery and deployment of a machine learning model.&nbsp;&nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management.&nbsp;
In this project, we aim to create the MLOps project for the time series gaussian model Gaussian Model in Time Series using Python on the AWS cloud platform (Amazon Web Services) that is cost-optimized. Cost-optimized model deployment means that we will be using minimal services.&nbsp;
&nbsp;
Aim
To create an MLOps project using the Amazon Web Services (AWS) platform to deploy a time series gaussian model in production.
&nbsp;
Tech stack 

Language - Python
Libraries - Flask, pickle
Services - Flask, AWS, Docker, Lightsail, EC2

&nbsp;
Approach 

Model creation




Save the model in a pickle format(.pkl)




Flask app




Create a flask file




EC2 machine creation




Create an instance on the AWS management console
Launch the instance
Install the &lsquo;putty&rsquo; tool on your local for login




EC2 and Docker setup




Refer to the steps present in the &lsquo;install-docker.sh&rsquo; file




AWS CLI installation




Refer to the steps from &lsquo;install-aws-cli.sh&rsquo; file




Lightsail installation




Refer to the steps from &lsquo;install-lightsail-cli.sh&rsquo;




Upload files into the EC2 machine

Method1 



Upload the code file in zip format on AWS console (cloud shell)



Method 2 



Create an S3 storage bucket
Copy the object URL, get the URL on EC2 machine
Unzip the bitbucket folder




Deployment




Follow the order of installation, from &lsquo;lightsail-deployment.md&rsquo; file


"
135,AWS MLOps Project for ARCH and GARCH Time Series Models,"Business Objective 
Deployment is the method by which you integrate a machine learning model into an existing production environment to make practical business decisions based on data.
MLOps is a means of continuous delivery and deployment of a machine learning model.&nbsp;&nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management.&nbsp;
In this project, we aim to create the MLOps project for the time series arch model Build ARCH and GARCH Models in Time Series using Python on the AWS cloud platform (Amazon Web Services) that is cost-optimized. Cost-optimized model deployment means that we will be using minimal services.&nbsp;
&nbsp;
Aim
To create an MLOps project using the Amazon Web Services (AWS) platform to deploy the time series arch model in production.
&nbsp;
Tech stack 

Language - Python
Libraries - Flask, pickle
Services - Flask, AWS, Docker, Lightsail, EC2

&nbsp;
Approach 

Model creation




Save the model in a pickle format(.pkl)




Flask app




Create a flask file




EC2 machine creation




Create an instance on the AWS management console
Launch the instance
Install the &lsquo;putty&rsquo; tool on your local for login




EC2 and Docker setup




Refer to the steps present in the &lsquo;install-docker.sh&rsquo; file




AWS CLI installation




Refer to the steps from &lsquo;install-aws-cli.sh&rsquo; file




Lightsail installation




Refer to the steps from &lsquo;install-lightsail-cli.sh&rsquo;




Upload files into the EC2 machine

Method1



Upload the code file in zip format on AWS console (cloud shell)



Method 2



Create an S3 storage bucket
Copy the object URL, get the URL on ec2 machine
Unzip the bitbucket folder




Deployment




Follow the order of installation, from &lsquo;lightsail-deployment.md&rsquo; file


"
136,ML Model Deployment on AWS for Customer Churn Prediction,"Business Overview
Churn rate, also known as turnover or customer churn, is the rate at which a customer suspends a transaction with a company. Most commonly, it is expressed as the percentage of service subscribers who canceled their subscription within a certain time period. In this project, We aim to deploy a model which predicts whether the customer is going to churn in the near future or not. Amazon Web Services(AWS) is used as a cloud provider. We would advise you to have a basic understanding of Customer Churn Prediction Analysis using Ensemble Techniques before jumping into this project.&nbsp;

Aim
To deploy a model on AWS which predicts whether the customer is going to churn in the near future or not.

Tech Stack
➔ Language: Python
➔ Libraries: Flask, gunicorn
➔ Services: Flask, Docker, AWS, Gunicorn, Terraform

Prerequisites
It is advisable to have a basic knowledge of the following services to get an understanding of the project.


Flask


Terraform


AWS s3


Aws ECR


AWS ECS


AWS EC2 Load balancer


AWS Code commit


AWS Code Build


AWS Code Deploy


AWS Code Pipeline


&nbsp;"
137,End-to-End Speech Emotion Recognition Project using ANN,"Business Objective
Emotions are incredibly vital in the mental existence of humans. It is a means of communicating one's point of view or emotional state to others. Humans can sense the emotional&nbsp;state&nbsp;of one another through their sensory organs. Whereas doing the same for a computer is not an easy task. Although computers can quickly comprehend content-based information, obtaining the depth underlying content is challenging, which is what speech emotion recognition aims to accomplish.&nbsp;&nbsp;
The extraction of the speaker's emotional state from his or her speech signal is known as Speech Emotion Recognition (SER). There are a few universal emotions that any intelligent system with finite processing resources can be trained to recognize or synthesize as needed. These include neutral, calm, fear, anger, happiness, sad, etc. Speech emotion recognition is being more widely used in areas such as&nbsp;medical, entertainment, and education.
In this project, we will build a model that will be able to recognize emotions from sound files with the help of Keras and TensorFlow libraries. We will also build a model using an MLP from the sklearn library.&nbsp;
We have used TrueFoundry's ML Monitoring and Experiment tracking Solution known as MLFoundry to log the experiments, models, metrics, data &amp; features which can be used to generate informative dashboards and insights.&nbsp;
&nbsp;
&nbsp;
Data Description
The dataset in use is the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). It contains a total of 7356 files. Two lexically-matched statements are vocalized in a neutral North American accent by 24 professional actors (12 female, 12 male) in the database. Calm, happy, sad, angry, afraid, surprise, and disgust expressions can be found in speech, whereas calm, happy, sad, angry, and fearful emotions can be found in song. Each expression has two emotional intensity levels (normal and strong), as well as a neutral expression. All three modalities are available: audio-only (16bit, 48kHz.wav), audio-video (720p H.264, AAC 48 kHz, .mp4), and video-only (720p H.264, AAC 48 kHz, .mp4) (no sound).&nbsp;
&nbsp;
&nbsp;
Aim
To build an artificial neural network model that is able to correctly classify speech audio files into different emotions such as happy, sad, anger, neutral, etc.
&nbsp;
&nbsp;
Tech Stack


Language - Python


Libraries &ndash; Keras, TensorFlow, librosa, soundfile, sklearn, pandas, matplotlib, NumPy, pickle, mlfoundry


&nbsp;
Approach


Importing the required libraries and packages


Open the config.ini file. (This is a configuration file that can be edited according to your dataset)&nbsp;


Read the dataset (audio files)


Visualize the structure of a sound file


Understand the features of a sound file


Extract features


Load the entire data


Train-Test Split


Model Training






Using Keras and Tensorflow


Using MLP from scikit-learn






Hyperparameter optimization


Code modularization for production

Creating informative dashboards with MLFoundry
"
138,Tensorflow Transfer Learning Model for Image Classification,"Business Objective 
Image classification - Image classification refers to a process in computer vision that can classify an image according to its visual content. Image classification applications include&nbsp;recognizing various objects, such as vehicles, people, moving objects, etc., on the road to enable autonomous driving.
Transfer Learning - The reuse of a pre-trained model on a new problem is known as transfer learning in machine learning. A machine uses the knowledge learned from a prior assignment to increase prediction about a new task in transfer learning.
In this project, we will be dealing with a binary classification dataset. A model is to be built that can correctly classify a t-shirt image into plain or topographic. We will be building transfer learning models like InceptionV3 and Resnet.&nbsp;
&nbsp;
Data Description
We have a dataset of t-shirt images categorized into 2 types, namely plain(solid) and topography (contains text)
The training, testing and validation folders contain two subfolders (plain and topography) each.

Training - around 600 images
Testing - around 100 images
Validation - around 150 images

&nbsp;
Aim
To build a transfer learning model that can perform binary classification on the given dataset of images.
&nbsp;
Tech stack 

Language - Python
Libraries - pandas, numpy, seaborn, matplotlib, sklearn, tensorflow

&nbsp;
Approach 

Importing the required libraries.
Load and read the data images
Data Visualization
Model Training




Create a function for initiating Inception and Resnet models
Create a function that defines the optimizers
Add layers to the pretrained model
Function to compile the model
Fit and train the model
Plot the results
Save the model (.h5 file)




Create train and validation image data generators&nbsp;
Model Testing




Create testing images data generator
Load the saved model
Perform predictions by plotting the confusion matrix
Analysing the incorrect predictions


"
139,Detectron2 Object Detection and Segmentation Example Python,"Business Objective
&nbsp;
Antibiograms are often used by clinicians to assess local susceptibility rates to select empiric antibiotic therapy, and monitor resistance trends over time. The testing occurs in a&nbsp;medical laboratory&nbsp;and uses&nbsp;culture&nbsp;methods that expose bacteria to antibiotics to test if bacteria have genes that confer resistance. Culture methods often involve measuring the diameter of areas without bacterial growth, called zones of inhibition. The zone of inhibition is a circular area around the antibiotic spot in which the bacteria colonies do not grow. The zone of inhibition can be used to measure the susceptibility of the bacteria to the antibiotic. The process of locating the zones and related inhibitions can be automated using image detection and segmentation.
&nbsp;
Image Classification helps us classify what is contained in an image, whereas object Detection specifies the location of multiple objects in the image. On the other hand, image Segmentation will create a pixel-wise mask of each object in the images, which helps identify the shapes of different objects in the image.
&nbsp;
We have dealt with image classification in the project, Build a Multi Class Image Classification Model Python using CNN. This project will perform image detection and segmentation on a given set of images to detect the zones and inhibition of the bacteria present in a collection of images using the Detectron2 model.
&nbsp;
Data Description
&nbsp;
The dataset contains several antibiogram test images divided into training and testing folders. There are around 50 images in the training dataset and five in the testing.
&nbsp;
Aim
&nbsp;
To build a Dectectron2 model that can detect zones and inhibitions in a given image.
&nbsp;
Tech stack
&nbsp;

Language - Python
Libraries - torch, detectron2, cv2, matplotlib, numpy

&nbsp;
Approach
&nbsp;

Import Detectron2 using the git command
Importing the required libraries.
Setup the input, output, train, test and annotations.json path
Register the coco instances with register name and annotations and form a dataset with configurations (in metadatacatalog)
Data Visualization
Training the Dectectron2 model




Define a class for coco Trainer
Get the default configurations from cfg and add the necessary parameters
Load the pre-trained model weights.
Perform training on the data.




Inference model




Set the default config file and add the required parameters
Define an inference image
Load the weights of trained model
Define default predictor object
Make the predictions and visualize the results.
Interpret the generated results
Plot the bounding boxes and masks in a given image


"
140,Loan Eligibility Prediction Project using Machine learning on GCP,"Business Objective:
The primary source of profit for the banking sector is from loan interests. Hence, Banks invest a lot of time and money in analyzing a loan applicant&rsquo;s profile and their repaying capacity before granting them a loan. Going through the profile of every applicant manually and assessing each applicant&rsquo;s repaying capacity is a highly tedious job, and this is why automating this process is necessary.
To build such a predictive model, we will need a data set, also known as training data.&nbsp; In today&rsquo;s world, where data can be stored in various database structures, MySQL, a fully-featured relational database, is a widely used management system.&nbsp;
In this project, we will build an automated machine learning model to predict the repaying capacity of applicants by using MySQL as our database. The Google Cloud Platform(GCP) is used as the cloud platform to create a jupyter lab for model training.&nbsp;

Aim :
To build a predictive model with MySQL as a data store to predict if an applicant is capable of repaying the loan or not.

Tech Stack:


Language: Python


Packages: Pandas, Numpy, Scikit-Learn, XGBoost


Data Management: MySQL, DBMS, SQL queries


Cloud: GCP



Approach:
Step 1: Create a virtual machine instance in the google cloud platform
Step 2: Install Anaconda, Python, and MySQL into the virtual machine
Step 3: Configure Jupyter notebook in the Virtual machine instance
Step 4: Read training and test data from MySQL Server
Step 5: Perform data cleaning steps
Step 6: Perform Exploratory data analysis
Step 7: Build various tree-based models
Step 8: Evaluate the performance of each model using various performance metrics
Step 9: Perform predictions on the best model"
141,Linear Regression Model Project in Python for Beginners Part 1,"Overview
Regression is one of the foundational techniques in Machine Learning. Being one of the most well-understood algorithms, beginners always struggle to understand some fundamental terminology related to regression. In this series of projects, we try to give you basic ideas of underlying concepts with the help of practical examples. If you are starting your career or want to brush up on your knowledge of regression, this course is made up for you. This project begins by introducing some simple real-life examples for regression. From a brief introduction to most of the concepts used in regression to hands-on experience, this project will give you enough understanding to apply those in real-world problems. With the help of the background developed, you will code your regression model in python.
Aim
To give a gentle introduction to the fundamentals of regression and build a simple linear regression model in python.
Data Description
The dataset used is the soccer player dataset. It has information about various players from different clubs, and it provides data over ten features with a number of goals as the target variable.
Tech Stack


Language: Python


Libraries:&nbsp; pandas, statsmodel, seaborn, matplotlib, sklearn, scipy


Approach
This project starts with a real-life example for regression analysis, with an introduction to simple and multiple linear regression. Building the statistical foundation for the regression, it gives you a brief idea of the formula of regression. With this background, the first regression model in python is built. Going through the interpolation and extrapolation also explains errors in regression and Lurking variables. The point estimators of mean and variance and distributions of underlying parameters are also discussed. The coefficient of determination is also known, and R squared is briefly explained. The project ends with diagnostics and remedial measures for regression with a practical explanation.&nbsp;"
142,Isolation Forest Model and LOF for Anomaly Detection in Python,"Business Objective
In the Banking or payment industry, fraud is an illegal usage of credit card details without the real cardholder&rsquo;s knowledge. A stolen credit card/card number is usually the cause of a fraudulent charge. Once a cardholder sees a payment transaction, he did not make on his credit card statement, he/she has the right to dispute the charge by contacting his/her bank. The bank or Credit Card Company conducts an investigation and returns the money to the cardholder.
But what if we can detect fraudulent transaction activity in real-time? This way we can take the required action to stop the same. We can use machine learning techniques to detect fraudulent transactions. We can use supervised or unsupervised methods of learning depending upon the dataset. For this project, we will be opting for unsupervised learning using Isolation Forest and Local Outlier Factor (LOF) algorithms.
Isolation Forests are similar to Random forests that are built based on decision trees. There are no pre-defined labels here and hence it is an unsupervised algorithm. It was built based on the fact that anomalies or outliers are the data points that are &ldquo;few and different.&rdquo; In an Isolation Forest, randomly sub-sampled data is processed in a tree structure based on randomly selected features. The sub-samples that travel deeper into the tree are less likely to be anomalies as they required more cuts to isolate them. The ones which ended up in shorter branches indicated anomalies as it was easier for the tree to separate them from other observations.
Local Outlier Factor or LOF is an unsupervised ML algorithm that identifies outliers with respect to the local neighborhoods as opposed to using the entire data distribution. The advantage of using a LOF is identifying points that are outliers relative to a local cluster of points. This algorithm is based on a concept of local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density.
&nbsp;
Data Description
The dataset that we will use for this project consists of 15 numerical columns with 140000 rows which are a result of PCA transformation. These numerical values are nothing but masked credit card transactions. We do not have any background information on the features due to confidentiality reasons.
&nbsp;
Aim
To build a model that is able to correctly identify fraudulent credit card transactions from the valid transactions using Isolation Forest and Local Outlier Factor.
&nbsp;
Tech Stack

Language - Python
Libraries - sklearn, pandas, matplotlib, numpy, seaborn

Approach

Importing the required libraries and packages
Open the config.ini file. (This is a configuration file that can be edited according to your dataset)
Read the dataset (audio files)
Perform exploratory data analysis
Handle Missing Values
Find contamination amount
Model Training
Making predictions
"
143,Deep Learning Project for Time Series Forecasting in Python,"
Business Objective
Deep Learning has become a fundamental part of the new generation of Time Series Forecasting models, obtaining excellent results While in classical Machine Learning models - such as autoregressive models (AR) or exponential smoothing - feature engineering is performed manually and often some parameters are optimized also considering the domain knowledge, Deep Learning models learn features and dynamics only and directly from the data. Thanks to this, they speed up the process of data preparation and are able to learn more complex data patterns in a more complete way.
Till now, in this sequence of time-series projects, we have covered the machine learning topics such as Autoregression modelling, Moving Average Smoothing techniques, ARIMA model, Multiple linear regression, Gaussian process, and ARCH- GARCH models.
In this project, we will demonstrate how deep learning in time series can be used. There are four major models which will be built,

Multi-Layer Perceptron (MLP)
Convolutional Neural Network (CNN)
Long Short-Term Memory (LSTM)
Hybrid CNN &ndash; LSTM

(This is the seventh project of our time series list of projects; you can refer to the previous project through this link:&nbsp;Build ARCH and GARCH Models in Time Series using Python)
Data Description
The dataset is &ldquo;Call-centres&rdquo; data. This data is at month level wherein the calls are segregated at domain level as the call centre operates for various domains. There are also external regressors like no of channels and no of phone lines which essentially indicate the traffic prediction of the inhouse analyst and the resources available.
The total number of rows are 132 and number of columns are 8:

Month, healthcare, telecom, banking, technology, insurance, no of phonelines and no of channels.

Aim
We aim to build four deep learning models such as MLP, CNN, LSTM, and a hybrid CNN-LSTM model on the given time series dataset.

Tech stack

Language - Python
Libraries - pandas, numpy, matplotlib, TensorFlow

Approach

Import the required libraries and read the dataset
Perform descriptive analysis
Data pre-processing

Setting date as Index
Setting frequency as month


Exploratory Data Analysis (EDA) -

Data Visualization


Set the training format

Reshape the input data


Perform train-test split
MLP model

Define learning rate, number of epochs, and optimizer.
Build a sequential model with dense layers
Fit and train the model
Make predictions on the test data
Plot the results


CNN model

Reshape the data in three dimensions
Define learning rate, number of epochs, and optimizer.
Build a sequential model with convolution, max-pooling layers
Fit and train the model
Make predictions on the test data
Plot the results


LSTM model

Define learning rate, number of epochs, and optimizer.
Build a sequential model with LSTM layer and dense layers
Fit and train the model
Make predictions on the test data
Plot the results


CNN-LSTM model

Reshape the data in four dimensions
Define learning rate, number of epochs, and optimizer.
Build a sequential model
Fit the model
Make predictions on the test data
Plot the results


"
144,Recommender System Machine Learning Project for Beginners-3,"Business Overview&nbsp;
Imagine you want to buy a gift for your dear ones. You opened an eCommerce website on your phone and started searching for the gift. After a few minutes, you find a section which shows very similar gifts you wanted. Now you must be thinking, how do they know similar items based on one thing? Most platforms use the Recommendation system at the backend to show you the best items based on your search history. There are several choices in the digital market world, which are somehow too overwhelming for the user to choose from. There is always a need to filter and prioritize the relevant items for each user to engage the customers with the platform, which eventually impacts the revenue. Content-based filtering is one of the most used techniques to personalize the content for each user. It uses previous actions and feedback about users' liking to provide them with similar recommendations.&nbsp;
&nbsp;
As a part of a series of Recommender system projects, this project covers Recommendations using a wide variety of Content-Based Filtering algorithms in Python. It also demonstrates one of the developed algorithms using the Streamlit app framework. If you haven&rsquo;t already visited, here is the previous project of the series Recommender System Machine Learning Project for Beginners-2

Aim
To build a Recommender System using various content-based filtering techniques and similarity measures and to create a web application for the same using Streamlit

Data Description&nbsp;
The dataset contains about 4k products in various categories for a UK-based non-store online retail business. The company mainly sells unique all-occasion gifts with maximum wholesaler customers.&nbsp;

Tech Stack


Language: Python


Libraries:&nbsp; pandas, NumPy, seaborn, matplotlib, re, gensim



Approach


Data Description


Data Preprocessing


Ranking Functions


Building Recommendation system


Count Vectorizer


TFIDF Vectorizer


Word2Vec


FastText


Glove


Co-occurrence Matrix




Streamlit Web App 

"
145,"Streaming Data Pipeline using Spark, HBase and Phoenix","Business objective
Sensor networks clearly focus on a wide range of practice-oriented research as individual sensors and related networks for structural monitoring and highly efficient monitoring. The sensor network is designed to respond to emergencies in real-time while observing the radiation field. An intelligent system-based sensor network is presented and used to monitor the condition of remote wells. The network consists of three sensors used to collect oil field data, including temperature, pressure, and gas. Intelligent microprocessor sensors are generally designed for oil well data processing, critical failure alarms or signals, traditional data storage or signals, and data/status connections.&nbsp;
&nbsp;
Aim
To build an application that monitors oil wells. Sensors in oil rigs generate streaming data processed by Spark and stored in HBase for use by various analytical and reporting tools.
&nbsp;
Streaming data
Streaming data is continuously generated by thousands of data sources and typically sends datasets in small sizes (in the order of kilobytes) at the same time. Streaming data includes customer-generated log files using mobile or web applications, e-commerce purchases, in-game player activity, social networks, information from financial exchanges or geospatial services.
&nbsp;
Apache Phoenix
Apache Phoenix is ​​a massively parallel relational open-source database engine that uses Apache HBase as backing storage to support OLTP for Hadoop. Phoenix provides a JDBC driver that hides the complexity of NoSQL memory and allows users to create, delete, and modify SQL tables, views, indexes, and sequences. It allows inserting and deleting rows at once, one at a time, and querying the data via SQL. Phoenix compiles queries and other statements into the native NoSQL storage API instead of MapReduce. This allows you to build low latency applications on NoSQL storage.
&nbsp;
HBase
HBase is a non-relational distributed open-source database written in Java. It was developed as part of the Apache Software Foundation's Apache Hadoop project and ran on HDFS (Hadoop Distributed File System), providing Hadoop with Bigtable-like functionality. HBase provides column-by-column compression, in-memory operations, and Bloom filters, as described in the original Bigtable paper. HBase tables serve as input and output for MapReduce jobs running in Hadoop and can be accessed via the Java API, but can also be accessed via REST, Avro, or the Thrift Gateway API.
&nbsp;
Tech Stack


AWS EC2


Docker


Scala


HBase


Apache Spark SQL


Spark Structured Streaming


HDFS


Apache Phoenix


SBT


&nbsp;
Approach


Create an AWS EC2 instance and launch it.


Create docker images using docker-compose file on EC2 machine via ssh.


Download the dataset and load it into HDFS storage.


Read data from HDFS storage and write into HBase table using Spark.


Create Phoenix view on top of HBase table to analyze data using SQL queries.


&nbsp;
Architecture
&nbsp;
"
146,Build a Graph Based Recommendation System in Python -Part 1,"Business Objective:
There are hundreds of eCommerce websites with millions of products listed. Personalizing the content is much needed to engage the user with the platform. This is where recommendation systems come into the picture. You must have heard about some recommendation systems such as Content-Based, Collaborative filtering, etc. In recent years Graph, Learning-based Recommendation systems have witnessed fast development.This project aims to give you a brief idea about recommendation systems and how they work. Moreover, we build a recommendation system using Graph-based learning for an eCommerce platform.
Aim:
To build a Graph-based recommender system that will recommend the best product for the users in e-commerce platforms depending on their purchase and search history
Data Overview :
The dataset contains user information over 9 attributes for an eCommerce website
Tech Stack:


Language: Python


Packages: DuckDB, pandas, Numpy


File Management: Parquet


Database Management: SQL, SQL querying


Approach:


Understand the problem statement


Extract appropriate data


Understanding user journey on an eCommerce platform.&nbsp;


Introducing the concept of graphs to build the recommendation engine


Understanding graphs and their types


Predict using the Recommender engine

"
147,SQL Project for Data Analysis using Oracle Database-Part 1,"What is Dataset Analysis?
Dataset Analysis is defined as manipulating or processing unstructured data or raw data to draw valuable insights and conclusions that will help derive critical decisions that will add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.
&nbsp;
Data Pipeline:&nbsp;
It refers to a system for moving data from one system to another. The data may or may not be transformed, and it may be processed in real-time (or streaming) instead of batches. A data pipeline is extracting or capturing data using various tools, storing raw data, cleaning, validating data, transforming data into a query-worthy format, visualization of KPIs including Orchestration of the above process is data pipeline.
&nbsp;
What is the Agenda of the project?
The project&rsquo;s Agenda involves Analyzing the data using SQL on the Oracle Database Software. We first download the Oracle Database 21c edition from the official Oracle website and understand the problem. Then a Password is set up for the &ldquo;SYSTEM&rdquo; username during the setup of Oracle Database 21c software. Then SQL Plus is used to connect to Oracle Database using the same credentials of the Oracle Database 21c.&nbsp;
Further, a Database connection is established from SQL Developer to the Oracle Database using the &ldquo;SYSTEM&rdquo; username and the password. Then tables are created in the database followed by data insertion into tables and exploration, i.e., noticing relationships between tables, walking through the columns, and seeing comments. Records are displayed in an ordered manner, handling NULL values, and selecting records based on patterns like Wildcard, Operators, etc. Then working on Data Manipulation commands(DML) for Data Analysis. Then taking Backup of the Table where migration is going on and use COMMIT and ROLLBACK commands. Then listing down distinct records, further renaming columns, and finally, listing down employee details based on the complex nested conditions.
&nbsp;"
148,AWS MLOps Project to Deploy a Classification Model [Banking],"Business Objective
MLOps is a means of continuous delivery and deployment of a machine learning model.&nbsp;&nbsp;Practicing MLOps means that you advocate for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management.
In this project, we will be deploying the machine learning application for the Build Classification Algorithms for Digital Transformation [Banking]. Hence it is advised to go through this project first before. This project uses Amazon EKS (cloud platform), Amazon EC2, Elastic Load Balancing, etc services. Amazon EKS is a fully managed service that makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS.
&nbsp;
Aim
Deploy a machine learning model to identify the potential borrower customers to support focused marketing and deploy them through a cloud provider (AWS)
&nbsp;
Tech stack

Language - Python
Services - AWS EKS, ECR, Load balancer, code commit, code deploy, code pipeline

&nbsp;
Prerequisites
It is advisable to have a basic knowledge of the following services to get an understanding of the project.

Flask
Aws ECR
AWS ECS
AWS EC2 Load balancer
AWS Code commit
AWS Code Build
AWS Code Deploy
AWS Code Pipeline

&nbsp;
Approach

Cluster creation steps




Create an EKS cluster master node.
Create Node groups in the EKS cluster
Create OIDC connection in the AWS Identity provider with EKS cluster
Install Kubernetes dashboard for monitoring
Install AWS Load balancer controller



&nbsp;

Create an ECR repository for the docker image

&nbsp;

EKS Yamls for our application




Create deployment yaml
create service yaml
create ingress yaml



&nbsp;

Code Pipeline for EKS




Create 2 code commit repositories one for flask ml application and another one for EKS yamls
Create 2 code build projects one for flask ml application and another one for EKS yamls
Create 2 code pipelines one for flask ml application and another one for EKS yamls




In the EKS yaml code pipeline, have both ECR repo and code commit repo as the source

&nbsp;

Push ML application into EKS




Create Flask application for our ML Application
Convert that Flask application into a docker Application
Test the docker application in your local
Push the Flask code to code commit, check the pipeline runs without any issues.
Once the Flask code pipeline ran successfully, check whether it triggers the second EKS yaml code pipeline.
If the second pipeline is triggered, check the API response for the update values.


"
149,Recommender System Machine Learning Project for Beginners-4,"Business Objective
We often ask our friends with the same tastes to recommend products to use, movies to watch, and whatnot. We trust the recommendations of friends whose taste is similar to ours. Collaborative filtering does the same job. Collaborative filtering is primarily focused on finding similarities between users and mutually encouraging user preferences. Collaborative filtering is the predictive process behind the recommendation engine. The recommendation engine analyzes information about users with similar tastes and assesses the target person's likelihood of enjoying something. Collaborative filtering uses various algorithms to filter data from user reviews and provide personalized recommendations to users with similar settings. Collaborative filtering uses interactions and data collected from other users to filter information.&nbsp;
As a part of a series of Recommender system projects, this project covers Recommendations using a wide variety of Collaborative Filtering algorithms in Python. If you haven&rsquo;t already visited, here is the previous project of the series Recommender System Machine Learning Project for Beginners-3

Data Description
This is a transactional data set containing the transactions for a UK-based non-store online retail. The company mainly sells unique all-occasion gifts.&nbsp;

Aim
To build a Recommender system using various Model-Based and Memory-based collaborative filtering techniques

Tech Stack


Language: Python


Libraries:&nbsp; sklearn, surprise, pandas, matplotlib, scipy, numpy, pickle



Approach:


Data Description


Data cleaning


Memory-based approach




User-to-User collaborative recommendation


Item-to-Item collaborative recommendation




Models:




KNN model


Non-negative Matrix Factorization (NMF)


Co-Clustering model




Evaluation metrics:




Root mean square error


Mean absolute error


Cross-validation score


 &nbsp; &nbsp; &nbsp; "
150,Build Portfolio Optimization Machine Learning Models in R,"Business Objective
Portfolio optimization is the process of choosing the optimal portfolio (asset distribution) from a set of all possible portfolios based on some criterion. Typically, the goal is to optimize parameters like expected return while minimizing variables like financial risk. In financial sense of word, the standard deviation of a time series is referred to as Risk or volatility and the difference between two points in a time series is defined as Return. &nbsp;
Optimizing the weights of asset classes to hold and optimizing the weights of assets within the same asset class are two common steps of portfolio optimization. The portfolios we create can be tailored to the preference of any individual. The said preferences are dependent on each person&rsquo;s unique utility function of the form &ndash;
F (U) = &alpha;Returns &ndash; &beta;Risk
We can see that the optimal amount of Risk and Return is proportional to the ratio of &alpha;/.
In this project, we will be creating a portfolio from stocks in the Canadian stock market. Log-returns of adjusted closing values of stocks being traded will be extracted using the Yahoo Finance API.&nbsp; We will select the top-performing Stocks next. Here TSX, that is Canada Stock Market Index, is used as a benchmark. We will develop multiple portfolios, learn their investment growth rate compared to TSX and then choose the optimal portfolio by referring to the CAPM analysis on multiple portfolios and the keymatrix.
&nbsp;
Data Description
We will be extracting the list of companies that currently have their stocks up for trading from Wikipedia and log returns of stocks using Yahoo Finance API.
&nbsp;
Aim
To perform CAPM analysis on stocks and find best suited portfolio using portfolio optimization.
&nbsp;
Tech Stack

Language - R
Libraries &ndash; tidyverse, tidyquant, htmltab, dplyr, rlang

&nbsp;
Approach

Importing the required libraries and packages
Open the config.ini file. (This is a configuration file which can be edited according to your dataset)
Extracting list of stocks currently being traded from Wikipedia
Formatting ticker names so that they can be recognized by Yahoo Finance API
Extracting log returns of stocks and the Index (XLK)
Perform CAPM analysis
Finding top stock percentile
Creating portfolios based on top performing tickers
Creating multiple portfolios using same Tickers as before but multiplied by our desired number of repetitions
Performing CAPM analysis on them
Calculating Key matrix
Deciding suitable portfolio by choosing the portfolio of interest from CAPM analysis and looking it up on the key matrix table
"
151,Build a Customer Churn Prediction Model using Decision Trees,"&nbsp;
Business Objective 
A Decision Tree is a Supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules, and each leaf node represents the outcome.
It is a graphical representation of all possible solutions to a problem/decision based on given conditions. It is called a decision tree because similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure. A decision tree asks a question and based on the answer (Yes/No), it further splits the tree into subtrees.
In our case study, we will be working on a churn dataset. Churned Customers are those who have decided to end their relationship with their existing company.
XYZ is a service-providing company that provides customers with a one-year subscription plan for their product. The company wants to know if the customers will renew the subscription for the coming year or not.
We have already seen how the logistics regression model works on this dataset. It is advised to check this project first, Churn Analysis for Streaming App using Logistic Regression
In this project, we will try to fit the decision tree classifier on the given dataset.
&nbsp;
Data Description&nbsp;
This data provides information about a video streaming service&nbsp;company, where they want to predict if the customer will churn or not. The CSV consists of around 2000 rows and 16 columns
&nbsp;
Aim
Build a decision tree model on the given dataset to determine whether the customer will churn or not.
&nbsp;
Tech stack 

Language - Python
Libraries - NumPy, pandas, matplotlib, sklearn, pickle, imblearn

&nbsp;
Approach 

Importing the required libraries and reading the dataset.
Feature Engineering




Dropping of unwanted columns




Model Building




Performing train test split
Decision tree Model




Model Validation (predictions)




Accuracy score
Confusion matrix
ROC and AUC
Recall score
Precision score
F1-score




Feature Importance




Create a function to find important features
Plot the features


"
152,Build Deep Autoencoders Model for Anomaly Detection in Python,"Objective:
Autoencoders are used to learn compressed representations of raw data with Encoder and decoder as sub-parts. As a part of a series of Deep Learning projects, this project briefs about Autoencoders and its architecture. In this project, we build a deep learning model based on Autoencoders for Anomaly detection and deploy it using Flask. If you haven&rsquo;t already visited, here is the previous project of the series Deep Learning Project for Beginners with Source Code Part 1.

Aim :


To understand the theory behind Autoencoders


To develop a deep learning model based on Autoencoders to learn distributions and relationships between features of normal transactions


To deploy the model using Flask



Data Overview:
The dataset used is a transaction dataset, and it contains information for more than 100K transactions over several features.

Tech Stack:


Language: Python


Packages: Pandas, Numpy, matplotlib, Keras, Tensorflow


API service: Flask, Gunicorn



Approach


Understand business objective


Understand the data using EDA


Normalize and clean the data using Imputation


Understand idea behind Auto-encoders


Build a base auto-encoder model using Keras


Tune the model to extract the best performance


Extract predictions


Serve model as API endpoint using Flask


Perform real-time predictions

"
153,Build an AWS ETL Data Pipeline in Python on YouTube Data,"Business Overview
Many problems exist when deploying or transferring analytics to the cloud. Differences in features between on-premises and cloud data platforms, security, and governance are all technical concerns. The danger of moving on-premises data into the cloud has prompted organizations to limit cloud analytics initiatives, especially in regulated industries where data protection is crucial. Cloud-based safe Data Lake solutions aid in the development of rich analytics on data while classifying it into several storage phases, such as raw, cleansed, and analytical. This project aims to securely manage, streamline, and perform analysis on the structured and semi-structured YouTube videos data based on the video categories and the trending metrics.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
This Kaggle dataset contains statistics (CSV files) on daily popular YouTube videos over the course of many months. There are up to 200 trending videos published every day for many locations. The data for each region is in its own file. The video title, channel title, publication time, tags, views, likes and dislikes, description, and comment count are among the items included in the data. A category_id field, which differs by area, is also included in the JSON file linked to the region.
&nbsp;
Tech Stack:
➔ Languages SQL, Python3
➔ Services AWS S3, AWS Glue, QuickSight, AWS Lambda, AWS Athena, AWS IAM
&nbsp;
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
&nbsp;
AWS IAM
This is nothing but identity and access management which enables us to manage access to AWS services and resources securely. One can create and manage AWS users and groups, use permissions to allow and deny their access to AWS resources. It is a feature of AWS with no additional charge.
&nbsp;
QuickSight&nbsp;
Amazon QuickSight is a scalable, serverless, embeddable, machine learning powered business intelligence (BI) service built for the cloud. It is the first BI service to offer pay-per-session pricing, where you only pay when your users access their dashboards or reports, making it cost-effective for large-scale deployments. It can connect to various sources like Redshift, S3, Dynamo, RDS, files like JSON, text, CSV, TSV, Jira, Salesforce, and on-premises oracle SQL-server.
&nbsp;
AWS Glue&nbsp;
A serverless data integration service makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. Also, you pay storage costs for Data Catalog objects. Tables may be added to the AWS Glue Data Catalog using a crawler. The majority of AWS Glue users employ this strategy. In a single run, a crawler can crawl numerous data repositories. The crawler adds or modifies one or more tables in your Data Catalog after it's finished.
&nbsp;
AWS Lambda
Lambda is a computing service that allows programmers to run code without having to create or manage servers. Lambda executes the code on high-availability computing infrastructure and manages all aspects of it, including server and operating system maintenance, capacity provisioning and automated scaling, code monitoring, and logging. Lambda allows you to run code for almost any form of application or backend service.
&nbsp;
AWS Athena
Athena is an interactive query service for S3 in which there is no need to load data it stays in S3. It is serverless and supports many data formats e.g CSV, JSON, ORC, Parquet, AVRO.
&nbsp;
Architecture

&nbsp;"
154,"Learn Object Tracking (SOT, MOT) using OpenCV and Python","Business Objective 
Object tracking is the task of automatically identifying objects in a video and interpreting them as a set of trajectories with high accuracy. Object tracking is an application of deep learning. It takes an initial set of object detections, creates a unique ID for each of the initial detections, and then tracks each object as they move around frames in a video, maintaining the ID assignment.
Object tracking can be done on two levels, Single Object Tracking (SOT) and Multiple Object Tracking (MOT). In single object tracking, only a single object is tracked even if the environment has multiple objects in it. The object to be tracked is determined by the initialization in the first frame. Whereas in multiple object tracking, all the objects present in the environment are tracked over time. If a detection-based tracker is used, it can also track new objects that emerge in the middle of the video.
&nbsp;
Data Description
In this particular project, we will use various video clips to perform object tracking.
&nbsp;
Aim
To perform single object tracking (SOT) and multiple object tracking (MOT) on a given input video using various algorithms.
&nbsp;
Tech stack 

Language - Python
Libraries - cv2, numpy, os, moviepy

&nbsp;
Approach 

Create a new virtual environment in your terminal
Importing the required libraries.
Single Object Tracking




Define a class for single object tracking
List the required algorithms that need to be implemented
Run the single object tracker object class
Select the desired object in the video and press enter




Multiple Object Tracking




Create a Multiple object tracking class
List the required algorithms that need to be implemented
Run the multiple object tracker object class
Select the desired objects in the video and press enter




Centroid tracking algorithm




Create a centroid algorithm class
Read the input video for face detection
Download the Caffe model
Execute the model to detect the faces from the input video




Stacking results




Import the required dependencies
Create a single object tracking class
List all the required algorithms
Create a function for stacking videos
Create a stacked video using the MoviePy module


"
155,MLOps Project to Deploy Resume Parser Model on Paperspace,"OverviewA public cloud service is offered to multiple users, whereas a private cloud service is not shared with any other organization. Google Cloud Platform(GCP), Amazon web services(AWS), and Microsoft Azure are some examples of Public cloud. Public clouds are very cost-effective but less secure. On the other hand, the Private cloud is dedicated to a particular organization and is highly secure.&nbsp;
Paperspace is one of the high-performance private clouds used for building, training, and deploying machine learning models. This project will deploy a streamlit application developed in Resume parsing with Machine learning - NLP with Python OCR and Spacy project on the private cloud Paperspace. You can also have a look at GCP deployment of the same model here.

Aim
To deploy a Streamlit Application for Resume Parsing on a private cloud Paperspace

Prerequisites
It is advisable to have a basic knowledge of the following services to understand the project.


Docker


Git


Resume parsing with Machine learning - NLP with Python OCR and Spacy

"
156,Deploy Transformer-BART Model on Paperspace Cloud,"Overview
Publicly available clouds make the work of data scientists easier for model deployment. But they need a lot of monitoring, and anyone can easily access the data from them. Private clouds solve the problem of security monitoring and reduce the steps for the deployment, making it even easier. Paperspace is a private cloud where we can deploy the model in very few steps.
This project demonstrates how to deploy an Abstractive Text Summarization using Transformers-BART Model on the private cloud Paperspace. You can find the deployment of the same model on GCP here.&nbsp;

Aim
To deploy an Abstractive Text Summarizer Machine Learning model on Paperspace cloud

What is Paperspace?
Paperspace is a private cloud that can be used to deploy machine learning applications in just a few steps.
Paperspace comes with two products


Gradient (To train and deploy the Machine Learning Models)


Core (A full-fledged cloud with GPU support to run any Application)


In this project, we will be using the Gradient product. Paperspace comes with GPU support, which helps train the machine learning model quickly.&nbsp;

Why Paperspace?


Easy access


High scalability&nbsp;


Easy deployment&nbsp;


Less monitoring


GPU Interfacing&nbsp;



When to use Paperspace?


When you need less monitoring for your applications


When you have sensitive data and business operations


Government organization, Organisations with a highly regulated environment


To carry out your business which requires high computing and security



&nbsp;Prerequisites&nbsp;


Docker


Git&nbsp;


Abstractive Text Summarization using Transformers-BART Model

"
157,"Build Regression (Linear,Ridge,Lasso) Models in NumPy Python","Business Objective 
Regression is a supervised learning algorithm. Regression analysis is used to establish a relationship between one or more independent variables and a dependent variable. There are several variations in regression analysis like linear, multiple linear, and nonlinear.&nbsp;One can create regression models with the help of the &lsquo;Scikit-learn&rsquo; library, the most valuable and robust library for machine learning in Python. But, in this project, we will be building our models from scratch using NumPy. Building your model&nbsp;allows for more flexibility during the training process, and one can tweak the model to make it more robust and responsive to real-world data as required in the future during re-training or in production.
This project explains how linear regression works and how to build various regression models such as linear regression, ridge regression, lasso regression, and decision tree from scratch using the NumPy module.
&nbsp;
Data Description&nbsp;
The dataset provides information about the players of a particular sport, and the target is the predict the scores. The dataset consists of around 200 rows and 13 columns.
&nbsp;
Aim
To build multiple regression models from scratch using the NumPy module.
&nbsp;
Tech stack 

Language - Python
Libraries - Pandas, NumPy

&nbsp;
Approach 

Importing the required libraries and reading the dataset.
Data pre-processing




Removing the missing data points
Dropping categorical variables
Checking for multi-collinearity and removal of highly correlated features




Creating train and test data by randomly shuffling data
Performing train test split
Model building using NumPy




Linear Regression Model
Ridge Regression
Lasso Regressor
Decision Tree Regressor




Model Validation




Mean Absolute Error
R2 squared


"
158,Build a Multi-Class Classification Model in Python on Saturn Cloud,"Business Objective 
The Jupyter Notebook is a potent tool for interactively developing and presenting data science projects. The easiest way for anyone to get started with Jupyter Notebooks is by installing Anaconda. Anaconda is the most widely used Python distribution for data science and comes pre-loaded with all the most popular libraries and tools.
Just like Anaconda, Saturn Cloud is another computing environment. Saturn Cloud is a data science platform that helps people quickly do work using whatever technology they need, including high-memory computing, GPU processors, and Dask clusters. You can use Saturn Cloud with Python, R, or nearly any other programming language.
In this project, we will be creating a Jupyter Server on the Saturn cloud platform for executing our Multiclass label classification project.
&nbsp;
Data Description&nbsp;
We are dealing with a licenses dataset that holds various information related to the business license. This dataset consists of around 86k rows and 32 columns. Some of the essential categories are a timeline of the application, type of business, location details of the business, and payment details. Here, the dependent variable is license status that has five classes.
&nbsp;
Aim
The objective of our problem is to predict the license status for the given business.
&nbsp;
Tech stack 

Language - Python
Libraries - NumPy, pandas, matplotlib, seaborn, sklearn
Platform - Saturn Cloud

&nbsp;
Approach 

Import the required libraries and read the dataset.
Exploratory Data Analysis (EDA) -




Univariate Analysis
Bivariate Analysis




Feature Engineering




Change column names
Check for the outliers with boxplot, IQR, and Z-score method
Drop columns that are not relevant for the prediction.
Missing value imputation
Convert string object into date type
One Hot Encoding of categorical variables




Class imbalance




Over-sampling and under-sampling




Train-test data preparation




Selecting relevant features
Performing train test split
Min-Max scaling




Model Building




Logistics Regression
KNN
Na&iuml;ve Bayes
Decision Tree




Making predictions on the test data
Model Validation




Precision
Recall
F1-score
Confusion matrix




Plot feature importance variables
"
159,Build CNN Image Classification Models for Real Time Prediction,"Overview
Convolutional Neural Networks are part of deep neural networks which makes working with images and videos pretty easy. Convolutional Neural Networks capture useful information from images which helps to correctly classify them.&nbsp;
This project will give you a brief idea about CNN and related concepts. It also builds a CNN model for image classification and the model will also be tested for real-time prediction. If you haven&rsquo;t already visited, here is the previous project of the series Build Deep Autoencoders Model for Anomaly Detection in Python.

Aim&nbsp;


To understand the basic ideas related to CNN


To develop a Convolutional Neural Network model to classify images into different classes


To deploy the model using Flask



Why use CNN?
Before CNN came into existence, image classification was a very hectic and time-consuming job as features for images needed to be created and fed to the model. Whereas CNN uses automatic feature extraction using convolution to make a feature map containing useful information of the image which is used by CNN to classify the images.&nbsp;

When to use CNN?


Image Classification


Image Segmentation


Video Analysis


Object Detection



Prerequisites


Tensorflow


Flask


Deep learning and Activation functions


Build Deep Autoencoders Model for Anomaly Detection in Python


&nbsp;
&nbsp;
Tech Stack


Language: Python


Libraries: tensorflow, pandas, matplotlib, keras, flask, pathlib



Data Description
Dataset used in this project are images of driving license, social security, and others categorized into respective categories. The images are of different shapes and sizes which are preprocessed before modeling.&nbsp;

Approach


Data loading


Data Preprocessing


Model building and training


Data Augmentation


Deployment 

"
160,Azure Deep Learning-Deploy RNN CNN models for TimeSeries,"Business Objective 
Machine Learning Operations (MLOps) is based on DevOps principles and practices that increase the efficiency of workflows. For example, continuous integration, delivery, and deployment. MLOps enables the automated management of the end-to-end machine learning lifecycle. The main goal of MLOps is the faster deployment of models in production.
We will be using Azure as our cloud platform. Microsoft Azure provides many robust services in its ecosystem to create an end-to-end MLOps pipeline. In this project, we will be deploying our time-series deep learning model on the Azure cloud platform in a multi-part format.&nbsp;
&nbsp;
Aim
To create an MLOps project using the Microsoft Azure platform to deploy a deep learning model in production.
&nbsp;
Tech stack 

Language - Python
Libraries - Flask, pickle, pandas
Services - Flask, Azure

&nbsp;
Prerequisites
It is advisable to have a basic knowledge of the following services to understand the project.

Docker
Flask
Deep Learning Project for Time Series Forecasting in Python

&nbsp;
Approach 

Sign-in to the Microsoft Azure account
Create a virtual machine




Select the tab to create a new VM
Add the basic configuration details to create a VM instance.




Connect to the Virtual machine




Download and install the putty application
Add the configuration details
Refer to the &lsquo;setup-new-vm.sh&rsquo; file and add the commands.




Import codebase in a virtual machine




Download the Filezilla and create a new client




Flask app deployment




Run the app.py file on the putty terminal
Download the postman application and call the Flask API on it.




Docker installation




Follow the steps from the &lsquo;install-docker-sh&rsquo; to install docker




Docker-based deployment
"
161,Hands-On Approach to Master PyTorch Tensors with Examples,"Overview
Pytorch is one of the deep learning frameworks developed by Facebook(Meta). It is an open-source machine learning library based on Torch. Pytorch tensors are very similar to Numpy arrays with GPU support, making the coding fast and efficient. Tensors are generalizations of scalars, vectors, and matrices to an arbitrary number of indices and are understood as multi-dimensional arrays. Tensors are building blocks for any machine learning algorithm.&nbsp;
This project introduces the Pytorch library and various tensors operations in it.&nbsp;

Why Pytorch?


Easy to learn and simpler to code


Support of computational graphs at run time


Reach set of libraries


It is flexible, faster, and provides better optimization


Support of both CPU and GPU


Easy to debug using Python IDEs



Applications


Image Classification


Handwriting recognition


Text generation


Forecast time sequence


Style transfer



Aim


To give brief introductions to Tensors


To understand the working of Pytorch and tensors operations in it



Data Description
The dataset used in this project has information about the customer churn based on the various features. The dataset contains 2000 rows and 15 features to predict the churn.

Tech Stack


Language: Python


Libraries: Pandas, Pytorch



Approach


Creating tensors


Creating tensors from the dataset


Arithmetic, Trigonometric operations&nbsp;


Statistical operations&nbsp;


Functions operations


Gradient calculation&nbsp;

"
162,SQL Project for Data Analysis using Oracle Database-Part 2,"What is Dataset Analysis?&nbsp;
Dataset Analysis is defined as manipulating or processing unstructured or raw data to draw valuable insights and conclusions that will help derive critical decisions that add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.
Oracle SQL Developer :
Oracle SQL Developer is a free IDE that makes it easy to develop and operate Oracle Database in both traditional and cloud environments. SQL Developer is a complete end-to-end development of PL/SQL jobs, worksheets for running queries and scripts,&nbsp; DBA&nbsp; for database administration, reporting interfaces and comprehensive data modeling output.
What is the Agenda of the project?&nbsp;
This is the second project in the SQL project series, the first project involved the installation of Oracle and the basics of SQL. This project&rsquo;s Agenda involves Analyzing the data using SQL on the Oracle Database Software. Understanding different types of Joins(Inner join, Left outer join, Right outer join, Full outer join, Self join), different types of Operators(Minus, Union, Union all, Intersect).&nbsp;
Tech stack:&nbsp;&nbsp;


SQL Programming language


Oracle SQL Developer

"
163,SQL Project for Data Analysis using Oracle Database-Part 3,"What is Dataset Analysis?&nbsp;
Dataset Analysis is defined as manipulating or processing unstructured or raw data to draw valuable insights and conclusions that will help derive critical decisions that add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.
Oracle SQL Developer :
Oracle SQL Developer is a free IDE that makes it easy to develop and operate Oracle Database in both traditional and cloud environments. SQL Developer is a complete end-to-end development of PL/SQL jobs, worksheets for running queries and scripts,&nbsp; DBA&nbsp; for database administration, reporting interfaces, and comprehensive data modeling output.
What is the Agenda of the project?&nbsp;
This is the third project in the SQL project series; the second project involved analyzing the data using SQL on the Oracle Database Software. Understanding different types of Joins(Inner join, Left outer join, Right outer join, Full outer join, Self join), different types of Operators(Minus, Union, Union all, Intersect). This project involves the data analysis using Sub-query, Group-by clause and Exists clause. It also consists of using inline view and aggregate functions(Min, Max, Count, Avg) to perform better analysis on data.
Tech stack:&nbsp;&nbsp;


SQL Programming language


Oracle SQL Developer

"
164,SQL Project for Data Analysis using Oracle Database-Part 4,"What is Dataset Analysis?&nbsp;
Dataset Analysis is defined as manipulating or processing unstructured or raw data to draw valuable insights and conclusions that will help derive critical decisions that add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.
Oracle SQL Developer :
Oracle SQL Developer is a free IDE that makes it easy to develop and operate Oracle Database in both traditional and cloud environments. SQL Developer is a complete end-to-end development of PL/SQL jobs, worksheets for running queries and scripts,&nbsp; DBA&nbsp; for database administration, reporting interfaces, and comprehensive data modeling output.
What is the Agenda of the project?&nbsp;
This is the fourth project in the SQL project series; the third project involved the data analysis using Sub-query, Group-by clause, and Exists clause. It also consists of using inline view and aggregate functions(Min, Max, Count, Avg) to perform better analysis on data. This project involves the data analysis using WITH clause, the difference between COUNT(*) and COUNT(column_name), Categorization using the CASE statement, and various real-life case studies/problem statements.
Tech stack:&nbsp;&nbsp;


SQL Programming language


Oracle SQL Developer

"
165,Build a Churn Prediction Model using Ensemble Learning,"Business Objective 
Ensemble methods aim to combine the predictions of several base estimators built with a given learning algorithm to improve obustness over a single estimator. Ensemble methods are ideal for regression and classification, where they reduce bias and variance to boost the accuracy of models. The most famous ensemble methods are boosting and bagging.&nbsp;
Bagging is a homogeneous weak learners&rsquo; model that learns from each other independently in parallel and combines them for determining the model average. The Random Forest model uses Bagging.
Boosting is also a homogeneous weak learners&rsquo; model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm. The AdaBoost and Gradient Boosting use Boosting techniques.
In our case study, we will be working on a churn dataset. Churned Customers are those who have decided to end their relationship with their existing company. XYZ is a service-providing company that provides customers with a one-year subscription plan for their product. The company wants to know if the customers will renew the subscription for the coming year or not.
We have already seen how the logistics regression model works on this dataset in the first project of this series; &nbsp;Churn Analysis for Streaming App using Logistic Regression&nbsp; We have also implemented the decision tree algorithm in our second project; Build a Customer Churn Prediction Model using Decision Trees
It is advised to check these two projects first before starting with ensemble techniques.
&nbsp;
Data Description&nbsp;
This data provides information about a video streaming service&nbsp;company, where they want to predict if the customer will churn or not. The CSV consists of around 2000 rows and 16 columns
&nbsp;
Aim
To build ensemble models like Random Forest, Adaboost, and Gradient boosting models on the given dataset to determine whether the customer will churn or not.
&nbsp;
Tech stack 

Language - Python
Libraries - NumPy, pandas, matplotlib, sklearn, pickle, imblearn, lime

&nbsp;
Approach 

Importing the required libraries and reading the dataset.
Feature Engineering




Dropping of unwanted columns




Model Building




Performing train test split
Random Forest Model
AdaBoost Model
Gradient Boosting Model




Model Validation (predictions)




Accuracy score
Recall score
Precision score
F1-score
ROC and AUC




Feature Importance




Create a function to find important features
Plot the features




LIME implementation




Define a function for implementing the LIME technique over the dataset.


"
166,Time Series Analysis with Facebook Prophet Python and Cesium,"Business Objective 
Time series data can be difficult and frustrating to work with, and the various algorithms that generate models can be pretty tricky to tune. Suppose an analyst does not have significant knowledge of different ML models or deep learning models like RNN or LSTN. In that case, it is challenging for them to understand a time series analysis. Thus, FbProphet (developed by Facebook) and Cesium, two open-source libraries, were introduced that give quick, powerful, and accessible time-series modeling to data analysts and data scientists everywhere.
Facebook developed an open sourcing Prophet, a forecasting tool available in Python and R. It provides intuitive parameters that are easy to tune. Even someone who lacks deep expertise in time-series forecasting models can use this to generate meaningful predictions for various problems in business scenarios. On the other hand, Cesium is an open-source library that allows users to extract features from raw time series data, build machine learning models from these features, and generate predictions for new data.
In this sequence of time-series projects, we have covered the machine learning topics such as Autoregression modeling, Moving Average Smoothing techniques, ARIMA model, Multiple linear regression, Gaussian process, ARCH-GARCH models, and Deep Learning models.
In this project, we will be building models using FbProphet and Cesium open-source libraries.
(This is the eighth project of our time series list of projects; you can refer to the previous project through this link:&nbsp; Deep Learning Project for Time Series Forecasting in Python)
&nbsp;
Data Description&nbsp;
The dataset is &ldquo;Call-centres&rdquo; data. This data is at the month level wherein the calls are segregated at the domain level as the call center operates for various domains. There are also external regressors like no of channels and no of phone lines which essentially indicate the traffic prediction of the inhouse analyst and the resources available.
The total number of rows are 132 and number of columns are 8:

Month, healthcare, telecom, banking, technology, insurance, no of phonelines and no of channels.

&nbsp;
Aim
To build a prophet model using the FbProphet library and a multi-layer perceptron model using the Cesium model on the given time series dataset.
&nbsp;
Tech stack 

Language - Python
Libraries - pandas, NumPy, matplotlib, seaborn, TensorFlow, keras, fbprophet, cesium

&nbsp;
Approach 

Import the required libraries and read the dataset
Perform descriptive analysis
Exploratory Data Analysis (EDA) -




Data Visualization




FbProphet




Define a prophet model
Fit the model
Create a dataframe with future date values
Make predictions on the range
Create models with varying changepoints parameter
Plot the results




Cesium




Convert month to date timestamp format
Reshape the data
Extract features from raw time-series data
Perform train test split on the data
Build machine learning model from these features (MLP)
Fit and train the Multi-Layer Perceptron
Generate predictions for the test data.
Plot the results


"
167,Learn How to Build PyTorch Neural Networks from Scratch,"Overview
Neural Networks are a set of algorithms based on the human brain, and they are the heart of deep learning algorithms. Pytorch is one of the deep learning libraries to build neural networks. Pytorch is quite pythonic, which makes building neural networks pretty simple. Pytorch can help solve business problems such as classification, text recognition, image segmentation, etc.
As a part of the Pytorch project series, this project gives a brief introduction to Neural networks and how to build them in Pytorch. We will build a neural network sequentially for a classification problem in Pytorch and understand how neural networks are trained.&nbsp;
If you haven&rsquo;t already visited, here is the first project of the series Hands-On Approach to Master PyTorch Tensors with Examples.

Aim


To understand Neural Networks


To classify if a customer will churn or not by using a neural network in Pytorch



Data Description
The dataset used in this project has information about the customer churn based on the various features. The dataset contains 2000 rows and 15 features to predict the churn.

Tech Stack


Language: Python


Libraries: pandas, pytorch, matplotlib, sci-kit learn, numpy



Approach


Data cleaning&nbsp;


Data preprocessing&nbsp;


Building a sequential neural network


Model training

"
168,Airline Dataset Analysis using PySpark GraphFrames in Python,"What is the Agenda of the project?&nbsp;
This project uses GraphFrames in the Databricks notebook to quickly and easily analyze flight performance data organized into graph structures. The graph structure makes it easy to ask many questions that are not as intuitive as the table structure, like finding structural motifs, finding the shortest route between cities, and ranking airports with PageRank. In this, we will be utilizing departure delay data to perform analysis and answer the following questions:


Determine the number of airports and trips


Determining the longest delay in this dataset


Determining the number of delayed vs. on-time / early flights


Which flights departing SFO are most likely to have significant delays


Which destinations tend to have delays


Which destinations tend to have significant delays departing from SEA


Relationships through Motif Finding


Airport Ranking using PageRank


Most popular flights


Top Transfer Cities


&nbsp;
Dataset Description:
The dataset consists of 1048576 data points, including the following parameters:


Date


Delay


Distance


Origin


Destination


&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
➔Services: Databricks, Graphframes, Spark
&nbsp;
Databricks:
Databricks is a cloud platform for large-scale data engineering and collaborative data science. Databricks combines best-in-class data warehousing and data lakes into a laser house architecture. Databricks is developing a web-based Spark framework that provides automated cluster management and IPython-style notebooks. In addition to building the Databricks platform, the company co-hosted the Spark Massive Open Online Courses and a conference for the Spark community. Databricks also provides a platform for other workloads including machine learning, data warehousing, streaming analytics, and business intelligence.
&nbsp;
Graphframes:
GraphFrames is an Apache Spark module that creates DataFrame-based Graphs. The DataFrame API, paired with a new API for motif searching, allows users to design more expressive searches. DataFrame speed changes within the Spark SQL engine also help the user. It has Scala, Java, and Python high-level APIs. It intends to give GraphX capabilities as well as additional capability using Spark DataFrames. Among the new features are motif finding, DataFrame-based serialization, and very expressive graph searches.
&nbsp;
Project Architecture:

&nbsp;"
169,Learn Hyperparameter Tuning for Neural Networks with PyTorch,"Overview
Hyperparameters are a set of parameters whose value controls the learning process of the model. The performance of models can be greatly improved by tuning their hyperparameters. Tuning hyperparameters means you are trying to find out the set of optimal parameters, giving you better performance than the default hyperparameters of the model.&nbsp;
In the previous project of the series Learn How to Build Neural Networks from Scratch, we saw what Neural Networks are and how we can build a Neural Network for the classification model in Pytorch. In this project, we will tune the hyperparameters such as learning rate, epochs, dropout, Early stopping, checkpoints to improve the performance of our classification model.&nbsp;

Aim


To understand the hyperparameters of neural networks


To understand how to tune hyperparameters for improving model performance in PyTorch



Data Description
The dataset used in this project has information about the customer churn based on the various features. The dataset contains 2000 rows and 15 features to predict the churn.

Tech Stack


Language: Python


Libraries: Pandas, Pytorch,matplotlib,sci-kit learn, NumPy, imblearn



Approach


Data cleaning&nbsp;


Data preprocessing&nbsp;


Building a sequential neural network


Model training


Tuning hyperparameters

"
170,Build Multi Class Text Classification Models with RNN and LSTM,"Business Overview 
Text Classification is one of the essential applications of Natural Language Processing. Neural network-based methods have obtained significant progress on various natural language processing tasks. As deep learning is emerging, training complex data has become faster and easier with networks like RNNs and LSTM.
In the previous projects, we have witnessed how to use the Na&iuml;ve Bayes algorithm for text classification, and we have also implemented the skip-gram model for word embeddings. This project will see how to implement the Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) models for text classification.
Aim
To perform text classification on the dataset using RNN and LSTM models.
Data Description 
The dataset contains more than two million customer complaints about consumer financial products. Amongst the various available columns, we have a column that contains the actual text of the complaint and one column containing the product for which the customer is raising the complaint. 
Tech Stack

Language: Python
Libraries:&nbsp; pandas, torch, nltk, numpy, pickle, re, tqdm, sklearn

Prerequisite

The torch framework
Multiclass Text Classification using Naive Bayes in Python
Skip Gram Model Python Implementation for Word Embeddings

Approach

Installing the necessary packages through pip command
Importing the required libraries
Defining configuration file paths
Process glove embeddings




Read the text file
Convert embeddings to float array
Add embeddings for padding and unknown items
Save embeddings and vocabulary




Process Text data




Read the CSV file and drop the null values
Replace duplicate labels
Encode the label column and save the encoder and encoded labels




Data Preprocessing




Conversion to lower case
Punctuation removal
Digits removal
Additional spaces removal
Tokenization




Building Data loader
Model Building




RNN architecture
LSTM architecture
Train function
Test function




Model training




RNN model
LSTM model




Prediction on Test data
"
171,Learn How to Build a Linear Regression Model in PyTorch,"Overview
Linear Regression assumes linear relationships between the predictor and the target variable. To be more specific, the output variable can be calculated by the linear combination of the input variables. Linear regression generally uses the ordinary least squares method for calculating the coefficient of the input variable. The linear regression model can be used for solving many business problems like predicting the number of sales, predicting blood pressure, predicting house prices, and many more.&nbsp;
In this project, a linear regression model for predicting the number of days subscribed has been implemented from scratch in Pytorch. If you haven&rsquo;t already visited, here is the previous project of the series Learn How to Build PyTorch Neural Networks From Scratch.

Aim


To understand the linear regression and loss function


To build a linear regression model from scratch in PyTorch for predicting the number of days subscribed



Data Description
The dataset used in this project has information about the number of days subscribed based on the various features.

Tech Stack


Language: Python


Libraries: pandas, pytorch, matplotlib, sci-kit learn, numpy, torchvision, seaborn



Approach


Data Cleaning&nbsp;


Data Preprocessing&nbsp;


Building Linear Regression model


Model Training

"
172,MLOps using Azure Devops to Deploy a Classification Model,"Overview
Most of the time, we develop and train the machine learning models in our local Python notebook and hand off the resultant code to an app developer, who must then integrate it into an application and deploy it.&nbsp; Often, the bugs and the performance issue go unnoticed until the code has already been deployed, and it then becomes a cumbersome and frustrating task to resolve the issue. So, to tackle this problem, Azure DevOps came into the picture. You can build Machine learning apps faster through Azure DevOps. The whole process now becomes a part of the Continuous Integration (CI) and Continuous Delivery (CD) pipeline of the Azure DevOps. You can continue to write and train models in your favorite Python environment and rest assured that it is your Python code that is deployed to production without worrying about any conflicts between the app and the model.
This project is focused on how to build the Continuous Integration and Continuous Delivery pipelines using the model developed in Deep Learning Project for Beginners with Source Code Part 1 with Azure DevOps.

Aim


To understand Azure DevOps&nbsp;


To build a classification model which will predict the customer&rsquo;s license status&nbsp;


To deploy the license status classification model in a scalable way through Azure DevOps



Data Description
The dataset has information about the customers whose license status, i.e., issued, renewed, canceled, is to be predicted.

Tech Stack


Language: Python


Libraries: pandas, TensorFlow, keras, matplotlib,sci-kit learn, numPy, fastapi, unicorn, requests, json, h2o, seaborn



Prerequisites


Docker


GIT


Deep Learning Project for Beginners with Source Code Part 1

"
173,SQL Project for Data Analysis using Oracle Database-Part 5,"What is Dataset Analysis?&nbsp;
Dataset Analysis is defined as manipulating or processing unstructured or raw data to draw valuable insights and conclusions that will help derive critical decisions that add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.
&nbsp;
Oracle SQL Developer :
Oracle SQL Developer is a free IDE that makes it easy to develop and operate Oracle Database in both traditional and cloud environments. SQL Developer is a complete end-to-end development of PL/SQL jobs, worksheets for running queries and scripts,&nbsp; DBA&nbsp; for database administration, reporting interfaces, and comprehensive data modeling output.
&nbsp;
What is the Agenda of the project?&nbsp;
This is the fifth project in the SQL project series; the fourth project involved the data analysis using WITH clause, the difference between COUNT(*) and COUNT(column_name), Categorization using the CASE statement, and various real-life case studies/problem statements. This project involves data analysis using different SQL functions like ROW_NUMBER, RANK, DENSE_RANK, SUBSTR, INSTR, COALESCE and NVL. It also involves the use of some built-in functions like concat, upper, lower, initcap, rtrim, ltrim, length, lpad, rpad.&nbsp;&nbsp;
&nbsp;
Tech stack:&nbsp;&nbsp;


SQL Programming language


Oracle SQL Developer

"
174,Graph Database Modelling using AWS Neptune and Gremlin,"Business Overview
Instead of utilizing rows and columns in tables, graph data modeling allows users to define an arbitrary domain as a linked graph of nodes and relationships with properties and labels. Graph representation of data has several advantages over relational databases. A graph database may be searched by edge type or by traversing the complete graph. Because the associations between nodes are not computed at query time but are stored in the database, traversing the joins or relationships in graph databases is relatively fast. Graph databases are useful for use cases like social networking, recommendation engines, and fraud detection when you need to construct linkages between data and query these associations with minimum processing time.
In this project, the requirement from our customer (simulated), an Airport Operator, is to have an analytics platform using Graph Modeling to answer questions like:


Which are the best and worst performer airlines for flight delays?


Which are the most congested airports on the ground (a.k.a. ""taxiing"")?


Which are the busiest airport connections?


After data cleaning and modeling, we will load the airlines' data to the Amazon Neptune graph database. We will also look into ways of querying the data efficiently using Apache Gremlin.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
The Bureau of Transportation Statistics of the United States Department of Transportation monitors the on-time performance of domestic flights operated by big airlines. The databases include daily airline data such as flight details, origin and destination, carrier company, delay time, and generic delay explanation.
&nbsp;
Tech Stack:
➔ Languages-


SQL, Python3, Gremlin


➔ Services -


AWS S3, AWS Glue, AWS Athena, AWS IAM, Amazon Neptune, AWS Cloud9, AWS EC2, Apache Spark


&nbsp;
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
&nbsp;
AWS IAM
This is nothing but identity and access management, enabling us to manage access to AWS services and resources securely. One can create and manage AWS users and groups using permissions to allow and deny their access to AWS resources. It is a feature of AWS with no additional charge.
&nbsp;
AWS EC2
A virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture is known as an Amazon EC2 instance. The Amazon Elastic Compute Cloud (EC2) service allows corporate customers to run application applications in a computer environment. Using Amazon EC2 eliminates the need to invest in hardware upfront, allowing users to create and deploy apps quickly. Amazon EC2 allows users to launch as many or as few virtual servers as they want, set security and networking, and manage storage.
&nbsp;
AWS Glue&nbsp;
A serverless data integration service makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. Also, you pay storage costs for Data Catalog objects. Tables may be added to the AWS Glue Data Catalog using a crawler. The majority of AWS Glue users employ this strategy. In a single run, a crawler can crawl numerous data repositories. The crawler adds or modifies one or more tables in your Data Catalog after it's finished.
&nbsp;
AWS Athena
Athena is an interactive query service for S3 in which there is no need to load data, and it stays in S3. It is serverless and supports many data formats, e.g., CSV, JSON, ORC, Parquet, AVRO.&nbsp;
&nbsp;
Gremlin
The Apache Software Foundation's Apache TinkerPop has created Gremlin, a graph traversal language. Gremlin may be used with both OLTP and OLAP graph databases and processors. Gremlin is a data-flow language that allows users to express sophisticated property graph traversals in a concise manner. A Gremlin traversal is made up of several steps. On the data stream, a step does an atomic action. At each stage, the items in the stream are transformed, removed, or statistics are computed about the stream.
&nbsp;
Amazon Neptune
Amazon Neptune is a fully-managed graph database service that makes it simple to create and run applications that interact with large, interconnected datasets. A high-performance graph database engine lies at the core of Neptune, and this engine is designed to handle billions of relationships while querying the graph in milliseconds. The popular graph query languages Apache TinkerPop Gremlin and W3C's SPARQL are supported by Neptune, allowing you to create searches that effectively explore densely linked datasets. The basic unit of Amazon Neptune graph data is a four-position (quad) element (Subject-Predicate-Object-Graph), which is similar to a Resource Description Framework (RDF) quad.
&nbsp;
Agenda


Understand the concepts of Graph database and modeling


Upload data to S3 using AWS CLI with partitions


Run Glue Crawler to create Athena table (external) for initial querying&nbsp;


Run Glue Job to pre-process data-


Extract Edges and Vertices files according to Neptune schema


Store the data as compressed Parquet format (Columnar) for quick reads




Perform Graph Querying using Gremlin to gain insights

"
175,Learn How to Build a Logistic Regression Model in PyTorch,"Overview
Logistic regression is a probabilistic model of modeling the probabilities of the discrete outcomes given the input variables. Despite the regression name, it is a classification model rather than a regression model. In simpler terms, logistic regression is linear regression but for classification. It uses the sigmoid function to calculate the probabilities of the outcome. Unlike linear regression, logistic regression doesn&rsquo;t require linear relationships between input and output variables because of the non-linear transformation of the input variable.
In the previous project of the series, we have built a linear regression model in Pytorch. This project focuses on building a logistic regression model for predicting customer churn in PyTorch.&nbsp;

Aim


To understand the logistic regression and sigmoid function


To predict the customer churn using logistic regression in PyTorch



Data Description
The dataset used in this project has information about the customer churn based on various features.

Tech Stack


Language: Python


Libraries: pandas, pytorch, matplotlib, sci-kit learn, numpy, torchvision, seaborn



Approach


Data Cleaning&nbsp;


Data Preprocessing&nbsp;


Building Logistic Regression Model


Model Training

"
176,Learn to Build Generative Models Using PyTorch Autoencoders,"Overview
Many of us are aware of the Discriminative models, which generally focus on predicting the data labels. In contrast, Generative Models help to generate the output, which is very similar to the input data. For example, we have some data related to cars that can be used to classify the cars, but Generative models can learn the patterns in the data and generate car features completely different than the input data.&nbsp;
In this project, we start by introducing Generative Models. The PyTorch framework is used to build Autoencoders on the MNIST dataset. Finally, we learn how to use Autoencoders as Generative Models followed by generating new images of digits by using the Generative Model.

Aim
To build Generative Models by using Autoencoders in PyTorch

Tech Stack


Language: Python


Libraries: torch, torchvision, torchinfo, numpy, matplotlib&nbsp;



Approach


Introduction to Generative Models


Introduction to Autoencoders


Buiding Autoencoders on PyTorch


Model training on Google Colab


Building Generative model by using Autoencoders

"
177,Hands-On Real Time PySpark Project for Beginners,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. For quick analytic queries against any quantity of data, it uses in-memory caching and efficient query execution. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
&nbsp;
Data Pipeline:
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
&nbsp;
PySpark:
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark."
178,PySpark Big Data Project to Learn RDD Operations,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. It uses in-memory caching and efficient query execution for quick analytic queries against any quantity of data. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
Data Pipeline:
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
Agenda:
This is the second project in the Pyspark series. The first project involves the Pyspark introduction, Spark component and architecture, and basic introduction about RDD and DAG. This project involves in-depth knowledge of RDD, different types of RDD operations, the difference between transformation and action, and the various functions available in transformation and action with their execution.
&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
PySpark:
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.
&nbsp;"
179,PySpark Project for Beginners to Learn DataFrame Operations,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. It uses in-memory caching and efficient query execution for quick analytic queries against any quantity of data. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
Data Pipeline:
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
Agenda:
This is the third project in the Pyspark series. The second project involves in-depth knowledge of RDD, different types of RDD operations, the difference between transformation and action, and the various functions available in transformation and action with their execution. This project involves in-depth knowledge of Dataframes, different types of Dataframe operations, also implementation of transformation and action functions on spark dataframe.
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
PySpark:
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.
&nbsp;"
180,PySpark Project to Learn Advanced DataFrame Concepts,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. It uses in-memory caching and efficient query execution for quick analytic queries against any quantity of data. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Agenda
This is the fourth project in the Pyspark series. The third project involves in-depth knowledge of Dataframes, different types of Dataframe operations, also implementation of transformation and action functions on spark dataframe. This project involves advanced functionalities of Dataframes with the help of a business case study, and the use of the Spark submit command.
&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
&nbsp;
PySpark:
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.
&nbsp;"
181,End-to-End Big Data Project to Learn PySpark SQL Functions,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. It uses in-memory caching and efficient query execution for quick analytic queries against any quantity of data. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
Agenda
This is the fifth project in the Pyspark series. The fourth project involves advanced functionalities of Dataframes with the help of business case study, also the use of Spark submit command. This project mainly focuses on PySpark SQL, SQL function, and various joins available in PySpark SQL with the help of business case study.
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
PySpark:
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.
&nbsp;"
182,Snowflake Azure Project to build real-time Twitter feed dashboard,"Business Overview
Companies miss opportunities and are exposed to risk as a result of delays in company operations and decision-making. Organizations can move rapidly based on real-time data since it reveals issues and opportunities. Data that is gathered, processed, and evaluated in real-time is referred to as real-time data, and it comprises data that is ready to utilize as soon as it is created. A snapshot of historical data is what near real-time data is. When speed is critical, near real-time processing is preferred, although processing time in minutes rather than seconds is acceptable. Batch data that has been previously stored is considerably slower, and by the time it is ready to use, it might be days old.
In this project, we ingest generated Twitter feeds to Snowflake in near real-time that will power an in-built dashboard utility in Snowflake to obtain popularity feeds reports.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
We will use the Twitter API and fetch tweets and their metadata(re-tweets, comments, likes) using Python.
&nbsp;
Tech Stack
➔Language: Python
➔Services: Azure Storage Account, Azure Queue, Snowpipe, Snowflake, Azure Resource Group
&nbsp;
Snowflake
Snowflake is a data storage, processing, and analytics platform that blends a unique SQL query engine with a cloud-native architecture. Snowflake delivers all the features of an enterprise analytic database to the user. Snowflake components include:
- Warehouse/Virtual Warehouse
- Database and Schema
- Table
- View
- Stored procedure
- Snowpipe
- Stream
- Task
&nbsp;
Azure Queue
Azure Queue Storage allows application components to communicate in the cloud. Application components are frequently separated when developing for scale so that they may scale independently. Queue Storage enables asynchronous communications between application components operating in the cloud, on the desktop, on an on-premises server, or a mobile device. Queue Storage also allows you to manage asynchronous activities and create process workflows.
&nbsp;
Approach


We write API calls to fetch Twitter insights in real-time via Python and this code can be run in a local machine every day once


We create a Snowpipe component in Snowflakes by using Azure IAM integration(cross-account access) as Snowflakes hosted on the Azure account is different from the Azure account we own. This in turn uses Azure EventGrid and Function App in the backend to automate the file load


As soon as the script generates files in Azure Blob storage, Snowpipe recognizes the file arrival and loads the snowflakes table with file data automatically


We create a dashboard in Snowflakes that is scheduled to refresh every 30 mins to show actual feed data from Twitter, Eg. No of likes and comments/feed to understand popular feed and their sentiment.


&nbsp;
Architecture
"
183,Learn Real-Time Data Ingestion with Azure Purview,"Business Overview
Purview is a single data governance solution from Azure that lets you manage and regulate your data across on-premises, multi-cloud, and software-as-a-service (SaaS) environments. With automated data discovery, sensitive data categorization, and end-to-end data lineage, you can create a holistic, up-to-date map of your data environment. It allows data curators to manage and safeguard their data estates and data consumers to locate important reliable data.
By delivering data scanning and categorization as a service for assets throughout your data estate, Azure Purview automates data discovery. Metadata and descriptions of data assets identified are combined into a comprehensive map of your data estate. There are purpose-built apps above this map that enable data discovery, access management, and data landscape analytics environments.
Data Map makes your data more meaningful by visualizing your data assets and their interactions throughout your data estate. The data map is used to find information and control who has access.
Data Catalog helps you identify reliable data sources by exploring and searching your data assets. The data catalog identifies data sources by aligning your assets with business terminology and data classification.
Data Insights provides a snapshot of your data estate, allowing you to see what kind of data you have and where it is stored.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Agenda
This project is the first part of a two projects series. This project will focus on data ingestion and preparation for Azure Purview using Azure Logic Apps, Azure Storage Account, Azure Data Factory, and Azure SQL Databases. The next project focuses on data consumption and analysis using Azure Purview.
&nbsp;
Dataset Description
This project uses Hospital data with 30 unique data fields, a few of which include:


Hospital Name


Address


Hospital Type


Mortality national comparison


Patient experience national comparison footnote


Hospital overall rating


Meets criteria for meaningful use of EHRs


&nbsp;
Tech Stack
➔Services: Azure Logic Apps, Azure Storage Account, Azure Data Factory, Azure SQL Databases, DBeaver
&nbsp;
Architecture&nbsp;

&nbsp;"
184,Getting Started with Azure Purview for Data Governance,"Business Overview
Purview is a single data governance solution from Azure that lets you manage and regulate your data across on-premises, multi-cloud, and software-as-a-service (SaaS) environments. With automated data discovery, sensitive data categorization, and end-to-end data lineage, you can create a holistic, up-to-date map of your data environment. It allows data curators to manage and safeguard their data estates and data consumers to locate important reliable data.
By delivering data scanning and categorization as a service for assets throughout your data estate, Azure Purview automates data discovery. Metadata and descriptions of data assets identified are combined into a comprehensive map of your data estate. There are purpose-built apps above this map that enable data discovery, access management, and data landscape analytics environments.
Data Map makes your data more meaningful by visualizing your data assets and their interactions throughout your data estate. The data map is used to find information and control who has access.
Data Catalog helps you identify reliable data sources by exploring and searching your data assets. The data catalog identifies data sources by aligning your assets with business terminology and data classification.
Data Insights provides a snapshot of your data estate, allowing you to see what kind of data you have and where it is stored.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Agenda
This project is the second part of a two projects series. The first project focuses on data ingestion and preparation for Azure Purview using Azure Logic Apps, Azure Storage Account, Azure Data Factory, and Azure SQL Databases. This project focuses on data consumption and analysis using Azure Purview.
&nbsp;
Dataset Description
This project uses Hospital data with 30 unique data fields, a few of which include:


Hospital Name


Address


Hospital Type


Mortality national comparison


Patient experience national comparison footnote


Hospital overall rating


Meets criteria for meaningful use of EHRs


&nbsp;
Tech Stack
➔Services: Azure Logic Apps, Azure Storage Account, Azure Data Factory, Azure SQL Databases, DBeaver, Azure Purview
&nbsp;
Architecture&nbsp;
"
185,Build a Review Classification Model using Gated Recurrent Unit,"Overview
Gated Recurrent Unit(GRU) is an improved version of the standard Recurrent Neural Network(RNNs). As a variant of the recurrent neural network, they are able to process memories of sequential data by storing previous inputs in the internal state of networks and plan from the history of previous inputs.
In this project, an application review classification model has been built using GRU. The project focuses on classifying the review of the application based on a 1 to 5 scale with 1 being negative sentiment and 5 being positive sentiment. If you haven&rsquo;t visited already, here is the previous project of the series Build CNN Image Classification Models for Real Time Prediction

Aim


To understand the Gated Recurrent Unit


To classify the review of an app on a scale of 1 to 5 using Gated Recurrent Unit



Data Description
The dataset contains the reviews and the ratings of the app. The dataset has a score column and content column. The score columns have a number range between 1 to 5 based on the content column.&nbsp;

Tech Stack


Language: Python


Libraries: pandas, tensorflow, matplotlib, sci-kit learn, pillow, gunicorn, textblob, nltk, keras, flask



Approach


Data Preprocessing&nbsp;


Converting words to lower case


Lemmatization of the words


Tokenization of the words


One hot encoding of the scores




Model Training


Training sequential model in Tensorflow




Model Evaluation


Evaluation of model on test data



"
186,Build a CNN Model with PyTorch for Image Classification,"Overview
Convolutional Neural Network(CNN) is a deep learning algorithm that learns directly from data, eliminating the need for manual feature extraction. CNNs are particularly useful for the image data which helps in finding patterns in images to recognize objects.
In this project, we will build a CNN model for image classification where images will be classified into classes of social security cards, driving licenses, and others. We have used PyTorch for building the model which is different than other deep learning frameworks as it uses dynamic computational graphs.&nbsp; Also, PyTorch is pythonic in nature and it is built over NumPy which makes it easy to use.&nbsp;
If you haven't visited already, here is the previous project of the series Learn How to Build a Logistic Regression Model in PyTorch.&nbsp;

Aim


To understand the working of CNN


To build a Convolutional Neural Network model to classify images into different classes in PyTorch



Tech Stack


Language: Python


Libraries: pytorch, pandas, matplotlib, numpy, opnecv_python_headless, torchvision



Data Description
Dataset used in this project are images of driving license, social security, and others categorized into respective categories. The images are of different shapes and sizes which are preprocessed before modeling.&nbsp;

Approach


Data Loading


Data Preprocessing


Resizing and scaling of the images


Encoding of the class labels






Model  Building and Training


CNN model building in PyTorch&nbsp;



"
187,Build a real-time Streaming Data Pipeline using Flink and Kinesis,"Business Overview
Perishable data is information whose value can significantly decline over a period of time. When the operating conditions change to the point that the knowledge is no longer helpful, the information loses a large amount of its value. This information is frequently produced in an IoT computing context. Managing perishable data provides a number of benefits. Real-time data provides you with the information you need almost instantly and in the context which helps make smarter business decisions. The use of data from edge computing devices speeds up reaction time and minimizes time to action. Such data does not need to be kept in data centers for a long time and maybe deleted after usage, saving storage space and money. Systems are usually designed on constant patterns and can&rsquo;t scale with peaks in the load resulting in latency and server failures.
This Project will simulate real-time accidents data and architect a pipeline that will help us analyze and take quick actions using AWS Kinesis, Apache Flink, Grafana, and Amazon SNS.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
This Project uses the US car accidents dataset which includes a few of the following fields:


Severity


Start_Time


End_Time


Location


Description


City


State


&nbsp;
Tech Stack:
➔ Languages-


SQL, Python3


➔ Services -


AWS S3, AWS Glue, AWS Athena, AWS Cloud9, Apache Flink, Amazon Kinesis, Amazon SNS, AWS Lambda, Amazon CloudWatch, Grafana, Apache Zepplin


&nbsp;
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
&nbsp;
AWS Glue&nbsp;
A serverless data integration service makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. Also, you pay storage costs for Data Catalog objects. Tables may be added to the AWS Glue Data Catalog using a crawler. The majority of AWS Glue users employ this strategy. In a single run, a crawler can crawl numerous data repositories. The crawler adds or modifies one or more tables in your Data Catalog after it's finished.
&nbsp;
AWS Athena
Athena is an interactive query service for S3 in which there is no need to load data, and it stays in S3. It is serverless and supports many data formats, e.g., CSV, JSON, ORC, Parquet, AVRO.&nbsp;
&nbsp;
Grafana
Grafana is a web application for interactive visualization and analytics that is open source and cross-platform. When linked to supported data sources, it displays charts, graphs, and alerts on the web for mainly time series data.
&nbsp;
Apache Flink
Flink is a scalable data analytics platform and distributed processing engine. Flink may be used to handle massive data streams and give real-time analytical insights about the processed data to your streaming application. Flink is built to work in a variety of cluster setups, with in-memory calculations of any size. For distributed computations over data streams, Flink also offers communication, fault tolerance, and data distribution. Flink applications use unbounded or bounded data sets to process streams of events. Unbounded streams have no fixed termination and are handled indefinitely. Bounded streams have a defined beginning and endpoint and may be handled in batches.
&nbsp;
Amazon Kinesis
Amazon Kinesis Data Streams is a real-time data collection and processing service from Amazon. Kinesis Data Streams apps are data-processing applications that may be created. Kinesis Data Firehose is part of the Kinesis streaming data platform, which also includes Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. When using Kinesis Data Firehose, the user does not need to develop apps or manage resources. Configure the data producers to send data to Kinesis Data Firehose, and the data will be automatically transferred to the specified destination. Kinesis Data Firehose may also be used to transform data before delivering it.
&nbsp;
Architecture

&nbsp;"
188,Build a Hybrid Recommender System in Python using LightFM,"
Business Objective
Recommendation systems are widely used in various applications for recommending products or items to customers. These recommendations are made with two methods mainly, content-based and collaborative filtering.
Collaborative filtering is primarily focused on finding similarities between users and mutually encouraging user preferences. In comparison, Content-based filtering is used to personalize the content for each user. It uses previous actions and feedback about users' liking to provide similar recommendations.
But these methods find it difficult to make recommendations when there is insufficient data to learn the relation between customers and items. To overcome the limitations of these methods, Hybrid Recommendation System was introduced.
As part of a series of Recommender system projects, this project covers Hybrid Recommendations, a combination of content and collaborative filtering. It is advised to visit the previous projects of this series to better understand the Hybrid recommender system.

Content-based recommender: Recommender System Machine Learning Project for Beginners-3
Collaborative filtering recommender: Recommender System Machine Learning Project for Beginners-4

&nbsp;
&nbsp;
Data Description
This is a transactional data set containing the transactions for a UK-based non-store online retail. The company mainly sells unique all-occasion gifts.
&nbsp;
&nbsp;
Aim
To build a Hybrid Recommendation system using various loss functions with LightFM library.
&nbsp;
&nbsp;
Tech Stack
➔ Language: Python
➔ Libraries: pandas, numpy, scipy, lightfm

&nbsp;
&nbsp;
Approach 

Import required libraries
Read and merge the data
Perform data transformation
Perform train test split
Model building




Model building with loss function as WARP
Model building with loss function as logistics
Model building with loss function as BPR




Combine the data to build the final model
Get recommendations
"
189,Build an Analytical Platform for eCommerce using AWS Services,"Business Overview
Ecommerce analytics is the process of collecting data from all of the sources that affect a certain shop. Analysts can then utilize this information to deduce changes in customer behavior and online shopping patterns. Ecommerce analytics spans the whole customer journey, from discovery through acquisition, conversion, and eventually retention and support.
In this project, we will use an eCommerce dataset to simulate the logs of user purchases, product views, cart history, and the user&rsquo;s journey on the online platform to create two analytical pipelines, Batch and Real-time. The Batch processing will involve data ingestion, Lake House architecture, processing, visualization using Amazon Kinesis, Glue, S3, and QuickSight to draw insights regarding the following:


Unique visitors per day


During a certain time, the users add products to their carts but don&rsquo;t buy them


Top categories per hour or weekday (i.e. to promote discounts based on trends)


To know which brands need more marketing


The Real-time channel involves detecting Distributed denial of service (DDoS) and Bot attacks using AWS Lambda, DynamoDB, CloudWatch, and AWS SNS.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
This dataset contains user behavioral information from a large multi-category online store along with fields such as event_time, event_type, product_id, price, user_id. Each row in the file represents one of the following event types:


View


Cart


Removed from Cart


Purchase


&nbsp;
Tech Stack:
➔ Languages-


SQL, Python3


➔ Services -


AWS S3, AWS Glue, AWS Athena, AWS Cloud9, Apache Flink, Amazon Kinesis, Amazon SNS, AWS Lambda, Amazon CloudWatch, QuickSight, Apache Zepplin, Amazon DynamoDB, AWS Glue DataBrew


&nbsp;
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
&nbsp;
AWS Glue&nbsp;
A serverless data integration service makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. Also, you pay storage costs for Data Catalog objects. Tables may be added to the AWS Glue Data Catalog using a crawler. The majority of AWS Glue users employ this strategy. In a single run, a crawler can crawl numerous data repositories. The crawler adds or modifies one or more tables in your Data Catalog after it's finished.
&nbsp;
AWS Athena
Athena is an interactive query service for S3 in which there is no need to load data, and it stays in S3. It is serverless and supports many data formats, e.g., CSV, JSON, ORC, Parquet, AVRO.&nbsp;
&nbsp;
Apache Flink
Flink is a scalable data analytics platform and distributed processing engine. Flink may be used to handle massive data streams and give real-time analytical insights about the processed data to your streaming application. Flink is built to work in a variety of cluster setups, with in-memory calculations of any size. For distributed computations over data streams, Flink also offers communication, fault tolerance, and data distribution. Flink applications use unbounded or bounded data sets to process streams of events. Unbounded streams have no fixed termination and are handled indefinitely. Bounded streams have a defined beginning and endpoint and may be handled in batches.
&nbsp;
Amazon Kinesis
Amazon Kinesis Data Streams is a real-time data collection and processing service from Amazon. Kinesis Data Streams apps are data-processing applications that may be created. Kinesis Data Firehose is part of the Kinesis streaming data platform, which also includes Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. When using Kinesis Data Firehose, the user does not need to develop apps or manage resources. Configure the data producers to send data to Kinesis Data Firehose, and the data will be automatically transferred to the specified destination. Kinesis Data Firehose may also be used to transform data before delivering it.
&nbsp;
QuickSight&nbsp;
Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. It is the first BI service to offer pay-per-session pricing, where you only pay when your users access their dashboards or reports, making it cost-effective for large-scale deployments. It can connect to various sources like Redshift, S3, Dynamo, RDS, files like JSON, text, CSV, TSV, Jira, Salesforce, and on-premises oracle SQL-server.
&nbsp;
AWS Glue DataBrew
AWS Glue DataBrew is a visual data preparation tool that allows users to clean and normalize data without writing any code. When compared to custom-created data preparation, DataBrew helps minimize the time it takes to prepare data for analytics and machine learning. Many pre-built transformations are available to automate data preparation activities such as screening anomalies, transforming data to standard formats, and rectifying erroneous values.
&nbsp;
Amazon DynamoDB
Amazon DynamoDB is a fully managed key-value NoSQL database service that delivers quick and predictable performance while also allowing for seamless scaling. DynamoDB relieves developers of the administrative responsibilities associated with running and growing a distributed database. DynamoDB allows you to design database tables that can store and retrieve any quantity of data while also serving any degree of request volume. You may increase or decrease the throughput capacity of your tables without experiencing downtime or performance reduction. Amazon DynamoDB supports PartiQL, an open-source SQL-compatible query language that enables efficient data querying independent of where or how it is stored.
&nbsp;
Architecture
"
190,Deploy an Application to Kubernetes in Google Cloud using GKE,"Business Overview
Google Cloud is a collection of physical assets, such as computers and hard disk drives, and virtual resources, such as virtual machines (VMs), housed in Google data centers worldwide. This resource distribution has various advantages, including redundancy in a failure and decreased latency by putting resources closer to customers. This release also presents some guidelines for combining resources.
GCP offers a web-based graphical user interface for managing Google Cloud projects and resources. If a user prefers to work at the command line, the G-Cloud command-line tool can handle most Google Cloud activities.
This is the fourth project in the GCP Roadmap project series. The previous projects utilize services such as PubSub, Compute Engine, Cloud Storage, BigQuery, and GCP Dataflow with Apache Beam. This project will automate and deploy the dataflow service code from the previous project as a microservice template using Docker, Google Kubernetes Engine (GKE), and Google Cloud Functions.
&nbsp;
Tech Stack
➔ Language: Python3
➔ Libraries: FastAPI
➔ Services: Cloud Storage, Dataflow, Apache Beam, BigQuery, G-Cloud SDK, Google Kubernetes Engine, Google Container Registry, Google Cloud Functions, kubectl, Docker
&nbsp;
Cloud Storage
Cloud Storage is a service that allows users to store their data on the Google Cloud. An object is an immutable piece of data that consists of a file in any format. Objects can be stored in containers known as buckets. All buckets are related to a project, and the user may organize their projects into organizations. After starting a project, users may create Cloud Storage buckets, upload things to the buckets, and get objects. Users can also give rights to make data accessible to certain domains or for specific use cases such as establishing a website.
&nbsp;
BigQuery
Google Bigquery is a Cloud Datawarehouse powered by Google, which is Serverless, highly scalable, and cost-effectively designed for making data-driven business decisions quickly. It offers both the batch and streaming insertion capabilities and is integrated with Tensorflow as well to perform machine learning using SQL-like dialects.
&nbsp;
Apache Beam
Apache Beam is a batch and streaming data processing unified programming model. It offers many APIs for interacting with various data sources and processing data using various backends, such as Spark or Dataflow. As a result, the data may be stored elsewhere, and computation can be performed on it in a serverless manner or on a specified backend.
&nbsp;
Dataflow
Google Cloud Dataflow is a cloud-based data processing service that can handle batch and real-time data streaming. It allows users to build processing pipelines for integrating, preparing, and analyzing massive data sets, which is typical of big data processing.
&nbsp;
Google Kubernetes Engine
Google Kubernetes Engine (GKE) provides a controlled environment for deploying, maintaining, and scaling containerized applications on Google infrastructure. The GKE environment is made up of many Compute Engine instances joined together to form a cluster. The Kubernetes open-source cluster management technology powers GKE clusters and provide tools for interacting with your cluster. Kubernetes commands and resources are used to deploy and manage applications, execute administrative activities, define policies, and evaluate the health of your deployed workloads.
&nbsp;
Cloud Functions
Cloud Functions is a serverless Function as a Service (FaaS) provided by Google to run your code in the cloud, supporting many programming languages. Functions naturally scale and are highly available and fault-tolerant. Cloud Functions are ideal for designing serverless backends, real-time data processing, and adaptive apps.
&nbsp;"
191,AWS Athena Big Data Project for Querying COVID-19 Data,"Business Overview
Amazon Athena is a query service that allows you to analyze data directly in Amazon S3 using conventional SQL. Using a few clicks in the AWS Management Console, you can aim Athena at Amazon S3 data and start running ad-hoc searches with traditional SQL in seconds. Because Athena is serverless, you don't have to worry about setting up or maintaining infrastructure, and you just pay for the queries you perform. Even with big datasets and sophisticated queries, Athena grows automatically while processing queries in parallel, resulting in fast responses. In addition, Athena supports a variety of data formats, including CSV, JSON, ORC, Parquet, and AVRO.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Dataset Description
A Covid-19 dataset will be used for this project's demo purpose, which includes timestamps, posts, and comments related to Covid.
&nbsp;
➔ Languages-


SQL, Python3


➔ Services -


AWS S3, AWS Glue, AWS Athena, Amazon CloudWatch


&nbsp;
Amazon S3&nbsp;
Amazon S3 is an object storage service that provides manufacturing scalability, data availability, security, and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.
&nbsp;
AWS Glue&nbsp;
A serverless data integration service makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. Also, you pay storage costs for Data Catalog objects. Tables may be added to the AWS Glue Data Catalog using a crawler. The majority of AWS Glue users employ this strategy. In a single run, a crawler can crawl numerous data repositories. The crawler adds or modifies one or more tables in your Data Catalog after it's finished.
&nbsp;"
192,Linear Regression Model Project in Python for Beginners Part 2,"Overview
We have started our journey of understanding the background and basics of regression in the first project of this series. We understood the fundamentals of regression and created a simple linear regression model.
In this project, we will get familiarised with multiple linear regression. Unlike linear regression, multiple linear regression is used to estimate the relationship between two or more independent variables and one target / dependent variable.&nbsp;
Before starting this project, please do visit the first project of the series Linear Regression Model Project in Python for Beginners Part 1.
&nbsp;
&nbsp;
Aim
To build a multiple linear regression model in python.
&nbsp;
&nbsp;
Data Description
The dataset used is the soccer player dataset. It has information about various players from different clubs, and it provides data over ten features with a number of goals as the target variable.
&nbsp;
&nbsp;
Tech Stack


Language: Python


Libraries:&nbsp; numpy, pandas, statsmodel, seaborn, matplotlib, sklearn, scipy


&nbsp;
&nbsp;
Approach


Import the required libraries and dataset


Check for the correlation between features


Plot a graph for correlations


Remove the weakly correlated and highly multicollinear variables


Perform train test split on the dataset


Fit the multiple linear regression model


Convert categorical variables into dummy/indicator variables


Plot the results

"
193,PyTorch Project to Build a LSTM Text Classification Model,"Overview
Long short-term memory(LSTM) is a recurrent neural network architecture(RNN) used in the field of deep learning. These architectures are capable of learning long-term dependencies faced by recurrent neural networks. LSTMs have feedback connections which makes them different from the traditional feed-forward neural networks. LSTMs are particularly good at text data, speech, and time series.&nbsp;
In this project, an LSTM model for classifying the review of an app on a scale of 1 to 5 based on the feedback has been built in PyTorch. If you haven't visited already, here is the previous project of the series Build a CNN Model with PyTorch for Image Classification

Aim


To understand the&nbsp; working of LSTM&nbsp;


To classify the review of an app on a scale of 1 to 5 using LSTM



Data Description
The dataset contains the reviews and the ratings of the app. The dataset has a score column and content column. The score columns have a number range between 1 to 5 based on the content column.&nbsp;

Tech Stack


Language: Python


Libraries: pandas, tensorflow, matplotlib, sci-kit learn, nltk, numpy, pytorch



Approach


Data Preprocessing


Lowering Text, removing punctuation, removing links&nbsp;


Balancing classes


Tokenizing the text


Scaling




&nbsp;


Model Training


Training LSTM model in PyTorch




&nbsp;


Model Evaluation


Evaluation of model on test data



"
194,Build Regression Models in Python for House Price Prediction,"Business Objective 
The price of a house is based on several characteristics such as location, total area, number of rooms, various amenities available, etc.
In this project, we will perform house price prediction for 200 apartments in Pune city. Different regression models such as Linear, Random Forest, XGBoost, etc. will be implemented.
This house price prediction project will help you predict the price of houses based on various features and house properties.
&nbsp;
&nbsp;
Data Description
We are given a real estate dataset with around 200 rows and 17 different variables that play an important role in predicting our target variable i.e., price.
&nbsp;
&nbsp;
Aim
The goal is to predict sale prices for homes in Pune city.&nbsp;
&nbsp;
&nbsp;
Tech stack 

Language - Python
Libraries - sklearn, pandas, numpy, matplotlib, seaborn, xgboost

&nbsp;
Approach 

Data Cleaning




Importing the required libraries and reading the dataset.
Preliminary exploration
Check for the outliers and removal of outliers.
Dropping of redundant feature columns
Missing value handling
Regularising the categorical columns
Save the cleaned data




Data Analysis




Import the required libraries and read the cleaned dataset.
Converting binary columns to dummy variables
&nbsp;Feature Engineering
Univariate and Bivariate analysis
Check for correlation
Feature selection
Data Scaling
Saving the final updated dataset




Model Building




Data preparation
Performing train test split
Linear Regression
Ridge Regression
Lasso Regression
Elastic Net Regression
Random Forest Regression
XGBoost Regression
K-Nearest Neighbours Regression
Support Vector Regression




Model Validation




Mean Absolute Error
R2 score
Plot for residuals




Performs&nbsp;the&nbsp;grid&nbsp;search&nbsp;and&nbsp;cross-validation&nbsp;for&nbsp;the&nbsp;given&nbsp;regressor
Fitting the model and making predictions on the test data
Checking for Feature Importance
Model comparisons.
"
195,Learn to Build a Polynomial Regression Model from Scratch,"Overview
Polynomial regression is a form of regression analysis in which the relationship between the dependent variable and independent variable is mapped using nth degree polynomial. If we apply linear regression to the linear dataset, then it gives good results. On the other hand, if we apply linear regression to non-linear data then the results are drastic. So, for such cases, we need polynomial regression which will capture the non-linear relationship in the data. Hence, in polynomial regression, the original features are converted into polynomial features of the required degree and modeled using the linear model.&nbsp;
In this project, we will be building a polynomial regression model to predict the points scored by the sports team. The project will give in-depth intuition about the business problem, exploratory data analysis, data preprocessing, and model building. The project will also cover a detailed explanation of various regression metrics.&nbsp;

Aim
To predict points scored by the sports team using polynomial regression&nbsp;

Data Description
The dataset contains information about the points scored by sports teams based on various attributes.&nbsp;

Tech Stack


Language: Python


Libraries: pandas, numpy, scipy, matplotlib, seaborn, sklearn, statsmodel



Approach


Data Preprocessing


Outlier removal


Imputing null values


Onehot encoding




&nbsp;


Model Building


Linear regression model building&nbsp;


Polynomial regression model building




&nbsp;


Model Evaluation


Evaluation of model on test data


Discussion on various regression matrix-like R-squared, AIC, AICC, F-statistics



"
196,SQL Project for Data Analysis using Oracle Database-Part 6,"What is Dataset Analysis?&nbsp;
Dataset Analysis is defined as manipulating or processing unstructured or raw data to draw valuable insights and conclusions that will help derive critical decisions that add some business value. The dataset analysis process is followed by organizing the dataset, transforming the dataset, visualizing the dataset, and finally modeling the dataset to derive predictions for solving the business problems, making informed decisions, and effectively planning for the future.
Oracle SQL Developer :
Oracle SQL Developer is a free IDE that makes it easy to develop and operate Oracle Database in both traditional and cloud environments. SQL Developer is a complete end-to-end development of PL/SQL jobs, worksheets for running queries and scripts,&nbsp; DBA&nbsp; for database administration, reporting interfaces, and comprehensive data modeling output.
What is the Agenda of the project?&nbsp;
This is the sixth project in the SQL project series; the fifth project involved the data analysis using different SQL functions like ROW_NUMBER, RANK, DENSE_RANK, SUBSTR, INSTR, COALESCE and NVL. It also involved the use of some built-in functions like concat, upper, lower, initcap, rtrim, ltrim, length, lpad, rpad. This project involves the introduction to Data Wrangling, operations on missing data, unwanted features and duplicated records. It also involves the use of the pivot and unpivot functions in SQL.&nbsp;
Tech stack:&nbsp;&nbsp;


SQL Programming language


Oracle SQL Developer

"
197,PySpark ETL Project-Build a Data Pipeline using S3 and MySQL,"Business Overview
Apache Spark is a distributed processing engine that is open source and used for large data applications. It uses in-memory caching and efficient query execution for quick analytic queries against any quantity of data. It offers code reuse across many workloads such as batch processing, interactive queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.
&nbsp;
Data Pipeline
A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.
&nbsp;
Agenda
This is the sixth project in the Pyspark series. The fifth project involved the introduction to PySpark SQL, SQL function, and various joins available in PySpark SQL with the help of business case study. This project mainly focuses on the integration of PySpark with Amazon S3 and MySQL database to perform ETL(Extract-Transform-Load) and ELT(Extract-Load-Transform) operations.
&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: Pyspark
➔Services: AWS S3, MySQL
&nbsp;
PySpark:
PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.
&nbsp;
MySQL
MySQL is a SQL (Structured Query Language) based relational database management system. The platform can be used for data warehousing, e-commerce, logging applications, etc.
&nbsp;
Amazon S3
Amazon S3 is a simple web service interface for object storage that allows you to store and retrieve unlimited amounts of data from anywhere on the internet. It is used for Backup and archive for on-premises or cloud data Content, media, and software storage and distribution.
&nbsp;"
198,Build a Graph Based Recommendation System in Python-Part 2,"Business Objective:
In the previous project; (Build a Graph Based Recommendation System in Python), we understood what recommendation systems are and how they work.&nbsp;
We also understood what clickstream data is and how to convert the clickstream data into various types of graphs.&nbsp;
This project will see how to build a recommendation system for our eCommerce platform by converting the generated graphs into embeddings and further using similarity search.&nbsp;

Aim:
To build a Graph based recommender system that will recommend similar products to the users of our e-commerce platform.

Data Overview :
The dataset contains user information over 9 attributes for an eCommerce website
The data has been converted into graph format for further use.

Tech Stack:


Language: Python


Packages: pandas, numpy, pecanpy, gensim, plotly, umap, faiss


File Management: Parquet



Prerequisites:&nbsp;


Build a Graph Based Recommendation System in Python


Understanding of Word2Vec technique



File structure:&nbsp;


Data - The data folder consists of 3 subfolders.






ConstructedGraph - The generated graphs from the previous project are saved here.


Edg_Graphs_DataFile - The graphs are saved in (.edg) format for further processing


Embedding_Data -&nbsp; Generated embeddings are saved here






Documentation - This folder consists of different learning resources and references&nbsp;


Model - Generated models are saved in this folder


Notebook - This folder contains the notebooks we have used






Data Exploration and Data Analysis [RECAP].ipynb


GraphConstruct [RECAP].ipynb


Deepwalk and Node2vec Model Training.ipynb


Result Analysis.ipynb


Embedding Vector Search with ANN FAISS (Recommendation).ipynb


Constants.py






Script - Scripts available in .py file


requirements.txt - The requirements.txt file has all the required libraries with respective versions. Kindly install the requirements at the beginning.&nbsp;



Approach:


Understand the problem statement


Deepwalk and Node2vec Model training






Import the necessary packages and libraries


Read the selected graph


Save the graph in .edg format


Graph random walk generation&nbsp;


Model Building with Word2Vec and Node2Vec


Save the embeddings in parquet format






Result analysis and Visualization






Import the necessary packages and libraries


Read the data


Define a function for generating levels of category


Use UMAP (Uniform Manifold Approximation and Projection) method as a dimension reduction strategy


Visualize the clusters






Embedding vector search






Import the necessary packages and libraries


Use FAISS (Facebook AI Similarity Search) for generating recommendations&nbsp;



"
199,Image Classification Model using Transfer Learning in PyTorch,"Overview
Transfer Learning is a machine learning algorithm where we reuse a pre-trained model as the starting point for a model on a new task. The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world.&nbsp;
In this project, the ResNet model has been used as a pre-trained model for image classification in PyTorch. The images have been classified into classes of social security cards, driving licenses, and others. If you haven't visited already, here is the previous project of the series PyTorch Project to Build a LSTM Text Classification Model

Aim


To understand the transfer learning and ResNet model.


To build a transfer learning model for image classification in PyTorch



Tech Stack


Language: Python


Libraries: pytorch, pandas, matplotlib, numpy, opnecv_python_headless, torchvision



Data Description
Dataset used in this project are images of driving licenses, social security, and others categorized into respective categories. The images are of different shapes and sizes which are preprocessed before modeling.

Approach


Data Loading


Data Preprocessing


Resizing and scaling the images


Encoding the class labels




&nbsp;


Model  Building and Training


ResNet model building in PyTorch 



"
200,Build a Text Classification Model with Attention Mechanism NLP,"Business Overview&nbsp;
Attention is an increasingly popular mechanism used in many applications today. The attention mechanism focus on dynamically highlighting relevant features of the input data. Here,&nbsp; the model gives more weightage to the most relevant word in a sentence to make predictions.&nbsp;
In the previous projects, we have seen how to implement the Na&iuml;ve Bayes algorithm, skip-gram model, Recurrent Neural Network (RNN), and Long Short-Term Memory (LSTM) models for text classification.
In this project, let's see how well does the attention model work for our text classification problem.&nbsp;
&nbsp;
&nbsp;
Aim
To perform multiclass text classification on the dataset with the help of the attention mechanism&nbsp;
&nbsp;
&nbsp;
Data Description&nbsp;
The dataset contains more than two million customer complaints about consumer financial products. Amongst the various available columns, we have a column that contains the actual text of the complaint and one column containing the product for which the customer is raising the complaint.
&nbsp;
&nbsp;
Tech Stack


Language: Python


Libraries:&nbsp; pandas, torch, nltk, numpy, pickle, re, tqdm, sklearn



Prerequisite


The torch framework


Multiclass Text Classification using Naive Bayes in Python


Skip Gram Model Python Implementation for Word Embeddings


Build Multi Class Text Classification Models with RNN and LSTM



Approach


Installing the necessary packages through the pip command


Importing the required libraries


Defining configuration file paths


Process glove embeddings






Read the text file&nbsp;


Convert embeddings to float array


Add embeddings for padding and unknown items


Save embeddings and vocabulary&nbsp;






Process Text data






Read the CSV file and drop the null values


Replace duplicate labels


Encode the label column and save the encoder and encoded labels






Data Preprocessing






Conversion to lower case


Punctuation removal


Digits removal


Remove more than one consecutive instance of 'x'


Additional spaces removal


Tokenize the text&nbsp;


Save the tokens&nbsp;






Model Building






Create attention model


Create a function for the PyTorch dataset&nbsp;


Function to train the model


Function to test the model






Train model






Load the files


Split data into train, test, and validation


Create PyTorch datasets


Create data loaders


Create model object


Move the model to GPU if available


Define loss function and optimizer


Training the model


Test the model




&nbsp; &nbsp; &nbsp; 9. Prediction of new text"
201,Learn to Build a Neural network from Scratch using NumPy,"Overview
A neural network is a set of algorithms that aim to recognize the underlying pattern in the data. These are subsets of Machine learning and the heart of deep learning algorithms. Neural networks are inspired by the working of the human brain.&nbsp;
Neural networks are applied in many real-life applications like self-driving cars, speech recognition, medical diagnosis, and many more.
In this project, we will be building a neural network from scratch just by using NumPy only and not by using any deep learning frameworks like Tensorflow or PyTorch. The built neural network will predict the price of the house in Pune, India. This project will also give you an in-depth idea about the working of neural networks.&nbsp;

Aim


To understand the working of Neural networks


To build a neural network from scratch using NumPy&nbsp;



Data Description
The dataset contains information about the prices of houses based on various attributes. The dataset contains the prices of houses from Pune, Maharashtra(India).&nbsp;

Tech Stack


Language: Python


Libraries: pandas, sci-kit learn, numpy



Approach


Data Preprocessing


Removing null values


Scaling numerical features


One-Hot encoding of categorical data




&nbsp;


Model Training


Training neural network model




&nbsp;


Model Evaluation


Evaluation of model on test data



"
202,PySpark Project-Build a Data Pipeline using Hive and Cassandra,"Agenda
This is the seventh project in the Pyspark series. The sixth project focuses on integrating PySpark with Amazon S3 and MySQL databases to perform ETL(Extract-Transform-Load) and ELT(Extract-Load-Transform) operations. This project mainly focuses on integrating PySpark with Apache Cassandra and Apache Hive to perform ETL(Extract-Transform-Load) and ELT(Extract-Load-Transform) operations.
&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: PySpark
➔Services: AWS EC2, Docker, Apache Cassandra, Hive
&nbsp;
AWS EC2
Amazon EC2 instance is a virtual server on Amazon's Elastic Compute Cloud (EC2) for executing applications on the Amazon Web Services (AWS) architecture. Corporate customers can use the Amazon Elastic Compute Cloud (EC2) service to run applications in a computer environment. Amazon EC2 eliminates the requirement for upfront hardware investment, allowing customers to design and deploy projects quickly. Users can launch as many or as few virtual servers as they like, configure security and networking, and manage storage on Amazon EC2.
&nbsp;
Docker
Docker is a free and open-source containerization platform, and it enables programmers to package programs into containers. These standardized executable components combine application source code with the libraries and dependencies required to run that code in any environment.
&nbsp;
Apache Cassandra
Apache Cassandra is a distributed data storage system that is free and open-source. It is a column-oriented database. It is fault-tolerant and scalable. Its design enables users to respond to abrupt spikes in demand by allowing them to simply add extra hardware to accommodate more customers and data. Cassandra can handle organized, semi-structured, and unstructured data, allowing users to store data in a variety of ways. Cassandra employs numerous data centers to facilitate data delivery wherever and wherever it is required. Cassandra supports the ACID properties of atomicity, consistency, isolation, and durability.
&nbsp;
Hive
Apache Hive is a fault-tolerant distributed data warehouse that allows for massive-scale analytics. Using SQL, Hive will enable users to read, write, and manage petabytes of data. Hive is built on top of Apache Hadoop, an open-source platform for storing and processing large amounts of data. As a result, Hive is inextricably linked to Hadoop and is designed to process petabytes of data quickly. Hive is distinguished by its ability to query large datasets with a SQL-like interface utilizing Apache Tez or MapReduce. "
203,PySpark Project-Build a Data Pipeline using Kafka and Redshift,"Agenda
This is the eighth project in the Pyspark series. The seventh project focuses on integrating PySpark with Apache Cassandra and Apache Hive to perform ETL(Extract-Transform-Load) and ELT(Extract-Load-Transform) operations. This project mainly focuses on the integration of PySpark with Confluent Kafka and Amazon Redshift to perform ETL(Extract-Transform-Load) and ELT(Extract-Load-Transform) operations.
&nbsp;
Tech stack:&nbsp;&nbsp;
➔Language: Python
➔Package: PySpark
➔Services: Docker, Confluent Kafka, Amazon Redshift
&nbsp;
Amazon Redshift
Amazon Redshift is a fully managed petabyte-scale cloud data warehouse service. Redshift Spectrum also runs SQL queries directly against structured or unstructured data in Amazon S3 without loading them into the Redshift cluster. Redshift lets us run complex, analytic queries against structured and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage like SSD, and massively parallel query execution. It is an OLAP solution to store petabytes of information without owning Infrastructure (Paas).
&nbsp;
Confluent Kafka
Kafka is a distributed data storage designed for real-time input and processing of streaming data. Streaming data is information that is continuously generated by thousands of data sources, all of which transmit data records at the same time. A streaming platform must be able to cope with the constant influx of data and process it sequentially and progressively. Kafka efficiently stores records streams in the order in which they were created. Kafka is most commonly used to create real-time streaming data pipelines and applications that react to changing data streams. It mixes communications, storage, and stream processing to enable both historical and real-time data storage and analysis.
&nbsp;"
204,NLP Project for Multi Class Text Classification using BERT Model,"Business Overview&nbsp;
So far, in this series of NLP projects for our multiclass text classification problem, we have come across several algorithms such as; the Na&iuml;ve Bayes algorithm, skip-gram model, Recurrent Neural Network (RNN), and Long Short-Term Memory (LSTM) as well as attention mechanism. All these models were built and implemented from scratch.&nbsp;
In this particular project, we will be using a pre-trained model to predict our text known as BERT. BERT is an open-source ML framework for Natural Language Processing. BERT stands for Bidirectional Encoder Representations and is a pre-trained model from Google known for producing state-of-the-art results in a wide variety of NLP tasks.

Aim
To perform multiclass text classification on the dataset using the pre-trained BERT model.
&nbsp;
&nbsp;
Data Description&nbsp;
The dataset contains more than two million customer complaints about consumer financial products. Amongst the various available columns, we have a column that contains the actual text of the complaint and one column containing the product for which the customer is raising the complaint.
&nbsp;
&nbsp;
Tech Stack


Language: Python


Libraries:&nbsp; pandas, torch, nltk, numpy, pickle, re, tqdm, sklearn, transformers&nbsp;


&nbsp;
Prerequisite


The torch framework


Multiclass Text Classification using Naive Bayes in Python


Skip Gram Model Python Implementation for Word Embeddings


Build Multi Class Text Classification Models with RNN and LSTM


Build a Text Classification Model with Attention Mechanism NLP


&nbsp;
Approach


Installing the necessary packages through the pip command


Importing the required libraries


Defining configuration file paths


Process Text data






Read the CSV file and drop the null values


Replace duplicate labels


Encode the label column and save the encoder and encoded labels






Data Preprocessing






Conversion to lower case


Punctuation removal


Digits removal


Remove more than one consecutive instance of 'x'


Additional spaces removal


Tokenize the text&nbsp;


Save the tokens&nbsp;






Model Building






Create BERT model


Create a function for the PyTorch dataset&nbsp;


Function to train the model


Function to test the model






Train BERT model&nbsp;






Load the files


Split data into train, test, and validation


Create PyTorch datasets


Create data loaders


Create model object


Define loss function and optimizer&nbsp;


Move the model to GPU if available


Training the model


Test the model




&nbsp; &nbsp;8 . Predictions of new text"
205,SQL Project for Data Analysis using Oracle Database-Part 7,"What is the Agenda of the project?&nbsp;
This is the seventh project in the SQL project series; the sixth project involved the introduction to Data Wrangling, operations on missing data, unwanted features, and duplicated records. It also involves the use of the pivot and unpivot functions in SQL. This project involves the understanding of the Online Shopping Database, and using this database to perform the following Data Wrangling activities-


Split full name into the first name and last name


Correct phone numbers and emails which are not in a proper format


Correct contact number and remove full name


Read BLOB column and fetch attribute details from the regular tag


Read BLOB column and fetch attribute details from nested columns


Read BLOB column and fetch attribute details from nested columns


Create separate tables for blob attributes


Remove invalid records from order_items where shipment_id is not mapped


Map missing first name and last name with email id credentials


&nbsp;
Tech stack:&nbsp;&nbsp;


SQL Programming language


Oracle SQL Developer

"
